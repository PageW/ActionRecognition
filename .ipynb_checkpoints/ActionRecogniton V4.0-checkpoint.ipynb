{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、import some package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6b53a1a4024d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexposure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_as_float\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimshow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimread_collection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcatenate_images\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_plugin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexpected_warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_binary_blobs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbinary_blobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmanage_plugins\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msift\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\manage_plugins.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread_collection_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\io\\collection.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtifffile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTiffFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PILLOW_VERSION'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         raise ImportError(\"The _imaging extension was built for another \"\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils,plot_model\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mping\n",
    "import numpy as np\n",
    "import resnet\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from skimage import data, exposure, img_as_float\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "import scipy\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input,BatchNormalization\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "imageio.plugins.ffmpeg.download()\n",
    "\n",
    "import skimage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、读取图片进入X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_rows=320\n",
    "img_cols=568\n",
    "img_channels=3\n",
    "nb_classes=5\n",
    "m=180\n",
    "\n",
    "filename = r'C:\\Users\\wangpeizhi\\Desktop\\video.mp4'\n",
    "vid = imageio.get_reader(filename,  'ffmpeg') \n",
    "X=np.zeros([m,320,568,3])\n",
    "Y=np.zeros([m,nb_classes])\n",
    "indice=0\n",
    "for im in enumerate(vid):\n",
    "    if(im[0]%30==0)&(indice<m):\n",
    "        X[indice,:,:,:]=im[1]\n",
    "        indice +=1\n",
    "        print(indice)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、网络的一些超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=7, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.0005, patience=40)\n",
    "csv_logger = CSVLogger('imagemodel3.1.csv')\n",
    "checkpointer = ModelCheckpoint('imagemodel3.1.h5', verbose=1, save_best_only=True)\n",
    "batch_size = 10\n",
    "nb_epoch = 300\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、预处理，白化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.astype('float32')\n",
    "mean_X=np.mean(X,axis=0)\n",
    "X -= mean_X  #白化，X减去所有数据的均值\n",
    "X /=128 #这样X大概就是在-1~1之间的分布了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五、搭建网络模型，编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 64, 64, 64)   9472        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 64, 64, 64)   256         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 64, 64, 64)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 32, 32, 64)   36928       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 32, 32, 64)   256         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 32, 64)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 32, 32, 64)   36928       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 32, 32, 64)   0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 32, 32, 64)   256         add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 32, 64)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 64)   36928       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 32, 32, 64)   256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 32, 64)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 64)   36928       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 32, 32, 64)   0           add_33[0][0]                     \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 32, 32, 64)   256         add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 64)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 64)   36928       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 64)   256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 64)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 64)   36928       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 32, 32, 64)   0           add_34[0][0]                     \n",
      "                                                                 conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 64)   256         add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 64)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 16, 16, 128)  73856       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 16, 16, 128)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 16, 16, 128)  8320        add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 16, 16, 128)  147584      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 16, 16, 128)  0           conv2d_82[0][0]                  \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 16, 16, 128)  512         add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 16, 16, 128)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 16, 16, 128)  147584      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 16, 16, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 16, 16, 128)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 16, 16, 128)  147584      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 16, 16, 128)  0           add_36[0][0]                     \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 16, 16, 128)  512         add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 16, 16, 128)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 128)  147584      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 16, 16, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 16, 16, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 128)  147584      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 16, 16, 128)  0           add_37[0][0]                     \n",
      "                                                                 conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 16, 16, 128)  512         add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 16, 16, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 128)  147584      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 16, 16, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 128)  147584      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 16, 16, 128)  0           add_38[0][0]                     \n",
      "                                                                 conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 128)  512         add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 16, 16, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 256)    295168      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 256)    1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 256)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 256)    33024       add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 256)    590080      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 8, 8, 256)    0           conv2d_91[0][0]                  \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 256)    1024        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 256)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 256)    590080      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 256)    1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 256)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 256)    590080      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 8, 8, 256)    0           add_40[0][0]                     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 256)    1024        add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 256)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 256)    590080      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 256)    1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 256)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 8, 8, 256)    590080      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 8, 8, 256)    0           add_41[0][0]                     \n",
      "                                                                 conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 256)    1024        add_42[0][0]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 256)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 8, 8, 256)    590080      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 256)    1024        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 256)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 8, 8, 256)    590080      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 8, 8, 256)    0           add_42[0][0]                     \n",
      "                                                                 conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 256)    1024        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 256)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 8, 8, 256)    590080      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 256)    1024        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 256)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 8, 8, 256)    590080      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 8, 8, 256)    0           add_43[0][0]                     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 256)    1024        add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 256)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 8, 8, 256)    590080      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 256)    1024        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 256)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 8, 8, 256)    590080      activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 8, 8, 256)    0           add_44[0][0]                     \n",
      "                                                                 conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 256)    1024        add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 256)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 4, 4, 512)    1180160     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 4, 4, 512)    2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 4, 4, 512)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 4, 4, 512)    131584      add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 4, 4, 512)    2359808     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 4, 4, 512)    0           conv2d_104[0][0]                 \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 4, 4, 512)    2048        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 4, 4, 512)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 4, 4, 512)    2359808     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 4, 4, 512)    2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 4, 4, 512)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 4, 4, 512)    2359808     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 4, 4, 512)    0           add_46[0][0]                     \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 4, 4, 512)    2048        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 4, 4, 512)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 4, 4, 512)    2359808     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 4, 4, 512)    2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 4, 4, 512)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 4, 4, 512)    2359808     activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 4, 4, 512)    0           add_47[0][0]                     \n",
      "                                                                 conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 4, 4, 512)    2048        add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 4, 4, 512)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 512)    0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 512)          0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1026        flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,307,650\n",
      "Trainable params: 21,292,418\n",
      "Non-trainable params: 15,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet.ResnetBuilder.build_resnet_34((img_channels, img_rows, img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model V2.0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "六、分割X={X_train,X_val}  Y={Y_train,Y_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= (33, 128, 128, 3)\n",
      "Y_train.shape= (33, 2)\n",
      "X_val.shape= (9, 128, 128, 3)\n",
      "Y_val.shape= (9, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"Y_train.shape=\",Y_train.shape)\n",
    "print(\"X_val.shape=\",X_val.shape)\n",
    "print(\"Y_val.shape=\",Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "七、数据增强，训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 33 samples, validate on 9 samples\n",
      "Epoch 1/300\n",
      "33/33 [==============================] - ETA: 22s - loss: 2.0819 - acc: 0.20 - ETA: 7s - loss: 1.9550 - acc: 0.5000 - ETA: 1s - loss: 1.9048 - acc: 0.600 - 16s 481ms/step - loss: 1.9865 - acc: 0.6061 - val_loss: 7.6501 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.65006, saving model to imagemodel3.1.h5\n",
      "Epoch 2/300\n",
      "33/33 [==============================] - ETA: 5s - loss: 1.6375 - acc: 0.900 - ETA: 2s - loss: 1.8340 - acc: 0.800 - ETA: 0s - loss: 1.7860 - acc: 0.766 - 8s 248ms/step - loss: 1.7530 - acc: 0.7879 - val_loss: 12.0755 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.4761 - acc: 0.900 - ETA: 2s - loss: 1.7240 - acc: 0.800 - ETA: 0s - loss: 1.6304 - acc: 0.866 - 7s 223ms/step - loss: 1.6079 - acc: 0.8788 - val_loss: 10.2182 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.8804 - acc: 0.700 - ETA: 2s - loss: 1.6355 - acc: 0.850 - ETA: 0s - loss: 1.5534 - acc: 0.900 - 8s 230ms/step - loss: 1.5365 - acc: 0.9091 - val_loss: 7.6323 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00004: val_loss improved from 7.65006 to 7.63231, saving model to imagemodel3.1.h5\n",
      "Epoch 5/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3626 - acc: 1.000 - ETA: 2s - loss: 1.3661 - acc: 1.000 - ETA: 0s - loss: 1.3604 - acc: 1.000 - 7s 223ms/step - loss: 1.4299 - acc: 0.9394 - val_loss: 8.7332 - val_acc: 0.4444\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3599 - acc: 1.000 - ETA: 2s - loss: 1.3515 - acc: 1.000 - ETA: 0s - loss: 1.4701 - acc: 0.966 - 7s 226ms/step - loss: 1.4659 - acc: 0.9697 - val_loss: 11.5773 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.4881 - acc: 1.000 - ETA: 2s - loss: 1.4119 - acc: 1.000 - ETA: 0s - loss: 1.3855 - acc: 1.000 - 7s 223ms/step - loss: 1.4074 - acc: 0.9697 - val_loss: 9.4201 - val_acc: 0.4444\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.4472 - acc: 1.000 - ETA: 2s - loss: 1.3885 - acc: 1.000 - ETA: 0s - loss: 1.7491 - acc: 0.866 - 8s 227ms/step - loss: 1.7175 - acc: 0.8788 - val_loss: 6.9466 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00008: val_loss improved from 7.63231 to 6.94663, saving model to imagemodel3.1.h5\n",
      "Epoch 9/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3669 - acc: 1.000 - ETA: 2s - loss: 1.4374 - acc: 0.950 - ETA: 0s - loss: 1.3993 - acc: 0.966 - 7s 223ms/step - loss: 1.3942 - acc: 0.9697 - val_loss: 3.1236 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00009: val_loss improved from 6.94663 to 3.12359, saving model to imagemodel3.1.h5\n",
      "Epoch 10/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3391 - acc: 1.000 - ETA: 2s - loss: 1.3624 - acc: 1.000 - ETA: 0s - loss: 1.3874 - acc: 0.966 - 7s 225ms/step - loss: 1.3807 - acc: 0.9697 - val_loss: 3.5160 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3111 - acc: 1.000 - ETA: 2s - loss: 1.3057 - acc: 1.000 - ETA: 0s - loss: 1.3583 - acc: 0.966 - 7s 222ms/step - loss: 1.3525 - acc: 0.9697 - val_loss: 5.5367 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.6050 - acc: 0.800 - ETA: 2s - loss: 1.4501 - acc: 0.900 - ETA: 0s - loss: 1.4045 - acc: 0.933 - 7s 225ms/step - loss: 1.3932 - acc: 0.9394 - val_loss: 3.3045 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2727 - acc: 1.000 - ETA: 2s - loss: 1.3434 - acc: 0.950 - ETA: 0s - loss: 1.3215 - acc: 0.966 - 7s 222ms/step - loss: 1.3170 - acc: 0.9697 - val_loss: 3.0486 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.12359 to 3.04865, saving model to imagemodel3.1.h5\n",
      "Epoch 14/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2633 - acc: 1.000 - ETA: 2s - loss: 1.2681 - acc: 1.000 - ETA: 0s - loss: 1.2747 - acc: 1.000 - 7s 226ms/step - loss: 1.2780 - acc: 1.0000 - val_loss: 2.4860 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.04865 to 2.48598, saving model to imagemodel3.1.h5\n",
      "Epoch 15/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2577 - acc: 1.000 - ETA: 2s - loss: 1.2519 - acc: 1.000 - ETA: 0s - loss: 1.2501 - acc: 1.000 - 8s 230ms/step - loss: 1.3094 - acc: 0.9697 - val_loss: 2.3538 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.48598 to 2.35378, saving model to imagemodel3.1.h5\n",
      "Epoch 16/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2335 - acc: 1.000 - ETA: 2s - loss: 1.3320 - acc: 0.950 - ETA: 0s - loss: 1.2978 - acc: 0.966 - 7s 225ms/step - loss: 1.3263 - acc: 0.9394 - val_loss: 5.3413 - val_acc: 0.4444\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2943 - acc: 1.000 - ETA: 2s - loss: 1.2571 - acc: 1.000 - ETA: 0s - loss: 1.2632 - acc: 1.000 - 8s 229ms/step - loss: 1.3820 - acc: 0.9697 - val_loss: 2.7630 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2728 - acc: 1.000 - ETA: 2s - loss: 1.4338 - acc: 0.900 - ETA: 0s - loss: 1.5292 - acc: 0.866 - 7s 227ms/step - loss: 1.6620 - acc: 0.8182 - val_loss: 3.9394 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.5277 - acc: 0.800 - ETA: 2s - loss: 1.4373 - acc: 0.900 - ETA: 0s - loss: 1.4480 - acc: 0.900 - 7s 223ms/step - loss: 1.4498 - acc: 0.8788 - val_loss: 4.7471 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3206 - acc: 1.000 - ETA: 2s - loss: 1.3167 - acc: 1.000 - ETA: 0s - loss: 1.4065 - acc: 0.933 - 7s 224ms/step - loss: 1.3923 - acc: 0.9394 - val_loss: 5.1432 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2277 - acc: 1.000 - ETA: 2s - loss: 1.2410 - acc: 1.000 - ETA: 0s - loss: 1.3430 - acc: 0.933 - 7s 225ms/step - loss: 1.3329 - acc: 0.9394 - val_loss: 4.6381 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3273 - acc: 0.900 - ETA: 2s - loss: 1.3573 - acc: 0.900 - ETA: 0s - loss: 1.3708 - acc: 0.900 - 7s 224ms/step - loss: 1.3782 - acc: 0.8788 - val_loss: 5.2637 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2496 - acc: 1.000 - ETA: 2s - loss: 1.2196 - acc: 1.000 - ETA: 0s - loss: 1.2138 - acc: 1.000 - 7s 227ms/step - loss: 1.2165 - acc: 1.0000 - val_loss: 3.7587 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "33/33 [==============================] - ETA: 5s - loss: 1.2045 - acc: 1.000 - ETA: 2s - loss: 1.2079 - acc: 1.000 - ETA: 0s - loss: 1.2019 - acc: 1.000 - 8s 232ms/step - loss: 1.2636 - acc: 0.9394 - val_loss: 3.4023 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2750 - acc: 1.000 - ETA: 2s - loss: 1.2314 - acc: 1.000 - ETA: 0s - loss: 1.2171 - acc: 1.000 - 7s 227ms/step - loss: 1.2131 - acc: 1.0000 - val_loss: 3.5437 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1777 - acc: 1.000 - ETA: 2s - loss: 1.1808 - acc: 1.000 - ETA: 0s - loss: 1.1828 - acc: 1.000 - 7s 225ms/step - loss: 1.1807 - acc: 1.0000 - val_loss: 3.5418 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1817 - acc: 1.000 - ETA: 2s - loss: 1.2404 - acc: 1.000 - ETA: 0s - loss: 1.2149 - acc: 1.000 - 8s 229ms/step - loss: 1.2096 - acc: 1.0000 - val_loss: 3.3950 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1660 - acc: 1.000 - ETA: 2s - loss: 1.1663 - acc: 1.000 - ETA: 0s - loss: 1.1623 - acc: 1.000 - 7s 224ms/step - loss: 1.1637 - acc: 1.0000 - val_loss: 3.2818 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1780 - acc: 1.000 - ETA: 2s - loss: 1.1675 - acc: 1.000 - ETA: 0s - loss: 1.1720 - acc: 1.000 - 7s 227ms/step - loss: 1.1705 - acc: 1.0000 - val_loss: 3.1931 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1461 - acc: 1.000 - ETA: 2s - loss: 1.2017 - acc: 1.000 - ETA: 0s - loss: 1.1855 - acc: 1.000 - 7s 225ms/step - loss: 1.1890 - acc: 1.0000 - val_loss: 3.0719 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1447 - acc: 1.000 - ETA: 2s - loss: 1.1462 - acc: 1.000 - ETA: 0s - loss: 1.1440 - acc: 1.000 - 8s 231ms/step - loss: 1.1457 - acc: 1.0000 - val_loss: 3.0167 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1384 - acc: 1.000 - ETA: 2s - loss: 1.2074 - acc: 1.000 - ETA: 0s - loss: 1.2033 - acc: 1.000 - 8s 232ms/step - loss: 1.2570 - acc: 0.9697 - val_loss: 2.9983 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1398 - acc: 1.000 - ETA: 2s - loss: 1.1440 - acc: 1.000 - ETA: 0s - loss: 1.1462 - acc: 1.000 - 8s 232ms/step - loss: 1.1462 - acc: 1.0000 - val_loss: 2.9965 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1489 - acc: 1.000 - ETA: 2s - loss: 1.1509 - acc: 1.000 - ETA: 0s - loss: 1.1484 - acc: 1.000 - 8s 233ms/step - loss: 1.1507 - acc: 1.0000 - val_loss: 3.0003 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1459 - acc: 1.000 - ETA: 2s - loss: 1.1405 - acc: 1.000 - ETA: 0s - loss: 1.1497 - acc: 1.000 - 8s 234ms/step - loss: 1.2463 - acc: 0.9394 - val_loss: 3.0063 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1370 - acc: 1.000 - ETA: 2s - loss: 1.1414 - acc: 1.000 - ETA: 0s - loss: 1.1384 - acc: 1.000 - 8s 234ms/step - loss: 1.1382 - acc: 1.0000 - val_loss: 2.9973 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1305 - acc: 1.000 - ETA: 2s - loss: 1.1358 - acc: 1.000 - ETA: 0s - loss: 1.1356 - acc: 1.000 - 8s 236ms/step - loss: 1.1354 - acc: 1.0000 - val_loss: 2.9982 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1325 - acc: 1.000 - ETA: 2s - loss: 1.1382 - acc: 1.000 - ETA: 0s - loss: 1.1364 - acc: 1.000 - 8s 233ms/step - loss: 1.1446 - acc: 1.0000 - val_loss: 3.0065 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1972 - acc: 1.000 - ETA: 2s - loss: 1.1695 - acc: 1.000 - ETA: 0s - loss: 1.1586 - acc: 1.000 - 8s 236ms/step - loss: 1.1629 - acc: 1.0000 - val_loss: 3.0101 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1363 - acc: 1.000 - ETA: 2s - loss: 1.1328 - acc: 1.000 - ETA: 0s - loss: 1.1384 - acc: 1.000 - 8s 232ms/step - loss: 1.1387 - acc: 1.0000 - val_loss: 2.9711 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1288 - acc: 1.000 - ETA: 2s - loss: 1.1467 - acc: 1.000 - ETA: 0s - loss: 1.1434 - acc: 1.000 - 8s 231ms/step - loss: 1.1466 - acc: 1.0000 - val_loss: 2.8680 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1322 - acc: 1.000 - ETA: 2s - loss: 1.1305 - acc: 1.000 - ETA: 0s - loss: 1.1315 - acc: 1.000 - 7s 225ms/step - loss: 1.1357 - acc: 1.0000 - val_loss: 2.7730 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1327 - acc: 1.000 - ETA: 2s - loss: 1.1340 - acc: 1.000 - ETA: 0s - loss: 1.1336 - acc: 1.000 - 7s 224ms/step - loss: 1.2024 - acc: 0.9697 - val_loss: 2.6873 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1362 - acc: 1.000 - ETA: 2s - loss: 1.1436 - acc: 1.000 - ETA: 0s - loss: 1.1386 - acc: 1.000 - 7s 226ms/step - loss: 1.1387 - acc: 1.0000 - val_loss: 2.5764 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1349 - acc: 1.000 - ETA: 2s - loss: 1.1329 - acc: 1.000 - ETA: 0s - loss: 1.1310 - acc: 1.000 - 7s 223ms/step - loss: 1.1858 - acc: 0.9697 - val_loss: 2.5264 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1650 - acc: 1.000 - ETA: 2s - loss: 1.1566 - acc: 1.000 - ETA: 0s - loss: 1.1466 - acc: 1.000 - 8s 228ms/step - loss: 1.1456 - acc: 1.0000 - val_loss: 2.4845 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1320 - acc: 1.000 - ETA: 2s - loss: 1.1308 - acc: 1.000 - ETA: 0s - loss: 1.1308 - acc: 1.000 - 7s 224ms/step - loss: 1.2162 - acc: 0.9697 - val_loss: 2.4475 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1317 - acc: 1.000 - ETA: 2s - loss: 1.1313 - acc: 1.000 - ETA: 0s - loss: 1.1315 - acc: 1.000 - 7s 226ms/step - loss: 1.1328 - acc: 1.0000 - val_loss: 2.3874 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "33/33 [==============================] - ETA: 5s - loss: 1.1304 - acc: 1.000 - ETA: 2s - loss: 1.1552 - acc: 1.000 - ETA: 0s - loss: 1.1476 - acc: 1.000 - 8s 231ms/step - loss: 1.1485 - acc: 1.0000 - val_loss: 2.3397 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00049: val_loss improved from 2.35378 to 2.33966, saving model to imagemodel3.1.h5\n",
      "Epoch 50/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1333 - acc: 1.000 - ETA: 2s - loss: 1.1332 - acc: 1.000 - ETA: 0s - loss: 1.1308 - acc: 1.000 - 8s 231ms/step - loss: 1.1303 - acc: 1.0000 - val_loss: 2.3089 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00050: val_loss improved from 2.33966 to 2.30891, saving model to imagemodel3.1.h5\n",
      "Epoch 51/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1313 - acc: 1.000 - ETA: 2s - loss: 1.1394 - acc: 1.000 - ETA: 0s - loss: 1.1360 - acc: 1.000 - 7s 225ms/step - loss: 1.1354 - acc: 1.0000 - val_loss: 2.2754 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00051: val_loss improved from 2.30891 to 2.27542, saving model to imagemodel3.1.h5\n",
      "Epoch 52/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1241 - acc: 1.000 - ETA: 2s - loss: 1.1269 - acc: 1.000 - ETA: 0s - loss: 1.1401 - acc: 1.000 - 7s 226ms/step - loss: 1.2070 - acc: 0.9697 - val_loss: 2.2548 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00052: val_loss improved from 2.27542 to 2.25475, saving model to imagemodel3.1.h5\n",
      "Epoch 53/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1284 - acc: 1.000 - ETA: 2s - loss: 1.1381 - acc: 1.000 - ETA: 0s - loss: 1.1344 - acc: 1.000 - 7s 223ms/step - loss: 1.2566 - acc: 0.9697 - val_loss: 2.2408 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00053: val_loss improved from 2.25475 to 2.24077, saving model to imagemodel3.1.h5\n",
      "Epoch 54/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1295 - acc: 1.000 - ETA: 2s - loss: 1.1282 - acc: 1.000 - ETA: 0s - loss: 1.1293 - acc: 1.000 - 7s 226ms/step - loss: 1.1303 - acc: 1.0000 - val_loss: 2.2200 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00054: val_loss improved from 2.24077 to 2.21998, saving model to imagemodel3.1.h5\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - ETA: 4s - loss: 1.1273 - acc: 1.000 - ETA: 2s - loss: 1.1300 - acc: 1.000 - ETA: 0s - loss: 1.1323 - acc: 1.000 - 7s 226ms/step - loss: 1.1351 - acc: 1.0000 - val_loss: 2.2025 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00055: val_loss improved from 2.21998 to 2.20251, saving model to imagemodel3.1.h5\n",
      "Epoch 56/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1745 - acc: 1.000 - ETA: 2s - loss: 1.1903 - acc: 1.000 - ETA: 0s - loss: 1.1727 - acc: 1.000 - 8s 232ms/step - loss: 1.1695 - acc: 1.0000 - val_loss: 2.1893 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00056: val_loss improved from 2.20251 to 2.18933, saving model to imagemodel3.1.h5\n",
      "Epoch 57/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1295 - acc: 1.000 - ETA: 2s - loss: 1.1285 - acc: 1.000 - ETA: 0s - loss: 1.1290 - acc: 1.000 - 8s 230ms/step - loss: 1.1287 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00057: val_loss improved from 2.18933 to 2.17323, saving model to imagemodel3.1.h5\n",
      "Epoch 58/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2646 - acc: 0.900 - ETA: 2s - loss: 1.1946 - acc: 0.950 - ETA: 0s - loss: 1.1916 - acc: 0.966 - 7s 227ms/step - loss: 1.1893 - acc: 0.9697 - val_loss: 2.1647 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00058: val_loss improved from 2.17323 to 2.16468, saving model to imagemodel3.1.h5\n",
      "Epoch 59/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1654 - acc: 1.000 - ETA: 2s - loss: 1.1451 - acc: 1.000 - ETA: 0s - loss: 1.1417 - acc: 1.000 - 7s 223ms/step - loss: 1.2294 - acc: 0.9697 - val_loss: 2.1569 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00059: val_loss improved from 2.16468 to 2.15687, saving model to imagemodel3.1.h5\n",
      "Epoch 60/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1293 - acc: 1.000 - ETA: 2s - loss: 1.1300 - acc: 1.000 - ETA: 0s - loss: 1.1287 - acc: 1.000 - 7s 227ms/step - loss: 1.1290 - acc: 1.0000 - val_loss: 2.1354 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00060: val_loss improved from 2.15687 to 2.13544, saving model to imagemodel3.1.h5\n",
      "Epoch 61/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1303 - acc: 1.000 - ETA: 2s - loss: 1.1407 - acc: 1.000 - ETA: 0s - loss: 1.1375 - acc: 1.000 - 7s 224ms/step - loss: 1.2271 - acc: 0.9394 - val_loss: 2.1135 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00061: val_loss improved from 2.13544 to 2.11355, saving model to imagemodel3.1.h5\n",
      "Epoch 62/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1236 - acc: 1.000 - ETA: 2s - loss: 1.1258 - acc: 1.000 - ETA: 0s - loss: 1.1270 - acc: 1.000 - 7s 227ms/step - loss: 1.1320 - acc: 1.0000 - val_loss: 2.0798 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00062: val_loss improved from 2.11355 to 2.07975, saving model to imagemodel3.1.h5\n",
      "Epoch 63/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1225 - acc: 1.000 - ETA: 2s - loss: 1.1323 - acc: 1.000 - ETA: 0s - loss: 1.1307 - acc: 1.000 - 7s 224ms/step - loss: 1.1316 - acc: 1.0000 - val_loss: 2.0605 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00063: val_loss improved from 2.07975 to 2.06053, saving model to imagemodel3.1.h5\n",
      "Epoch 64/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1287 - acc: 1.000 - ETA: 2s - loss: 1.1278 - acc: 1.000 - ETA: 0s - loss: 1.1267 - acc: 1.000 - 7s 226ms/step - loss: 1.1473 - acc: 0.9697 - val_loss: 2.0520 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00064: val_loss improved from 2.06053 to 2.05197, saving model to imagemodel3.1.h5\n",
      "Epoch 65/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1355 - acc: 1.000 - ETA: 2s - loss: 1.1316 - acc: 1.000 - ETA: 0s - loss: 1.1300 - acc: 1.000 - 7s 224ms/step - loss: 1.1293 - acc: 1.0000 - val_loss: 2.0219 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00065: val_loss improved from 2.05197 to 2.02192, saving model to imagemodel3.1.h5\n",
      "Epoch 66/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1225 - acc: 1.000 - ETA: 2s - loss: 1.1336 - acc: 1.000 - ETA: 0s - loss: 1.1334 - acc: 1.000 - 8s 227ms/step - loss: 1.1340 - acc: 1.0000 - val_loss: 1.9990 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00066: val_loss improved from 2.02192 to 1.99897, saving model to imagemodel3.1.h5\n",
      "Epoch 67/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1560 - acc: 1.000 - ETA: 2s - loss: 1.1575 - acc: 1.000 - ETA: 0s - loss: 1.1461 - acc: 1.000 - 7s 225ms/step - loss: 1.1446 - acc: 1.0000 - val_loss: 1.9905 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00067: val_loss improved from 1.99897 to 1.99050, saving model to imagemodel3.1.h5\n",
      "Epoch 68/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1405 - acc: 1.000 - ETA: 2s - loss: 1.1337 - acc: 1.000 - ETA: 0s - loss: 1.1572 - acc: 1.000 - 8s 228ms/step - loss: 1.1552 - acc: 1.0000 - val_loss: 1.9749 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00068: val_loss improved from 1.99050 to 1.97485, saving model to imagemodel3.1.h5\n",
      "Epoch 69/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1323 - acc: 1.000 - ETA: 2s - loss: 1.1276 - acc: 1.000 - ETA: 0s - loss: 1.1291 - acc: 1.000 - 7s 226ms/step - loss: 1.1327 - acc: 1.0000 - val_loss: 1.9565 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.97485 to 1.95654, saving model to imagemodel3.1.h5\n",
      "Epoch 70/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1265 - acc: 1.000 - ETA: 2s - loss: 1.1252 - acc: 1.000 - ETA: 0s - loss: 1.1242 - acc: 1.000 - 8s 229ms/step - loss: 1.1803 - acc: 0.9394 - val_loss: 1.9499 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00070: val_loss improved from 1.95654 to 1.94992, saving model to imagemodel3.1.h5\n",
      "Epoch 71/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1215 - acc: 1.000 - ETA: 2s - loss: 1.1235 - acc: 1.000 - ETA: 0s - loss: 1.1248 - acc: 1.000 - 7s 224ms/step - loss: 1.1249 - acc: 1.0000 - val_loss: 1.9308 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00071: val_loss improved from 1.94992 to 1.93075, saving model to imagemodel3.1.h5\n",
      "Epoch 72/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1272 - acc: 1.000 - ETA: 2s - loss: 1.1270 - acc: 1.000 - ETA: 0s - loss: 1.1255 - acc: 1.000 - 8s 228ms/step - loss: 1.1269 - acc: 1.0000 - val_loss: 1.9204 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00072: val_loss improved from 1.93075 to 1.92039, saving model to imagemodel3.1.h5\n",
      "Epoch 73/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1226 - acc: 1.000 - ETA: 2s - loss: 1.1270 - acc: 1.000 - ETA: 0s - loss: 1.1261 - acc: 1.000 - 7s 223ms/step - loss: 1.1857 - acc: 0.9697 - val_loss: 1.9115 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00073: val_loss improved from 1.92039 to 1.91146, saving model to imagemodel3.1.h5\n",
      "Epoch 74/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1616 - acc: 1.000 - ETA: 2s - loss: 1.1447 - acc: 1.000 - ETA: 0s - loss: 1.1386 - acc: 1.000 - 8s 228ms/step - loss: 1.1915 - acc: 0.9697 - val_loss: 1.9036 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00074: val_loss improved from 1.91146 to 1.90362, saving model to imagemodel3.1.h5\n",
      "Epoch 75/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1199 - acc: 1.000 - ETA: 2s - loss: 1.1278 - acc: 1.000 - ETA: 0s - loss: 1.1279 - acc: 1.000 - 7s 224ms/step - loss: 1.1279 - acc: 1.0000 - val_loss: 1.8893 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00075: val_loss improved from 1.90362 to 1.88931, saving model to imagemodel3.1.h5\n",
      "Epoch 76/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1275 - acc: 1.000 - ETA: 2s - loss: 1.1240 - acc: 1.000 - ETA: 0s - loss: 1.1251 - acc: 1.000 - 7s 227ms/step - loss: 1.1257 - acc: 1.0000 - val_loss: 1.8755 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00076: val_loss improved from 1.88931 to 1.87550, saving model to imagemodel3.1.h5\n",
      "Epoch 77/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1199 - acc: 1.000 - ETA: 2s - loss: 1.1260 - acc: 1.000 - ETA: 0s - loss: 1.1263 - acc: 1.000 - 7s 224ms/step - loss: 1.1279 - acc: 1.0000 - val_loss: 1.8641 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00077: val_loss improved from 1.87550 to 1.86411, saving model to imagemodel3.1.h5\n",
      "Epoch 78/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1246 - acc: 1.000 - ETA: 2s - loss: 1.1325 - acc: 1.000 - ETA: 0s - loss: 1.1287 - acc: 1.000 - 7s 227ms/step - loss: 1.1286 - acc: 1.0000 - val_loss: 1.8586 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00078: val_loss improved from 1.86411 to 1.85857, saving model to imagemodel3.1.h5\n",
      "Epoch 79/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1260 - acc: 1.000 - ETA: 2s - loss: 1.1404 - acc: 1.000 - ETA: 0s - loss: 1.1348 - acc: 1.000 - 7s 225ms/step - loss: 1.1356 - acc: 1.0000 - val_loss: 1.8500 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00079: val_loss improved from 1.85857 to 1.84995, saving model to imagemodel3.1.h5\n",
      "Epoch 80/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1224 - acc: 1.000 - ETA: 2s - loss: 1.1228 - acc: 1.000 - ETA: 0s - loss: 1.1215 - acc: 1.000 - 8s 227ms/step - loss: 1.1234 - acc: 1.0000 - val_loss: 1.8425 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00080: val_loss improved from 1.84995 to 1.84246, saving model to imagemodel3.1.h5\n",
      "Epoch 81/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1266 - acc: 1.000 - ETA: 2s - loss: 1.1289 - acc: 1.000 - ETA: 0s - loss: 1.1271 - acc: 1.000 - 7s 224ms/step - loss: 1.1274 - acc: 1.0000 - val_loss: 1.8431 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1390 - acc: 1.000 - ETA: 2s - loss: 1.1595 - acc: 1.000 - ETA: 0s - loss: 1.1485 - acc: 1.000 - 7s 227ms/step - loss: 1.1474 - acc: 1.0000 - val_loss: 1.8453 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1203 - acc: 1.000 - ETA: 2s - loss: 1.1230 - acc: 1.000 - ETA: 0s - loss: 1.1268 - acc: 1.000 - 7s 224ms/step - loss: 1.1290 - acc: 1.0000 - val_loss: 1.8464 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1373 - acc: 1.000 - ETA: 2s - loss: 1.1313 - acc: 1.000 - ETA: 0s - loss: 1.1370 - acc: 1.000 - 7s 226ms/step - loss: 1.2106 - acc: 0.9394 - val_loss: 1.8469 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1296 - acc: 1.000 - ETA: 2s - loss: 1.1243 - acc: 1.000 - ETA: 0s - loss: 1.1278 - acc: 1.000 - 7s 225ms/step - loss: 1.1827 - acc: 0.9697 - val_loss: 1.8487 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1205 - acc: 1.000 - ETA: 2s - loss: 1.1238 - acc: 1.000 - ETA: 0s - loss: 1.1244 - acc: 1.000 - 7s 225ms/step - loss: 1.1243 - acc: 1.0000 - val_loss: 1.8485 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1208 - acc: 1.000 - ETA: 2s - loss: 1.1219 - acc: 1.000 - ETA: 0s - loss: 1.1252 - acc: 1.000 - 7s 226ms/step - loss: 1.2276 - acc: 0.9697 - val_loss: 1.8467 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1193 - acc: 1.000 - ETA: 2s - loss: 1.1201 - acc: 1.000 - ETA: 0s - loss: 1.1200 - acc: 1.000 - 7s 224ms/step - loss: 1.1257 - acc: 1.0000 - val_loss: 1.8520 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1234 - acc: 1.000 - ETA: 2s - loss: 1.1241 - acc: 1.000 - ETA: 0s - loss: 1.1228 - acc: 1.000 - 7s 227ms/step - loss: 1.1231 - acc: 1.0000 - val_loss: 1.8485 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1190 - acc: 1.000 - ETA: 2s - loss: 1.1337 - acc: 1.000 - ETA: 0s - loss: 1.1295 - acc: 1.000 - 7s 225ms/step - loss: 1.1288 - acc: 1.0000 - val_loss: 1.8538 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1621 - acc: 1.000 - ETA: 2s - loss: 1.1399 - acc: 1.000 - ETA: 0s - loss: 1.1359 - acc: 1.000 - 7s 227ms/step - loss: 1.1342 - acc: 1.0000 - val_loss: 1.8516 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1220 - acc: 1.000 - ETA: 2s - loss: 1.1245 - acc: 1.000 - ETA: 0s - loss: 1.1267 - acc: 1.000 - 7s 227ms/step - loss: 1.1268 - acc: 1.0000 - val_loss: 1.8489 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1237 - acc: 1.000 - ETA: 2s - loss: 1.1207 - acc: 1.000 - ETA: 0s - loss: 1.1226 - acc: 1.000 - 8s 228ms/step - loss: 1.1228 - acc: 1.0000 - val_loss: 1.8467 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1208 - acc: 1.000 - ETA: 2s - loss: 1.1222 - acc: 1.000 - ETA: 0s - loss: 1.1206 - acc: 1.000 - 7s 225ms/step - loss: 1.1217 - acc: 1.0000 - val_loss: 1.8434 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3365 - acc: 1.000 - ETA: 2s - loss: 1.3703 - acc: 0.950 - ETA: 0s - loss: 1.2867 - acc: 0.966 - 8s 231ms/step - loss: 1.3270 - acc: 0.9394 - val_loss: 1.8434 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1223 - acc: 1.000 - ETA: 2s - loss: 1.1241 - acc: 1.000 - ETA: 0s - loss: 1.1217 - acc: 1.000 - 7s 227ms/step - loss: 1.1214 - acc: 1.0000 - val_loss: 1.8503 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1195 - acc: 1.000 - ETA: 2s - loss: 1.1224 - acc: 1.000 - ETA: 0s - loss: 1.1236 - acc: 1.000 - 7s 227ms/step - loss: 1.1260 - acc: 1.0000 - val_loss: 1.8472 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1246 - acc: 1.000 - ETA: 2s - loss: 1.1221 - acc: 1.000 - ETA: 0s - loss: 1.1210 - acc: 1.000 - 7s 227ms/step - loss: 1.1283 - acc: 1.0000 - val_loss: 1.8388 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00098: val_loss improved from 1.84246 to 1.83884, saving model to imagemodel3.1.h5\n",
      "Epoch 99/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1232 - acc: 1.000 - ETA: 2s - loss: 1.1215 - acc: 1.000 - ETA: 0s - loss: 1.1199 - acc: 1.000 - 8s 229ms/step - loss: 1.1249 - acc: 1.0000 - val_loss: 1.8376 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00099: val_loss improved from 1.83884 to 1.83762, saving model to imagemodel3.1.h5\n",
      "Epoch 100/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1207 - acc: 1.000 - ETA: 2s - loss: 1.1219 - acc: 1.000 - ETA: 0s - loss: 1.1234 - acc: 1.000 - 7s 225ms/step - loss: 1.1229 - acc: 1.0000 - val_loss: 1.8348 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00100: val_loss improved from 1.83762 to 1.83477, saving model to imagemodel3.1.h5\n",
      "Epoch 101/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1228 - acc: 1.000 - ETA: 2s - loss: 1.1243 - acc: 1.000 - ETA: 0s - loss: 1.1272 - acc: 1.000 - 8s 228ms/step - loss: 1.1279 - acc: 1.0000 - val_loss: 1.8317 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00101: val_loss improved from 1.83477 to 1.83173, saving model to imagemodel3.1.h5\n",
      "Epoch 102/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1209 - acc: 1.000 - ETA: 2s - loss: 1.1224 - acc: 1.000 - ETA: 0s - loss: 1.1219 - acc: 1.000 - 7s 226ms/step - loss: 1.1253 - acc: 1.0000 - val_loss: 1.8272 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00102: val_loss improved from 1.83173 to 1.82720, saving model to imagemodel3.1.h5\n",
      "Epoch 103/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1280 - acc: 1.000 - ETA: 2s - loss: 1.1235 - acc: 1.000 - ETA: 0s - loss: 1.1234 - acc: 1.000 - 7s 227ms/step - loss: 1.1684 - acc: 1.0000 - val_loss: 1.8271 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00103: val_loss improved from 1.82720 to 1.82714, saving model to imagemodel3.1.h5\n",
      "Epoch 104/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1849 - acc: 1.000 - ETA: 2s - loss: 1.1533 - acc: 1.000 - ETA: 0s - loss: 1.1736 - acc: 1.000 - 7s 225ms/step - loss: 1.2471 - acc: 0.9697 - val_loss: 1.8317 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1875 - acc: 1.000 - ETA: 2s - loss: 1.1587 - acc: 1.000 - ETA: 0s - loss: 1.1555 - acc: 1.000 - 7s 226ms/step - loss: 1.1521 - acc: 1.0000 - val_loss: 1.8309 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - ETA: 4s - loss: 1.1264 - acc: 1.000 - ETA: 2s - loss: 1.1258 - acc: 1.000 - ETA: 0s - loss: 1.1225 - acc: 1.000 - 7s 226ms/step - loss: 1.1227 - acc: 1.0000 - val_loss: 1.8270 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00106: val_loss improved from 1.82714 to 1.82700, saving model to imagemodel3.1.h5\n",
      "Epoch 107/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1211 - acc: 1.000 - ETA: 2s - loss: 1.1378 - acc: 1.000 - ETA: 0s - loss: 1.1317 - acc: 1.000 - 7s 225ms/step - loss: 1.1320 - acc: 1.0000 - val_loss: 1.8195 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00107: val_loss improved from 1.82700 to 1.81950, saving model to imagemodel3.1.h5\n",
      "Epoch 108/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1324 - acc: 1.000 - ETA: 2s - loss: 1.1312 - acc: 1.000 - ETA: 0s - loss: 1.1289 - acc: 1.000 - 7s 224ms/step - loss: 1.1346 - acc: 1.0000 - val_loss: 1.8154 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00108: val_loss improved from 1.81950 to 1.81538, saving model to imagemodel3.1.h5\n",
      "Epoch 109/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1226 - acc: 1.000 - ETA: 2s - loss: 1.1201 - acc: 1.000 - ETA: 0s - loss: 1.1200 - acc: 1.000 - 7s 224ms/step - loss: 1.1218 - acc: 1.0000 - val_loss: 1.8076 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00109: val_loss improved from 1.81538 to 1.80760, saving model to imagemodel3.1.h5\n",
      "Epoch 110/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1258 - acc: 1.000 - ETA: 2s - loss: 1.1585 - acc: 1.000 - ETA: 0s - loss: 1.1461 - acc: 1.000 - 7s 224ms/step - loss: 1.1492 - acc: 1.0000 - val_loss: 1.8117 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 111/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1352 - acc: 1.000 - ETA: 2s - loss: 1.1403 - acc: 1.000 - ETA: 0s - loss: 1.1342 - acc: 1.000 - 7s 224ms/step - loss: 1.1350 - acc: 1.0000 - val_loss: 1.8114 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 112/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1239 - acc: 1.000 - ETA: 2s - loss: 1.1252 - acc: 1.000 - ETA: 0s - loss: 1.1246 - acc: 1.000 - 7s 226ms/step - loss: 1.1250 - acc: 1.0000 - val_loss: 1.8108 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 113/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1245 - acc: 1.000 - ETA: 2s - loss: 1.1453 - acc: 1.000 - ETA: 0s - loss: 1.1661 - acc: 1.000 - 7s 224ms/step - loss: 1.1617 - acc: 1.0000 - val_loss: 1.8134 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 114/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1252 - acc: 1.000 - ETA: 2s - loss: 1.1231 - acc: 1.000 - ETA: 0s - loss: 1.1217 - acc: 1.000 - 8s 228ms/step - loss: 1.1218 - acc: 1.0000 - val_loss: 1.8106 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      "Epoch 115/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1209 - acc: 1.000 - ETA: 2s - loss: 1.1237 - acc: 1.000 - ETA: 0s - loss: 1.1257 - acc: 1.000 - 7s 225ms/step - loss: 1.1248 - acc: 1.0000 - val_loss: 1.8125 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 116/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1492 - acc: 1.000 - ETA: 2s - loss: 1.1357 - acc: 1.000 - ETA: 0s - loss: 1.1294 - acc: 1.000 - 8s 234ms/step - loss: 1.1478 - acc: 1.0000 - val_loss: 1.8032 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00116: val_loss improved from 1.80760 to 1.80321, saving model to imagemodel3.1.h5\n",
      "Epoch 117/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1250 - acc: 1.000 - ETA: 2s - loss: 1.1224 - acc: 1.000 - ETA: 0s - loss: 1.1243 - acc: 1.000 - 8s 230ms/step - loss: 1.1246 - acc: 1.0000 - val_loss: 1.8041 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1207 - acc: 1.000 - ETA: 2s - loss: 1.1186 - acc: 1.000 - ETA: 0s - loss: 1.1194 - acc: 1.000 - 7s 227ms/step - loss: 1.1197 - acc: 1.0000 - val_loss: 1.7999 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00118: val_loss improved from 1.80321 to 1.79990, saving model to imagemodel3.1.h5\n",
      "Epoch 119/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2078 - acc: 1.000 - ETA: 2s - loss: 1.1676 - acc: 1.000 - ETA: 0s - loss: 1.1564 - acc: 1.000 - 7s 223ms/step - loss: 1.1542 - acc: 1.0000 - val_loss: 1.8020 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 120/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1964 - acc: 1.000 - ETA: 2s - loss: 1.1604 - acc: 1.000 - ETA: 0s - loss: 1.1472 - acc: 1.000 - 7s 226ms/step - loss: 1.1444 - acc: 1.0000 - val_loss: 1.8060 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1195 - acc: 1.000 - ETA: 2s - loss: 1.1201 - acc: 1.000 - ETA: 0s - loss: 1.1217 - acc: 1.000 - 7s 224ms/step - loss: 1.1216 - acc: 1.0000 - val_loss: 1.7988 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00121: val_loss improved from 1.79990 to 1.79883, saving model to imagemodel3.1.h5\n",
      "Epoch 122/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1271 - acc: 1.000 - ETA: 2s - loss: 1.1268 - acc: 1.000 - ETA: 0s - loss: 1.1231 - acc: 1.000 - 7s 227ms/step - loss: 1.1239 - acc: 1.0000 - val_loss: 1.7977 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00122: val_loss improved from 1.79883 to 1.79774, saving model to imagemodel3.1.h5\n",
      "Epoch 123/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1176 - acc: 1.000 - ETA: 2s - loss: 1.1193 - acc: 1.000 - ETA: 0s - loss: 1.1199 - acc: 1.000 - 7s 226ms/step - loss: 1.1210 - acc: 1.0000 - val_loss: 1.7957 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00123: val_loss improved from 1.79774 to 1.79570, saving model to imagemodel3.1.h5\n",
      "Epoch 124/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1228 - acc: 1.000 - ETA: 2s - loss: 1.1220 - acc: 1.000 - ETA: 0s - loss: 1.1217 - acc: 1.000 - 7s 227ms/step - loss: 1.1240 - acc: 1.0000 - val_loss: 1.7928 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00124: val_loss improved from 1.79570 to 1.79280, saving model to imagemodel3.1.h5\n",
      "Epoch 125/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1163 - acc: 1.000 - ETA: 2s - loss: 1.1312 - acc: 1.000 - ETA: 0s - loss: 1.1280 - acc: 1.000 - 7s 224ms/step - loss: 1.1705 - acc: 0.9697 - val_loss: 1.7907 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00125: val_loss improved from 1.79280 to 1.79067, saving model to imagemodel3.1.h5\n",
      "Epoch 126/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.3101 - acc: 1.000 - ETA: 2s - loss: 1.2330 - acc: 1.000 - ETA: 0s - loss: 1.1984 - acc: 1.000 - 7s 227ms/step - loss: 1.1932 - acc: 1.0000 - val_loss: 1.7969 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1193 - acc: 1.000 - ETA: 2s - loss: 1.1222 - acc: 1.000 - ETA: 0s - loss: 1.1217 - acc: 1.000 - 7s 224ms/step - loss: 1.1234 - acc: 1.0000 - val_loss: 1.7937 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1196 - acc: 1.000 - ETA: 2s - loss: 1.1194 - acc: 1.000 - ETA: 0s - loss: 1.1204 - acc: 1.000 - 7s 226ms/step - loss: 1.2208 - acc: 0.9697 - val_loss: 1.7893 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00128: val_loss improved from 1.79067 to 1.78934, saving model to imagemodel3.1.h5\n",
      "Epoch 129/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1174 - acc: 1.000 - ETA: 2s - loss: 1.1231 - acc: 1.000 - ETA: 0s - loss: 1.1261 - acc: 1.000 - 7s 225ms/step - loss: 1.1303 - acc: 1.0000 - val_loss: 1.7975 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1208 - acc: 1.000 - ETA: 2s - loss: 1.1475 - acc: 1.000 - ETA: 0s - loss: 1.1395 - acc: 1.000 - 7s 226ms/step - loss: 1.1419 - acc: 1.0000 - val_loss: 1.7932 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1270 - acc: 1.000 - ETA: 2s - loss: 1.1225 - acc: 1.000 - ETA: 0s - loss: 1.1231 - acc: 1.000 - 7s 226ms/step - loss: 1.1226 - acc: 1.0000 - val_loss: 1.7916 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 132/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1217 - acc: 1.000 - ETA: 2s - loss: 1.1211 - acc: 1.000 - ETA: 0s - loss: 1.1205 - acc: 1.000 - 8s 234ms/step - loss: 1.1922 - acc: 0.9697 - val_loss: 1.7898 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1189 - acc: 1.000 - ETA: 2s - loss: 1.1179 - acc: 1.000 - ETA: 0s - loss: 1.1209 - acc: 1.000 - 7s 226ms/step - loss: 1.1666 - acc: 0.9697 - val_loss: 1.7880 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00133: val_loss improved from 1.78934 to 1.78802, saving model to imagemodel3.1.h5\n",
      "Epoch 134/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1190 - acc: 1.000 - ETA: 2s - loss: 1.1181 - acc: 1.000 - ETA: 0s - loss: 1.1185 - acc: 1.000 - 8s 229ms/step - loss: 1.1198 - acc: 1.0000 - val_loss: 1.7865 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00134: val_loss improved from 1.78802 to 1.78648, saving model to imagemodel3.1.h5\n",
      "Epoch 135/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1212 - acc: 1.000 - ETA: 2s - loss: 1.1430 - acc: 1.000 - ETA: 0s - loss: 1.1341 - acc: 1.000 - 7s 226ms/step - loss: 1.1882 - acc: 0.9697 - val_loss: 1.7851 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00135: val_loss improved from 1.78648 to 1.78508, saving model to imagemodel3.1.h5\n",
      "Epoch 136/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1173 - acc: 1.000 - ETA: 2s - loss: 1.1200 - acc: 1.000 - ETA: 0s - loss: 1.1304 - acc: 1.000 - 7s 224ms/step - loss: 1.2120 - acc: 0.9697 - val_loss: 1.7828 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00136: val_loss improved from 1.78508 to 1.78276, saving model to imagemodel3.1.h5\n",
      "Epoch 137/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1285 - acc: 1.000 - ETA: 2s - loss: 1.1222 - acc: 1.000 - ETA: 0s - loss: 1.1204 - acc: 1.000 - 7s 224ms/step - loss: 1.1221 - acc: 1.0000 - val_loss: 1.7803 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00137: val_loss improved from 1.78276 to 1.78033, saving model to imagemodel3.1.h5\n",
      "Epoch 138/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1295 - acc: 1.000 - ETA: 2s - loss: 1.1236 - acc: 1.000 - ETA: 0s - loss: 1.1282 - acc: 1.000 - 7s 225ms/step - loss: 1.1283 - acc: 1.0000 - val_loss: 1.7837 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00138: val_loss did not improve\n",
      "Epoch 139/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1228 - acc: 1.000 - ETA: 2s - loss: 1.1335 - acc: 1.000 - ETA: 0s - loss: 1.1306 - acc: 1.000 - 7s 226ms/step - loss: 1.1305 - acc: 1.0000 - val_loss: 1.7831 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00139: val_loss did not improve\n",
      "Epoch 140/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1617 - acc: 1.000 - ETA: 2s - loss: 1.1412 - acc: 1.000 - ETA: 0s - loss: 1.1384 - acc: 1.000 - 7s 224ms/step - loss: 1.1399 - acc: 1.0000 - val_loss: 1.7811 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00140: val_loss did not improve\n",
      "Epoch 141/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1946 - acc: 1.000 - ETA: 2s - loss: 1.1556 - acc: 1.000 - ETA: 0s - loss: 1.1485 - acc: 1.000 - 7s 226ms/step - loss: 1.1467 - acc: 1.0000 - val_loss: 1.7783 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00141: val_loss improved from 1.78033 to 1.77827, saving model to imagemodel3.1.h5\n",
      "Epoch 142/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1212 - acc: 1.000 - ETA: 2s - loss: 1.1196 - acc: 1.000 - ETA: 0s - loss: 1.1239 - acc: 1.000 - 7s 224ms/step - loss: 1.1243 - acc: 1.0000 - val_loss: 1.7785 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 143/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1288 - acc: 1.000 - ETA: 2s - loss: 1.1240 - acc: 1.000 - ETA: 0s - loss: 1.1241 - acc: 1.000 - 7s 227ms/step - loss: 1.1261 - acc: 1.0000 - val_loss: 1.7744 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00143: val_loss improved from 1.77827 to 1.77438, saving model to imagemodel3.1.h5\n",
      "Epoch 144/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1477 - acc: 1.000 - ETA: 2s - loss: 1.1333 - acc: 1.000 - ETA: 0s - loss: 1.1273 - acc: 1.000 - 7s 224ms/step - loss: 1.1329 - acc: 1.0000 - val_loss: 1.7713 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00144: val_loss improved from 1.77438 to 1.77127, saving model to imagemodel3.1.h5\n",
      "Epoch 145/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1332 - acc: 1.000 - ETA: 2s - loss: 1.1334 - acc: 1.000 - ETA: 0s - loss: 1.1357 - acc: 1.000 - 7s 227ms/step - loss: 1.2475 - acc: 0.9394 - val_loss: 1.7725 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 146/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1154 - acc: 1.000 - ETA: 2s - loss: 1.1262 - acc: 1.000 - ETA: 0s - loss: 1.1508 - acc: 1.000 - 7s 225ms/step - loss: 1.2063 - acc: 0.9697 - val_loss: 1.7707 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00146: val_loss improved from 1.77127 to 1.77070, saving model to imagemodel3.1.h5\n",
      "Epoch 147/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1320 - acc: 1.000 - ETA: 2s - loss: 1.1247 - acc: 1.000 - ETA: 0s - loss: 1.1324 - acc: 1.000 - 7s 227ms/step - loss: 1.1323 - acc: 1.0000 - val_loss: 1.7727 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 148/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1883 - acc: 1.000 - ETA: 2s - loss: 1.1663 - acc: 1.000 - ETA: 0s - loss: 1.1534 - acc: 1.000 - 7s 226ms/step - loss: 1.1521 - acc: 1.0000 - val_loss: 1.7730 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00148: val_loss did not improve\n",
      "Epoch 149/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1313 - acc: 1.000 - ETA: 2s - loss: 1.1256 - acc: 1.000 - ETA: 0s - loss: 1.1252 - acc: 1.000 - 8s 227ms/step - loss: 1.1260 - acc: 1.0000 - val_loss: 1.7714 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00149: val_loss did not improve\n",
      "Epoch 150/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1245 - acc: 1.000 - ETA: 2s - loss: 1.1233 - acc: 1.000 - ETA: 0s - loss: 1.1225 - acc: 1.000 - 7s 225ms/step - loss: 1.1223 - acc: 1.0000 - val_loss: 1.7745 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      "Epoch 151/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1175 - acc: 1.000 - ETA: 2s - loss: 1.1219 - acc: 1.000 - ETA: 0s - loss: 1.1206 - acc: 1.000 - 7s 226ms/step - loss: 1.1218 - acc: 1.0000 - val_loss: 1.7727 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00151: val_loss did not improve\n",
      "Epoch 152/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1288 - acc: 1.000 - ETA: 2s - loss: 1.1323 - acc: 1.000 - ETA: 0s - loss: 1.1263 - acc: 1.000 - 7s 224ms/step - loss: 1.1261 - acc: 1.0000 - val_loss: 1.7731 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00152: val_loss did not improve\n",
      "Epoch 153/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2512 - acc: 1.000 - ETA: 2s - loss: 1.1932 - acc: 1.000 - ETA: 0s - loss: 1.1716 - acc: 1.000 - 7s 227ms/step - loss: 1.1675 - acc: 1.0000 - val_loss: 1.7743 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00153: val_loss did not improve\n",
      "Epoch 154/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1227 - acc: 1.000 - ETA: 2s - loss: 1.1220 - acc: 1.000 - ETA: 0s - loss: 1.1204 - acc: 1.000 - 7s 226ms/step - loss: 1.1213 - acc: 1.0000 - val_loss: 1.7722 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00154: val_loss did not improve\n",
      "Epoch 155/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1195 - acc: 1.000 - ETA: 2s - loss: 1.1180 - acc: 1.000 - ETA: 0s - loss: 1.1177 - acc: 1.000 - 8s 234ms/step - loss: 1.1182 - acc: 1.0000 - val_loss: 1.7709 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      "Epoch 156/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1228 - acc: 1.000 - ETA: 2s - loss: 1.1207 - acc: 1.000 - ETA: 0s - loss: 1.1263 - acc: 1.000 - 7s 227ms/step - loss: 1.1297 - acc: 1.0000 - val_loss: 1.7724 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 157/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1232 - acc: 1.000 - ETA: 2s - loss: 1.1223 - acc: 1.000 - ETA: 0s - loss: 1.1268 - acc: 1.000 - 7s 224ms/step - loss: 1.1639 - acc: 1.0000 - val_loss: 1.7739 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 158/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1392 - acc: 1.000 - ETA: 2s - loss: 1.1291 - acc: 1.000 - ETA: 0s - loss: 1.1257 - acc: 1.000 - 7s 226ms/step - loss: 1.1253 - acc: 1.0000 - val_loss: 1.7761 - val_acc: 0.7778\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00158: val_loss did not improve\n",
      "Epoch 159/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1203 - acc: 1.000 - ETA: 2s - loss: 1.1269 - acc: 1.000 - ETA: 0s - loss: 1.1234 - acc: 1.000 - 7s 224ms/step - loss: 1.1811 - acc: 0.9394 - val_loss: 1.7706 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00159: val_loss improved from 1.77070 to 1.77064, saving model to imagemodel3.1.h5\n",
      "Epoch 160/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1188 - acc: 1.000 - ETA: 2s - loss: 1.1176 - acc: 1.000 - ETA: 0s - loss: 1.1214 - acc: 1.000 - 7s 227ms/step - loss: 1.1213 - acc: 1.0000 - val_loss: 1.7703 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00160: val_loss improved from 1.77064 to 1.77028, saving model to imagemodel3.1.h5\n",
      "Epoch 161/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1227 - acc: 1.000 - ETA: 2s - loss: 1.1217 - acc: 1.000 - ETA: 0s - loss: 1.1200 - acc: 1.000 - 7s 224ms/step - loss: 1.1207 - acc: 1.0000 - val_loss: 1.7713 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 162/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1247 - acc: 1.000 - ETA: 2s - loss: 1.1238 - acc: 1.000 - ETA: 0s - loss: 1.1216 - acc: 1.000 - 8s 228ms/step - loss: 1.1245 - acc: 1.0000 - val_loss: 1.7747 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00162: val_loss did not improve\n",
      "Epoch 163/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1228 - acc: 1.000 - ETA: 2s - loss: 1.1224 - acc: 1.000 - ETA: 0s - loss: 1.1204 - acc: 1.000 - 7s 224ms/step - loss: 1.2315 - acc: 0.9697 - val_loss: 1.7783 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00163: val_loss did not improve\n",
      "Epoch 164/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1228 - acc: 1.000 - ETA: 2s - loss: 1.1212 - acc: 1.000 - ETA: 0s - loss: 1.1235 - acc: 1.000 - 8s 228ms/step - loss: 1.1230 - acc: 1.0000 - val_loss: 1.7787 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 165/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1240 - acc: 1.000 - ETA: 2s - loss: 1.1320 - acc: 1.000 - ETA: 0s - loss: 1.1341 - acc: 1.000 - 7s 225ms/step - loss: 1.1329 - acc: 1.0000 - val_loss: 1.7802 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00165: val_loss did not improve\n",
      "Epoch 166/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1193 - acc: 1.000 - ETA: 2s - loss: 1.1185 - acc: 1.000 - ETA: 0s - loss: 1.1193 - acc: 1.000 - 7s 227ms/step - loss: 1.1191 - acc: 1.0000 - val_loss: 1.7789 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00166: val_loss did not improve\n",
      "Epoch 167/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1344 - acc: 1.000 - ETA: 2s - loss: 1.1394 - acc: 1.000 - ETA: 0s - loss: 1.1328 - acc: 1.000 - 7s 226ms/step - loss: 1.1325 - acc: 1.0000 - val_loss: 1.7765 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00167: val_loss did not improve\n",
      "Epoch 168/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1235 - acc: 1.000 - ETA: 2s - loss: 1.1252 - acc: 1.000 - ETA: 0s - loss: 1.1229 - acc: 1.000 - 7s 226ms/step - loss: 1.1222 - acc: 1.0000 - val_loss: 1.7762 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00168: val_loss did not improve\n",
      "Epoch 169/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1174 - acc: 1.000 - ETA: 2s - loss: 1.1173 - acc: 1.000 - ETA: 0s - loss: 1.1272 - acc: 1.000 - 7s 225ms/step - loss: 1.1268 - acc: 1.0000 - val_loss: 1.7786 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00169: val_loss did not improve\n",
      "Epoch 170/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1280 - acc: 1.000 - ETA: 2s - loss: 1.1236 - acc: 1.000 - ETA: 0s - loss: 1.1220 - acc: 1.000 - 7s 226ms/step - loss: 1.1220 - acc: 1.0000 - val_loss: 1.7779 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00170: val_loss did not improve\n",
      "Epoch 171/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1378 - acc: 1.000 - ETA: 2s - loss: 1.1293 - acc: 1.000 - ETA: 0s - loss: 1.1266 - acc: 1.000 - 7s 226ms/step - loss: 1.1269 - acc: 1.0000 - val_loss: 1.7778 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00171: val_loss did not improve\n",
      "Epoch 172/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1513 - acc: 1.000 - ETA: 2s - loss: 1.1334 - acc: 1.000 - ETA: 0s - loss: 1.1515 - acc: 1.000 - 7s 225ms/step - loss: 1.1485 - acc: 1.0000 - val_loss: 1.7767 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00172: val_loss did not improve\n",
      "Epoch 173/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1225 - acc: 1.000 - ETA: 2s - loss: 1.1204 - acc: 1.000 - ETA: 0s - loss: 1.1186 - acc: 1.000 - 7s 227ms/step - loss: 1.1185 - acc: 1.0000 - val_loss: 1.7768 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00173: val_loss did not improve\n",
      "Epoch 174/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1172 - acc: 1.000 - ETA: 2s - loss: 1.1196 - acc: 1.000 - ETA: 0s - loss: 1.1189 - acc: 1.000 - 8s 229ms/step - loss: 1.1191 - acc: 1.0000 - val_loss: 1.7755 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00174: val_loss did not improve\n",
      "Epoch 175/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1188 - acc: 1.000 - ETA: 2s - loss: 1.1171 - acc: 1.000 - ETA: 0s - loss: 1.1180 - acc: 1.000 - 8s 227ms/step - loss: 1.1209 - acc: 1.0000 - val_loss: 1.7741 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00175: val_loss did not improve\n",
      "Epoch 176/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1207 - acc: 1.000 - ETA: 2s - loss: 1.1204 - acc: 1.000 - ETA: 0s - loss: 1.1192 - acc: 1.000 - 7s 224ms/step - loss: 1.1190 - acc: 1.0000 - val_loss: 1.7742 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00176: val_loss did not improve\n",
      "Epoch 177/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1163 - acc: 1.000 - ETA: 2s - loss: 1.1177 - acc: 1.000 - ETA: 0s - loss: 1.1260 - acc: 1.000 - 8s 228ms/step - loss: 1.1252 - acc: 1.0000 - val_loss: 1.7743 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00177: val_loss did not improve\n",
      "Epoch 178/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1176 - acc: 1.000 - ETA: 2s - loss: 1.1310 - acc: 1.000 - ETA: 0s - loss: 1.1302 - acc: 1.000 - 7s 225ms/step - loss: 1.1299 - acc: 1.0000 - val_loss: 1.7691 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00178: val_loss improved from 1.77028 to 1.76907, saving model to imagemodel3.1.h5\n",
      "Epoch 179/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1196 - acc: 1.000 - ETA: 2s - loss: 1.1182 - acc: 1.000 - ETA: 0s - loss: 1.1188 - acc: 1.000 - 8s 232ms/step - loss: 1.1197 - acc: 1.0000 - val_loss: 1.7654 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00179: val_loss improved from 1.76907 to 1.76537, saving model to imagemodel3.1.h5\n",
      "Epoch 180/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1350 - acc: 1.000 - ETA: 2s - loss: 1.1367 - acc: 1.000 - ETA: 0s - loss: 1.1307 - acc: 1.000 - 7s 227ms/step - loss: 1.1314 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00180: val_loss did not improve\n",
      "Epoch 181/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1179 - acc: 1.000 - ETA: 2s - loss: 1.1196 - acc: 1.000 - ETA: 0s - loss: 1.1200 - acc: 1.000 - 7s 226ms/step - loss: 1.1200 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00181: val_loss did not improve\n",
      "Epoch 182/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1433 - acc: 1.000 - ETA: 2s - loss: 1.1325 - acc: 1.000 - ETA: 0s - loss: 1.1282 - acc: 1.000 - 7s 225ms/step - loss: 1.1278 - acc: 1.0000 - val_loss: 1.7689 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00182: val_loss did not improve\n",
      "Epoch 183/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1326 - acc: 1.000 - ETA: 2s - loss: 1.1244 - acc: 1.000 - ETA: 0s - loss: 1.1261 - acc: 1.000 - 7s 227ms/step - loss: 1.1270 - acc: 1.0000 - val_loss: 1.7673 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00183: val_loss did not improve\n",
      "Epoch 184/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1178 - acc: 1.000 - ETA: 2s - loss: 1.1233 - acc: 1.000 - ETA: 0s - loss: 1.1360 - acc: 1.000 - 7s 224ms/step - loss: 1.3074 - acc: 0.9697 - val_loss: 1.7703 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00184: val_loss did not improve\n",
      "Epoch 185/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1414 - acc: 1.000 - ETA: 2s - loss: 1.1431 - acc: 1.000 - ETA: 0s - loss: 1.1364 - acc: 1.000 - 7s 226ms/step - loss: 1.1352 - acc: 1.0000 - val_loss: 1.7710 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00185: val_loss did not improve\n",
      "Epoch 186/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1196 - acc: 1.000 - ETA: 2s - loss: 1.1200 - acc: 1.000 - ETA: 0s - loss: 1.1217 - acc: 1.000 - 7s 225ms/step - loss: 1.1215 - acc: 1.0000 - val_loss: 1.7675 - val_acc: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00186: val_loss did not improve\n",
      "Epoch 187/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1173 - acc: 1.000 - ETA: 2s - loss: 1.1196 - acc: 1.000 - ETA: 0s - loss: 1.1266 - acc: 1.000 - 7s 227ms/step - loss: 1.1274 - acc: 1.0000 - val_loss: 1.7653 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00187: val_loss improved from 1.76537 to 1.76531, saving model to imagemodel3.1.h5\n",
      "Epoch 188/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1195 - acc: 1.000 - ETA: 2s - loss: 1.1186 - acc: 1.000 - ETA: 0s - loss: 1.1198 - acc: 1.000 - 7s 224ms/step - loss: 1.1892 - acc: 0.9394 - val_loss: 1.7660 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00188: val_loss did not improve\n",
      "Epoch 189/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1213 - acc: 1.000 - ETA: 2s - loss: 1.1239 - acc: 1.000 - ETA: 0s - loss: 1.1227 - acc: 1.000 - 7s 226ms/step - loss: 1.1231 - acc: 1.0000 - val_loss: 1.7712 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00189: val_loss did not improve\n",
      "Epoch 190/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1494 - acc: 1.000 - ETA: 2s - loss: 1.1324 - acc: 1.000 - ETA: 0s - loss: 1.1321 - acc: 1.000 - 7s 226ms/step - loss: 1.1308 - acc: 1.0000 - val_loss: 1.7697 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00190: val_loss did not improve\n",
      "Epoch 191/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.2237 - acc: 1.000 - ETA: 2s - loss: 1.1735 - acc: 1.000 - ETA: 0s - loss: 1.1615 - acc: 1.000 - 7s 224ms/step - loss: 1.1629 - acc: 1.0000 - val_loss: 1.7703 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00191: val_loss did not improve\n",
      "Epoch 192/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1151 - acc: 1.000 - ETA: 2s - loss: 1.1293 - acc: 1.000 - ETA: 0s - loss: 1.1271 - acc: 1.000 - 7s 226ms/step - loss: 1.1265 - acc: 1.0000 - val_loss: 1.7744 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00192: val_loss did not improve\n",
      "Epoch 193/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1404 - acc: 1.000 - ETA: 2s - loss: 1.1292 - acc: 1.000 - ETA: 0s - loss: 1.1264 - acc: 1.000 - 7s 224ms/step - loss: 1.1256 - acc: 1.0000 - val_loss: 1.7726 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00193: val_loss did not improve\n",
      "Epoch 194/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1179 - acc: 1.000 - ETA: 2s - loss: 1.1196 - acc: 1.000 - ETA: 0s - loss: 1.1193 - acc: 1.000 - 7s 227ms/step - loss: 1.1200 - acc: 1.0000 - val_loss: 1.7699 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00194: val_loss did not improve\n",
      "Epoch 195/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1273 - acc: 1.000 - ETA: 2s - loss: 1.1214 - acc: 1.000 - ETA: 0s - loss: 1.1199 - acc: 1.000 - 7s 224ms/step - loss: 1.1214 - acc: 1.0000 - val_loss: 1.7675 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00195: val_loss did not improve\n",
      "Epoch 196/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1176 - acc: 1.000 - ETA: 2s - loss: 1.1207 - acc: 1.000 - ETA: 0s - loss: 1.1193 - acc: 1.000 - 8s 228ms/step - loss: 1.1209 - acc: 1.0000 - val_loss: 1.7688 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00196: val_loss did not improve\n",
      "Epoch 197/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1196 - acc: 1.000 - ETA: 2s - loss: 1.1168 - acc: 1.000 - ETA: 0s - loss: 1.1168 - acc: 1.000 - 7s 225ms/step - loss: 1.1195 - acc: 1.0000 - val_loss: 1.7668 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00197: val_loss did not improve\n",
      "Epoch 198/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1184 - acc: 1.000 - ETA: 2s - loss: 1.1181 - acc: 1.000 - ETA: 0s - loss: 1.1183 - acc: 1.000 - 7s 227ms/step - loss: 1.1193 - acc: 1.0000 - val_loss: 1.7656 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00198: val_loss did not improve\n",
      "Epoch 199/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1157 - acc: 1.000 - ETA: 2s - loss: 1.1158 - acc: 1.000 - ETA: 0s - loss: 1.1194 - acc: 1.000 - 7s 223ms/step - loss: 1.1212 - acc: 1.0000 - val_loss: 1.7703 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00199: val_loss did not improve\n",
      "Epoch 200/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1235 - acc: 1.000 - ETA: 2s - loss: 1.1193 - acc: 1.000 - ETA: 0s - loss: 1.1213 - acc: 1.000 - 7s 226ms/step - loss: 1.1721 - acc: 0.9697 - val_loss: 1.7691 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00200: val_loss did not improve\n",
      "Epoch 201/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1182 - acc: 1.000 - ETA: 2s - loss: 1.1188 - acc: 1.000 - ETA: 0s - loss: 1.1207 - acc: 1.000 - 7s 225ms/step - loss: 1.1215 - acc: 1.0000 - val_loss: 1.7709 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00201: val_loss did not improve\n",
      "Epoch 202/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1229 - acc: 1.000 - ETA: 2s - loss: 1.1230 - acc: 1.000 - ETA: 0s - loss: 1.1319 - acc: 1.000 - 7s 227ms/step - loss: 1.1305 - acc: 1.0000 - val_loss: 1.7747 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00202: val_loss did not improve\n",
      "Epoch 203/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1198 - acc: 1.000 - ETA: 2s - loss: 1.1228 - acc: 1.000 - ETA: 0s - loss: 1.1202 - acc: 1.000 - 7s 225ms/step - loss: 1.1195 - acc: 1.0000 - val_loss: 1.7747 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00203: val_loss did not improve\n",
      "Epoch 204/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1213 - acc: 1.000 - ETA: 2s - loss: 1.1203 - acc: 1.000 - ETA: 0s - loss: 1.1188 - acc: 1.000 - 7s 226ms/step - loss: 1.1651 - acc: 0.9697 - val_loss: 1.7785 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00204: val_loss did not improve\n",
      "Epoch 205/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1219 - acc: 1.000 - ETA: 2s - loss: 1.1227 - acc: 1.000 - ETA: 0s - loss: 1.1221 - acc: 1.000 - 7s 226ms/step - loss: 1.1688 - acc: 1.0000 - val_loss: 1.7797 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00205: val_loss did not improve\n",
      "Epoch 206/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1323 - acc: 1.000 - ETA: 2s - loss: 1.1315 - acc: 1.000 - ETA: 0s - loss: 1.1312 - acc: 1.000 - 7s 224ms/step - loss: 1.1307 - acc: 1.0000 - val_loss: 1.7793 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00206: val_loss did not improve\n",
      "Epoch 207/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1169 - acc: 1.000 - ETA: 2s - loss: 1.1208 - acc: 1.000 - ETA: 0s - loss: 1.1251 - acc: 1.000 - 7s 227ms/step - loss: 1.2177 - acc: 0.9697 - val_loss: 1.7776 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00207: val_loss did not improve\n",
      "Epoch 208/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1171 - acc: 1.000 - ETA: 2s - loss: 1.1199 - acc: 1.000 - ETA: 0s - loss: 1.1191 - acc: 1.000 - 7s 225ms/step - loss: 1.1193 - acc: 1.0000 - val_loss: 1.7766 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00208: val_loss did not improve\n",
      "Epoch 209/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1232 - acc: 1.000 - ETA: 2s - loss: 1.1234 - acc: 1.000 - ETA: 0s - loss: 1.1204 - acc: 1.000 - 8s 227ms/step - loss: 1.1210 - acc: 1.0000 - val_loss: 1.7745 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00209: val_loss did not improve\n",
      "Epoch 210/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1188 - acc: 1.000 - ETA: 2s - loss: 1.1185 - acc: 1.000 - ETA: 0s - loss: 1.1178 - acc: 1.000 - 7s 224ms/step - loss: 1.1189 - acc: 1.0000 - val_loss: 1.7671 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00210: val_loss did not improve\n",
      "Epoch 211/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1209 - acc: 1.000 - ETA: 2s - loss: 1.1207 - acc: 1.000 - ETA: 0s - loss: 1.1200 - acc: 1.000 - 7s 227ms/step - loss: 1.1206 - acc: 1.0000 - val_loss: 1.7681 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00211: val_loss did not improve\n",
      "Epoch 212/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1217 - acc: 1.000 - ETA: 2s - loss: 1.1203 - acc: 1.000 - ETA: 0s - loss: 1.1210 - acc: 1.000 - 7s 227ms/step - loss: 1.1207 - acc: 1.0000 - val_loss: 1.7713 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00212: val_loss did not improve\n",
      "Epoch 213/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1203 - acc: 1.000 - ETA: 2s - loss: 1.1200 - acc: 1.000 - ETA: 0s - loss: 1.1189 - acc: 1.000 - 7s 226ms/step - loss: 1.1190 - acc: 1.0000 - val_loss: 1.7751 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00213: val_loss did not improve\n",
      "Epoch 214/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1151 - acc: 1.000 - ETA: 2s - loss: 1.1559 - acc: 1.000 - ETA: 0s - loss: 1.1473 - acc: 1.000 - 7s 224ms/step - loss: 1.2452 - acc: 0.9697 - val_loss: 1.7758 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00214: val_loss did not improve\n",
      "Epoch 215/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1295 - acc: 1.000 - ETA: 2s - loss: 1.1282 - acc: 1.000 - ETA: 0s - loss: 1.1243 - acc: 1.000 - 7s 227ms/step - loss: 1.1247 - acc: 1.0000 - val_loss: 1.7786 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00215: val_loss did not improve\n",
      "Epoch 216/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1190 - acc: 1.000 - ETA: 2s - loss: 1.1165 - acc: 1.000 - ETA: 0s - loss: 1.1166 - acc: 1.000 - 7s 225ms/step - loss: 1.1199 - acc: 1.0000 - val_loss: 1.7725 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00216: val_loss did not improve\n",
      "Epoch 217/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1223 - acc: 1.000 - ETA: 2s - loss: 1.1209 - acc: 1.000 - ETA: 0s - loss: 1.1196 - acc: 1.000 - 7s 227ms/step - loss: 1.1192 - acc: 1.0000 - val_loss: 1.7716 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00217: val_loss did not improve\n",
      "Epoch 218/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1152 - acc: 1.000 - ETA: 2s - loss: 1.1585 - acc: 1.000 - ETA: 0s - loss: 1.1506 - acc: 1.000 - 7s 224ms/step - loss: 1.1475 - acc: 1.0000 - val_loss: 1.7734 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00218: val_loss did not improve\n",
      "Epoch 219/300\n",
      "33/33 [==============================] - ETA: 4s - loss: 1.1237 - acc: 1.000 - ETA: 2s - loss: 1.1204 - acc: 1.000 - ETA: 0s - loss: 1.1193 - acc: 1.000 - 7s 225ms/step - loss: 1.1210 - acc: 1.0000 - val_loss: 1.7714 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00219: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              verbose=1,\n",
    "              validation_data=(X_val, Y_val),\n",
    "              shuffle=True,\n",
    "              callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=True,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=True,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=False,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=False,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        epochs=nb_epoch, verbose=1, max_q_size=100,\n",
    "                        callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "八、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.77199346e-04   9.99722779e-01]\n",
      " [  2.76910424e-01   7.23089516e-01]\n",
      " [  8.17259789e-01   1.82740182e-01]\n",
      " [  2.14008000e-02   9.78599191e-01]\n",
      " [  9.83236730e-01   1.67632420e-02]\n",
      " [  9.87119675e-01   1.28803859e-02]\n",
      " [  3.58404068e-05   9.99964118e-01]\n",
      " [  2.49444298e-03   9.97505605e-01]\n",
      " [  5.89365466e-03   9.94106293e-01]]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "model = load_model('imagemodel3.1.h5')\n",
    "pred=model.predict(X_val)\n",
    "print(pred)\n",
    "print(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 2)\n",
      "[1 1 0 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "prediction=np.argmax(pred,axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比如V1.0版本，这里把resnet18改成了resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=X_val[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapmaxmin(x):\n",
    "    MIN=np.min(x)\n",
    "    MAX=np.max(x)\n",
    "    return (x-MIN)/(MAX-MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "out=mapmaxmin(error)\n",
    "print(np.min(out))\n",
    "print(np.max(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/RFvQale8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231b6eb0be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
