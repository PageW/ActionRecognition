{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、import some package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils,plot_model\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mping\n",
    "import numpy as np\n",
    "import resnet\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import pylab\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n",
    "import skimage\n",
    "\n",
    "#from skimage import data, exposure\n",
    "from skimage.transform import resize\n",
    "#from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "import scipy\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input,BatchNormalization\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、读取图片进入X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "17997\n"
     ]
    }
   ],
   "source": [
    "filename = r'C:\\Users\\wangpeizhi\\Desktop\\video2.mov'\n",
    "vid = imageio.get_reader(filename,  'ffmpeg') \n",
    "for im in enumerate(vid):\n",
    "    if(im[0]%100==0):\n",
    "        x=im[0]\n",
    "        print(x)\n",
    "    length=im[0]+1\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997\n"
     ]
    }
   ],
   "source": [
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "img_rows=720\n",
    "img_cols=1280\n",
    "img_channels=3\n",
    "nb_classes=10\n",
    "m=400\n",
    "\n",
    "filename = r'C:\\Users\\wangpeizhi\\Desktop\\video2.mov'\n",
    "vid = imageio.get_reader(filename,  'ffmpeg') \n",
    "X=np.zeros([m,img_rows,img_cols,img_channels])\n",
    "Y=np.zeros([m,nb_classes])\n",
    "indice=0\n",
    "for im in enumerate(vid):\n",
    "    if(im[0]%45==0)&(indice<m):\n",
    "        img=im[1]\n",
    "        #image = resize(img, (img_rows,img_cols), mode='constant', preserve_range=True)\n",
    "        X[indice,:,:,:]=img\n",
    "        indice +=1\n",
    "        print(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(43):\n",
    "    Y[j,0]=1\n",
    "for j in range(43,82):\n",
    "    Y[j,1]=1\n",
    "for j in range(82,121):\n",
    "    Y[j,2]=1\n",
    "for j in range(121,161):\n",
    "    Y[j,3]=1\n",
    "for j in range(161,202):\n",
    "    Y[j,4]=1\n",
    "for j in range(202,243):\n",
    "    Y[j,5]=1\n",
    "for j in range(243,280):\n",
    "    Y[j,6]=1\n",
    "for j in range(280,320):\n",
    "    Y[j,7]=1\n",
    "Y[320,0]=1\n",
    "Y[321,1]=1\n",
    "for j in range(322,361):\n",
    "    Y[j,8]=1\n",
    "for j in range(361,400):\n",
    "    Y[j,9]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、网络的一些超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=10, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.0005, patience=40)\n",
    "csv_logger = CSVLogger('imagemodel4.1.csv')\n",
    "checkpointer = ModelCheckpoint('imagemodel4.1.h5', verbose=1, save_best_only=True)\n",
    "batch_size = 10\n",
    "nb_epoch = 300\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、预处理，白化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.astype('float32')\n",
    "mean_X=np.mean(X,axis=0)\n",
    "X -= mean_X  #白化，X减去所有数据的均值\n",
    "X /=128 #这样X大概就是在-1~1之间的分布了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五、搭建网络模型，编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 720, 1280, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 360, 640, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 360, 640, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 360, 640, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 180, 320, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 180, 320, 64) 36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 180, 320, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 180, 320, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 180, 320, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 180, 320, 64) 0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 180, 320, 64) 256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 180, 320, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 180, 320, 64) 36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 180, 320, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 180, 320, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 180, 320, 64) 36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 180, 320, 64) 0           add_1[0][0]                      \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 180, 320, 64) 256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 180, 320, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 180, 320, 64) 36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 180, 320, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 180, 320, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 180, 320, 64) 36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 180, 320, 64) 0           add_2[0][0]                      \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 180, 320, 64) 256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 180, 320, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 90, 160, 128) 73856       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 90, 160, 128) 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 90, 160, 128) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 90, 160, 128) 8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 90, 160, 128) 147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 90, 160, 128) 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 90, 160, 128) 512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 90, 160, 128) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 90, 160, 128) 147584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 90, 160, 128) 512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 90, 160, 128) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 90, 160, 128) 147584      activation_10[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 90, 160, 128) 0           add_4[0][0]                      \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 90, 160, 128) 512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 90, 160, 128) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 90, 160, 128) 147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 90, 160, 128) 512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 90, 160, 128) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 90, 160, 128) 147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 90, 160, 128) 0           add_5[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 90, 160, 128) 512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 90, 160, 128) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 90, 160, 128) 147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 90, 160, 128) 512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 90, 160, 128) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 90, 160, 128) 147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 90, 160, 128) 0           add_6[0][0]                      \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 90, 160, 128) 512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 90, 160, 128) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 45, 80, 256)  295168      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 45, 80, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 45, 80, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 45, 80, 256)  33024       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 45, 80, 256)  590080      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 45, 80, 256)  0           conv2d_19[0][0]                  \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 45, 80, 256)  1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 45, 80, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 45, 80, 256)  590080      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 45, 80, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 45, 80, 256)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 45, 80, 256)  590080      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 45, 80, 256)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 45, 80, 256)  1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 45, 80, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 45, 80, 256)  590080      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 45, 80, 256)  1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 45, 80, 256)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 45, 80, 256)  590080      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 45, 80, 256)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 45, 80, 256)  1024        add_10[0][0]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 45, 80, 256)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 45, 80, 256)  590080      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 45, 80, 256)  1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 45, 80, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 45, 80, 256)  590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 45, 80, 256)  0           add_10[0][0]                     \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 45, 80, 256)  1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 45, 80, 256)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 45, 80, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 45, 80, 256)  1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 45, 80, 256)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 45, 80, 256)  590080      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 45, 80, 256)  0           add_11[0][0]                     \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 45, 80, 256)  1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 45, 80, 256)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 45, 80, 256)  590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 45, 80, 256)  1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 45, 80, 256)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 45, 80, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 45, 80, 256)  0           add_12[0][0]                     \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 45, 80, 256)  1024        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 45, 80, 256)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 23, 40, 512)  1180160     activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 23, 40, 512)  2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 23, 40, 512)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 23, 40, 512)  131584      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 23, 40, 512)  2359808     activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 23, 40, 512)  0           conv2d_32[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 23, 40, 512)  2048        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 23, 40, 512)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 23, 40, 512)  2359808     activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 23, 40, 512)  2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 23, 40, 512)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 23, 40, 512)  2359808     activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 23, 40, 512)  0           add_14[0][0]                     \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 23, 40, 512)  2048        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 23, 40, 512)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 23, 40, 512)  2359808     activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_32 (BatchNo (None, 23, 40, 512)  2048        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 23, 40, 512)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 23, 40, 512)  2359808     activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 23, 40, 512)  0           add_15[0][0]                     \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 23, 40, 512)  2048        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 23, 40, 512)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           5130        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,311,754\n",
      "Trainable params: 21,296,522\n",
      "Non-trainable params: 15,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet.ResnetBuilder.build_resnet_34((img_channels, img_rows,img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model V2.0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "六、分割X={X_train,X_val}  Y={Y_train,Y_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= (340, 720, 1280, 3)\n",
      "Y_train.shape= (340, 10)\n",
      "X_val.shape= (60, 720, 1280, 3)\n",
      "Y_val.shape= (60, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"Y_train.shape=\",Y_train.shape)\n",
    "print(\"X_val.shape=\",X_val.shape)\n",
    "print(\"Y_val.shape=\",Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "七、数据增强，训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 340 samples, validate on 60 samples\n",
      "Epoch 1/300\n",
      "340/340 [==============================] - ETA: 44:28 - loss: 3.8145 - acc: 0.0000e+ - ETA: 40:48 - loss: 3.6070 - acc: 0.1500   - ETA: 38:57 - loss: 3.5166 - acc: 0.26 - ETA: 38:02 - loss: 3.5476 - acc: 0.27 - ETA: 36:15 - loss: 3.4696 - acc: 0.30 - ETA: 34:41 - loss: 3.4652 - acc: 0.30 - ETA: 33:13 - loss: 3.4516 - acc: 0.30 - ETA: 31:52 - loss: 3.4399 - acc: 0.30 - ETA: 30:32 - loss: 3.4009 - acc: 0.30 - ETA: 29:13 - loss: 3.3719 - acc: 0.31 - ETA: 27:54 - loss: 3.3639 - acc: 0.30 - ETA: 26:38 - loss: 3.3679 - acc: 0.29 - ETA: 25:22 - loss: 3.3398 - acc: 0.30 - ETA: 24:06 - loss: 3.3601 - acc: 0.28 - ETA: 22:51 - loss: 3.3725 - acc: 0.26 - ETA: 21:37 - loss: 3.3623 - acc: 0.27 - ETA: 20:27 - loss: 3.3392 - acc: 0.28 - ETA: 19:13 - loss: 3.3096 - acc: 0.29 - ETA: 17:59 - loss: 3.3048 - acc: 0.29 - ETA: 16:46 - loss: 3.2866 - acc: 0.30 - ETA: 15:33 - loss: 3.2856 - acc: 0.30 - ETA: 14:21 - loss: 3.2851 - acc: 0.29 - ETA: 13:08 - loss: 3.2859 - acc: 0.29 - ETA: 11:56 - loss: 3.2848 - acc: 0.29 - ETA: 10:44 - loss: 3.2720 - acc: 0.30 - ETA: 9:32 - loss: 3.2472 - acc: 0.3077 - ETA: 8:20 - loss: 3.2302 - acc: 0.314 - ETA: 7:08 - loss: 3.2119 - acc: 0.321 - ETA: 5:57 - loss: 3.2038 - acc: 0.324 - ETA: 4:46 - loss: 3.1890 - acc: 0.340 - ETA: 3:34 - loss: 3.1713 - acc: 0.341 - ETA: 2:23 - loss: 3.1542 - acc: 0.346 - ETA: 1:11 - loss: 3.1422 - acc: 0.354 - 2578s 8s/step - loss: 3.1145 - acc: 0.3676 - val_loss: 5.2134 - val_acc: 0.2167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.21344, saving model to imagemodel4.1.h5\n",
      "Epoch 2/300\n",
      "340/340 [==============================] - ETA: 40:36 - loss: 2.3220 - acc: 0.80 - ETA: 39:14 - loss: 2.5177 - acc: 0.65 - ETA: 37:59 - loss: 2.6952 - acc: 0.53 - ETA: 36:41 - loss: 2.5550 - acc: 0.60 - ETA: 35:26 - loss: 2.5591 - acc: 0.56 - ETA: 34:13 - loss: 2.5799 - acc: 0.55 - ETA: 32:59 - loss: 2.5174 - acc: 0.60 - ETA: 32:16 - loss: 2.5264 - acc: 0.57 - ETA: 31:18 - loss: 2.5568 - acc: 0.57 - ETA: 30:25 - loss: 2.5513 - acc: 0.57 - ETA: 29:26 - loss: 2.5765 - acc: 0.57 - ETA: 28:20 - loss: 2.5588 - acc: 0.58 - ETA: 27:11 - loss: 2.5510 - acc: 0.58 - ETA: 25:57 - loss: 2.5666 - acc: 0.57 - ETA: 24:41 - loss: 2.5512 - acc: 0.58 - ETA: 23:26 - loss: 2.5446 - acc: 0.58 - ETA: 22:07 - loss: 2.5282 - acc: 0.59 - ETA: 20:43 - loss: 2.5272 - acc: 0.58 - ETA: 19:21 - loss: 2.5077 - acc: 0.59 - ETA: 18:00 - loss: 2.5143 - acc: 0.58 - ETA: 16:40 - loss: 2.5246 - acc: 0.57 - ETA: 15:21 - loss: 2.5226 - acc: 0.57 - ETA: 14:02 - loss: 2.5315 - acc: 0.56 - ETA: 12:44 - loss: 2.5197 - acc: 0.56 - ETA: 11:26 - loss: 2.5020 - acc: 0.57 - ETA: 10:08 - loss: 2.4999 - acc: 0.57 - ETA: 8:52 - loss: 2.5030 - acc: 0.5704 - ETA: 7:37 - loss: 2.4979 - acc: 0.567 - ETA: 6:20 - loss: 2.4988 - acc: 0.565 - ETA: 5:03 - loss: 2.4940 - acc: 0.566 - ETA: 3:47 - loss: 2.4734 - acc: 0.577 - ETA: 2:31 - loss: 2.4643 - acc: 0.578 - ETA: 1:15 - loss: 2.4586 - acc: 0.584 - 2709s 8s/step - loss: 2.4492 - acc: 0.5882 - val_loss: 3.7638 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.21344 to 3.76376, saving model to imagemodel4.1.h5\n",
      "Epoch 3/300\n",
      "340/340 [==============================] - ETA: 39:11 - loss: 1.9123 - acc: 0.90 - ETA: 37:58 - loss: 2.0597 - acc: 0.70 - ETA: 36:50 - loss: 1.9885 - acc: 0.73 - ETA: 35:37 - loss: 2.0480 - acc: 0.67 - ETA: 34:25 - loss: 2.0494 - acc: 0.70 - ETA: 33:15 - loss: 2.1508 - acc: 0.66 - ETA: 32:06 - loss: 2.2027 - acc: 0.64 - ETA: 30:56 - loss: 2.1974 - acc: 0.63 - ETA: 29:47 - loss: 2.2151 - acc: 0.61 - ETA: 28:37 - loss: 2.1782 - acc: 0.62 - ETA: 27:27 - loss: 2.1737 - acc: 0.61 - ETA: 26:15 - loss: 2.1707 - acc: 0.61 - ETA: 25:03 - loss: 2.1669 - acc: 0.61 - ETA: 23:52 - loss: 2.1801 - acc: 0.61 - ETA: 22:41 - loss: 2.1727 - acc: 0.63 - ETA: 21:29 - loss: 2.1563 - acc: 0.65 - ETA: 20:18 - loss: 2.1437 - acc: 0.65 - ETA: 19:06 - loss: 2.1547 - acc: 0.65 - ETA: 17:55 - loss: 2.1239 - acc: 0.67 - ETA: 16:43 - loss: 2.1046 - acc: 0.69 - ETA: 15:32 - loss: 2.0843 - acc: 0.69 - ETA: 14:22 - loss: 2.0724 - acc: 0.69 - ETA: 13:11 - loss: 2.0617 - acc: 0.69 - ETA: 11:59 - loss: 2.0622 - acc: 0.68 - ETA: 10:47 - loss: 2.0557 - acc: 0.68 - ETA: 9:35 - loss: 2.0526 - acc: 0.6885 - ETA: 8:23 - loss: 2.0602 - acc: 0.681 - ETA: 7:11 - loss: 2.0582 - acc: 0.685 - ETA: 5:59 - loss: 2.0535 - acc: 0.686 - ETA: 4:47 - loss: 2.0554 - acc: 0.686 - ETA: 3:35 - loss: 2.0573 - acc: 0.677 - ETA: 2:23 - loss: 2.0610 - acc: 0.675 - ETA: 1:11 - loss: 2.0589 - acc: 0.675 - 2587s 8s/step - loss: 2.0533 - acc: 0.6735 - val_loss: 4.4070 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "340/340 [==============================] - ETA: 39:45 - loss: 2.2667 - acc: 0.40 - ETA: 38:39 - loss: 1.9870 - acc: 0.60 - ETA: 37:25 - loss: 1.9851 - acc: 0.53 - ETA: 36:13 - loss: 1.9499 - acc: 0.52 - ETA: 35:05 - loss: 2.0407 - acc: 0.48 - ETA: 33:55 - loss: 1.9664 - acc: 0.55 - ETA: 32:42 - loss: 1.9572 - acc: 0.55 - ETA: 31:28 - loss: 1.9057 - acc: 0.60 - ETA: 30:16 - loss: 1.9380 - acc: 0.60 - ETA: 29:03 - loss: 1.9220 - acc: 0.63 - ETA: 27:56 - loss: 1.9158 - acc: 0.64 - ETA: 26:43 - loss: 1.8946 - acc: 0.65 - ETA: 25:29 - loss: 1.8905 - acc: 0.65 - ETA: 24:16 - loss: 1.8958 - acc: 0.64 - ETA: 23:02 - loss: 1.8741 - acc: 0.65 - ETA: 21:49 - loss: 1.8535 - acc: 0.66 - ETA: 20:35 - loss: 1.8568 - acc: 0.65 - ETA: 19:22 - loss: 1.8568 - acc: 0.66 - ETA: 18:10 - loss: 1.8630 - acc: 0.66 - ETA: 16:57 - loss: 1.8636 - acc: 0.66 - ETA: 15:44 - loss: 1.8661 - acc: 0.66 - ETA: 14:32 - loss: 1.8833 - acc: 0.65 - ETA: 13:19 - loss: 1.8747 - acc: 0.65 - ETA: 12:06 - loss: 1.8675 - acc: 0.65 - ETA: 10:54 - loss: 1.8707 - acc: 0.65 - ETA: 9:42 - loss: 1.8541 - acc: 0.6615 - ETA: 8:29 - loss: 1.8496 - acc: 0.659 - ETA: 7:17 - loss: 1.8455 - acc: 0.664 - ETA: 6:04 - loss: 1.8501 - acc: 0.658 - ETA: 4:51 - loss: 1.8499 - acc: 0.660 - ETA: 3:38 - loss: 1.8620 - acc: 0.654 - ETA: 2:25 - loss: 1.8492 - acc: 0.656 - ETA: 1:12 - loss: 1.8546 - acc: 0.648 - 2616s 8s/step - loss: 1.8489 - acc: 0.6500 - val_loss: 8.8963 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "340/340 [==============================] - ETA: 40:33 - loss: 1.7865 - acc: 0.50 - ETA: 39:07 - loss: 2.0064 - acc: 0.50 - ETA: 37:43 - loss: 1.8833 - acc: 0.53 - ETA: 36:25 - loss: 1.7809 - acc: 0.62 - ETA: 35:10 - loss: 1.7532 - acc: 0.64 - ETA: 34:00 - loss: 1.7111 - acc: 0.65 - ETA: 32:45 - loss: 1.7233 - acc: 0.62 - ETA: 31:34 - loss: 1.6871 - acc: 0.67 - ETA: 30:22 - loss: 1.7186 - acc: 0.64 - ETA: 29:10 - loss: 1.7227 - acc: 0.65 - ETA: 27:57 - loss: 1.7136 - acc: 0.66 - ETA: 26:47 - loss: 1.6733 - acc: 0.68 - ETA: 25:34 - loss: 1.6610 - acc: 0.69 - ETA: 24:21 - loss: 1.6474 - acc: 0.70 - ETA: 23:07 - loss: 1.6342 - acc: 0.70 - ETA: 21:54 - loss: 1.6221 - acc: 0.70 - ETA: 20:41 - loss: 1.6094 - acc: 0.71 - ETA: 19:28 - loss: 1.6126 - acc: 0.71 - ETA: 18:15 - loss: 1.6060 - acc: 0.72 - ETA: 17:02 - loss: 1.6073 - acc: 0.72 - ETA: 15:48 - loss: 1.6340 - acc: 0.71 - ETA: 14:35 - loss: 1.6248 - acc: 0.71 - ETA: 13:22 - loss: 1.6192 - acc: 0.72 - ETA: 12:09 - loss: 1.6133 - acc: 0.72 - ETA: 10:57 - loss: 1.6180 - acc: 0.72 - ETA: 9:44 - loss: 1.6174 - acc: 0.7231 - ETA: 8:31 - loss: 1.6191 - acc: 0.725 - ETA: 7:18 - loss: 1.6074 - acc: 0.732 - ETA: 6:05 - loss: 1.6130 - acc: 0.727 - ETA: 4:52 - loss: 1.6098 - acc: 0.723 - ETA: 3:39 - loss: 1.6121 - acc: 0.719 - ETA: 2:26 - loss: 1.6082 - acc: 0.718 - ETA: 1:13 - loss: 1.6106 - acc: 0.718 - 2621s 8s/step - loss: 1.6021 - acc: 0.7176 - val_loss: 3.8801 - val_acc: 0.3667\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 41:04 - loss: 1.2715 - acc: 0.80 - ETA: 39:19 - loss: 1.5751 - acc: 0.75 - ETA: 37:54 - loss: 1.5109 - acc: 0.76 - ETA: 36:38 - loss: 1.4579 - acc: 0.77 - ETA: 35:21 - loss: 1.4192 - acc: 0.78 - ETA: 34:05 - loss: 1.4706 - acc: 0.75 - ETA: 32:51 - loss: 1.4567 - acc: 0.74 - ETA: 31:37 - loss: 1.4956 - acc: 0.75 - ETA: 30:23 - loss: 1.5565 - acc: 0.71 - ETA: 29:11 - loss: 1.5796 - acc: 0.70 - ETA: 27:57 - loss: 1.5988 - acc: 0.70 - ETA: 26:45 - loss: 1.5861 - acc: 0.70 - ETA: 25:32 - loss: 1.5657 - acc: 0.71 - ETA: 24:19 - loss: 1.5552 - acc: 0.72 - ETA: 23:06 - loss: 1.5409 - acc: 0.72 - ETA: 21:53 - loss: 1.5388 - acc: 0.73 - ETA: 20:39 - loss: 1.5261 - acc: 0.73 - ETA: 19:26 - loss: 1.5044 - acc: 0.75 - ETA: 18:13 - loss: 1.5158 - acc: 0.74 - ETA: 17:00 - loss: 1.5038 - acc: 0.75 - ETA: 15:47 - loss: 1.5075 - acc: 0.74 - ETA: 14:34 - loss: 1.5184 - acc: 0.73 - ETA: 13:21 - loss: 1.5182 - acc: 0.73 - ETA: 12:08 - loss: 1.5199 - acc: 0.72 - ETA: 10:55 - loss: 1.5207 - acc: 0.73 - ETA: 9:43 - loss: 1.5136 - acc: 0.7308 - ETA: 8:30 - loss: 1.5110 - acc: 0.733 - ETA: 7:17 - loss: 1.5064 - acc: 0.739 - ETA: 6:04 - loss: 1.5145 - acc: 0.734 - ETA: 4:51 - loss: 1.5061 - acc: 0.743 - ETA: 3:38 - loss: 1.5062 - acc: 0.738 - ETA: 2:25 - loss: 1.5002 - acc: 0.743 - ETA: 1:12 - loss: 1.4935 - acc: 0.748 - 2618s 8s/step - loss: 1.4854 - acc: 0.7500 - val_loss: 3.9148 - val_acc: 0.2667\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "340/340 [==============================] - ETA: 39:43 - loss: 1.5916 - acc: 0.50 - ETA: 38:46 - loss: 1.4544 - acc: 0.65 - ETA: 37:44 - loss: 1.4970 - acc: 0.73 - ETA: 36:28 - loss: 1.4003 - acc: 0.80 - ETA: 35:14 - loss: 1.3866 - acc: 0.80 - ETA: 34:00 - loss: 1.3703 - acc: 0.81 - ETA: 32:45 - loss: 1.3207 - acc: 0.84 - ETA: 31:30 - loss: 1.3082 - acc: 0.86 - ETA: 30:19 - loss: 1.2777 - acc: 0.86 - ETA: 29:08 - loss: 1.2738 - acc: 0.87 - ETA: 27:56 - loss: 1.2591 - acc: 0.87 - ETA: 26:42 - loss: 1.2422 - acc: 0.87 - ETA: 25:29 - loss: 1.2541 - acc: 0.87 - ETA: 24:17 - loss: 1.2623 - acc: 0.87 - ETA: 23:09 - loss: 1.2546 - acc: 0.86 - ETA: 21:56 - loss: 1.2594 - acc: 0.86 - ETA: 20:43 - loss: 1.2533 - acc: 0.86 - ETA: 19:30 - loss: 1.2661 - acc: 0.86 - ETA: 18:16 - loss: 1.2704 - acc: 0.85 - ETA: 17:03 - loss: 1.2651 - acc: 0.86 - ETA: 15:50 - loss: 1.2558 - acc: 0.86 - ETA: 14:36 - loss: 1.2839 - acc: 0.85 - ETA: 13:23 - loss: 1.3038 - acc: 0.83 - ETA: 12:10 - loss: 1.2970 - acc: 0.83 - ETA: 10:57 - loss: 1.2890 - acc: 0.83 - ETA: 9:44 - loss: 1.2926 - acc: 0.8269 - ETA: 8:31 - loss: 1.2940 - acc: 0.822 - ETA: 7:18 - loss: 1.2910 - acc: 0.825 - ETA: 6:05 - loss: 1.2836 - acc: 0.824 - ETA: 4:52 - loss: 1.2816 - acc: 0.816 - ETA: 3:39 - loss: 1.2882 - acc: 0.812 - ETA: 2:26 - loss: 1.2899 - acc: 0.815 - ETA: 1:13 - loss: 1.2937 - acc: 0.812 - 2624s 8s/step - loss: 1.3079 - acc: 0.8059 - val_loss: 3.2851 - val_acc: 0.3500\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.76376 to 3.28512, saving model to imagemodel4.1.h5\n",
      "Epoch 8/300\n",
      "340/340 [==============================] - ETA: 39:58 - loss: 1.1897 - acc: 0.80 - ETA: 38:47 - loss: 1.1807 - acc: 0.85 - ETA: 37:35 - loss: 1.2847 - acc: 0.80 - ETA: 36:51 - loss: 1.3018 - acc: 0.80 - ETA: 35:30 - loss: 1.2773 - acc: 0.80 - ETA: 34:13 - loss: 1.3426 - acc: 0.78 - ETA: 32:59 - loss: 1.2875 - acc: 0.81 - ETA: 31:44 - loss: 1.2939 - acc: 0.81 - ETA: 30:32 - loss: 1.2647 - acc: 0.82 - ETA: 29:17 - loss: 1.2389 - acc: 0.84 - ETA: 28:02 - loss: 1.2650 - acc: 0.80 - ETA: 26:48 - loss: 1.3187 - acc: 0.79 - ETA: 25:35 - loss: 1.2961 - acc: 0.80 - ETA: 24:22 - loss: 1.2783 - acc: 0.82 - ETA: 23:09 - loss: 1.2789 - acc: 0.82 - ETA: 22:00 - loss: 1.3025 - acc: 0.80 - ETA: 20:46 - loss: 1.3100 - acc: 0.80 - ETA: 19:32 - loss: 1.3147 - acc: 0.80 - ETA: 18:19 - loss: 1.2969 - acc: 0.80 - ETA: 17:05 - loss: 1.2978 - acc: 0.80 - ETA: 15:52 - loss: 1.2859 - acc: 0.80 - ETA: 14:38 - loss: 1.2858 - acc: 0.80 - ETA: 13:25 - loss: 1.3277 - acc: 0.79 - ETA: 12:12 - loss: 1.3205 - acc: 0.79 - ETA: 10:58 - loss: 1.3199 - acc: 0.78 - ETA: 9:45 - loss: 1.3137 - acc: 0.7885 - ETA: 8:32 - loss: 1.3105 - acc: 0.785 - ETA: 7:18 - loss: 1.3039 - acc: 0.792 - ETA: 6:05 - loss: 1.2944 - acc: 0.796 - ETA: 4:52 - loss: 1.2894 - acc: 0.800 - ETA: 3:39 - loss: 1.2818 - acc: 0.803 - ETA: 2:26 - loss: 1.2766 - acc: 0.803 - ETA: 1:13 - loss: 1.2764 - acc: 0.797 - 2626s 8s/step - loss: 1.2774 - acc: 0.7971 - val_loss: 3.8044 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "340/340 [==============================] - ETA: 39:35 - loss: 1.0261 - acc: 0.90 - ETA: 38:35 - loss: 1.0283 - acc: 0.90 - ETA: 37:28 - loss: 1.0643 - acc: 0.86 - ETA: 36:16 - loss: 1.1196 - acc: 0.85 - ETA: 35:24 - loss: 1.1021 - acc: 0.84 - ETA: 34:07 - loss: 1.1175 - acc: 0.83 - ETA: 32:53 - loss: 1.1208 - acc: 0.82 - ETA: 31:39 - loss: 1.1556 - acc: 0.80 - ETA: 30:28 - loss: 1.1456 - acc: 0.81 - ETA: 29:14 - loss: 1.1441 - acc: 0.82 - ETA: 28:00 - loss: 1.1744 - acc: 0.80 - ETA: 26:47 - loss: 1.1594 - acc: 0.80 - ETA: 25:33 - loss: 1.1374 - acc: 0.82 - ETA: 24:20 - loss: 1.1387 - acc: 0.82 - ETA: 23:07 - loss: 1.1498 - acc: 0.82 - ETA: 21:54 - loss: 1.1563 - acc: 0.81 - ETA: 20:41 - loss: 1.1698 - acc: 0.81 - ETA: 19:28 - loss: 1.1865 - acc: 0.80 - ETA: 18:14 - loss: 1.1961 - acc: 0.80 - ETA: 17:01 - loss: 1.1928 - acc: 0.79 - ETA: 15:48 - loss: 1.2186 - acc: 0.79 - ETA: 14:35 - loss: 1.2232 - acc: 0.79 - ETA: 13:23 - loss: 1.2159 - acc: 0.79 - ETA: 12:09 - loss: 1.2155 - acc: 0.79 - ETA: 10:56 - loss: 1.2076 - acc: 0.79 - ETA: 9:43 - loss: 1.2037 - acc: 0.8000 - ETA: 8:30 - loss: 1.2072 - acc: 0.796 - ETA: 7:17 - loss: 1.1990 - acc: 0.803 - ETA: 6:04 - loss: 1.1876 - acc: 0.806 - ETA: 4:52 - loss: 1.1828 - acc: 0.806 - ETA: 3:39 - loss: 1.1773 - acc: 0.809 - ETA: 2:26 - loss: 1.1773 - acc: 0.806 - ETA: 1:13 - loss: 1.1746 - acc: 0.803 - 2623s 8s/step - loss: 1.2016 - acc: 0.7971 - val_loss: 1.6902 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.28512 to 1.69017, saving model to imagemodel4.1.h5\n",
      "Epoch 10/300\n",
      "340/340 [==============================] - ETA: 39:54 - loss: 0.9380 - acc: 0.90 - ETA: 38:51 - loss: 1.0271 - acc: 0.90 - ETA: 37:41 - loss: 1.0350 - acc: 0.90 - ETA: 36:30 - loss: 1.0387 - acc: 0.87 - ETA: 35:15 - loss: 1.0457 - acc: 0.86 - ETA: 34:04 - loss: 1.1265 - acc: 0.80 - ETA: 32:51 - loss: 1.1387 - acc: 0.78 - ETA: 31:37 - loss: 1.1570 - acc: 0.78 - ETA: 30:24 - loss: 1.1425 - acc: 0.80 - ETA: 29:11 - loss: 1.1427 - acc: 0.80 - ETA: 27:59 - loss: 1.1335 - acc: 0.80 - ETA: 26:46 - loss: 1.1056 - acc: 0.81 - ETA: 25:33 - loss: 1.1007 - acc: 0.82 - ETA: 24:19 - loss: 1.0875 - acc: 0.82 - ETA: 23:06 - loss: 1.0725 - acc: 0.84 - ETA: 21:53 - loss: 1.0677 - acc: 0.84 - ETA: 20:39 - loss: 1.0576 - acc: 0.84 - ETA: 19:26 - loss: 1.0556 - acc: 0.83 - ETA: 18:16 - loss: 1.0458 - acc: 0.83 - ETA: 17:03 - loss: 1.0452 - acc: 0.84 - ETA: 15:49 - loss: 1.0411 - acc: 0.84 - ETA: 14:36 - loss: 1.0418 - acc: 0.84 - ETA: 13:23 - loss: 1.0450 - acc: 0.83 - ETA: 12:10 - loss: 1.0414 - acc: 0.84 - ETA: 10:57 - loss: 1.0441 - acc: 0.84 - ETA: 9:44 - loss: 1.0359 - acc: 0.8462 - ETA: 8:31 - loss: 1.0432 - acc: 0.840 - ETA: 7:17 - loss: 1.0317 - acc: 0.846 - ETA: 6:04 - loss: 1.0292 - acc: 0.851 - ETA: 4:51 - loss: 1.0240 - acc: 0.853 - ETA: 3:39 - loss: 1.0239 - acc: 0.848 - ETA: 2:26 - loss: 1.0331 - acc: 0.846 - ETA: 1:13 - loss: 1.0312 - acc: 0.848 - 2622s 8s/step - loss: 1.0331 - acc: 0.8471 - val_loss: 3.1042 - val_acc: 0.3667\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:57 - loss: 0.6737 - acc: 1.00 - ETA: 39:01 - loss: 0.8743 - acc: 0.80 - ETA: 37:44 - loss: 0.7962 - acc: 0.86 - ETA: 36:28 - loss: 0.8058 - acc: 0.90 - ETA: 35:15 - loss: 0.8205 - acc: 0.92 - ETA: 34:02 - loss: 0.8720 - acc: 0.90 - ETA: 32:52 - loss: 0.8703 - acc: 0.88 - ETA: 31:47 - loss: 0.8946 - acc: 0.87 - ETA: 30:32 - loss: 0.8953 - acc: 0.87 - ETA: 29:19 - loss: 0.8902 - acc: 0.88 - ETA: 28:04 - loss: 0.8752 - acc: 0.89 - ETA: 26:50 - loss: 0.9024 - acc: 0.87 - ETA: 25:36 - loss: 0.9106 - acc: 0.86 - ETA: 24:23 - loss: 0.8937 - acc: 0.87 - ETA: 23:09 - loss: 0.8848 - acc: 0.87 - ETA: 21:55 - loss: 0.8866 - acc: 0.87 - ETA: 20:42 - loss: 0.8943 - acc: 0.87 - ETA: 19:29 - loss: 0.9165 - acc: 0.85 - ETA: 18:16 - loss: 0.9258 - acc: 0.84 - ETA: 17:05 - loss: 0.9202 - acc: 0.85 - ETA: 15:51 - loss: 0.9317 - acc: 0.84 - ETA: 14:38 - loss: 0.9284 - acc: 0.85 - ETA: 13:25 - loss: 0.9302 - acc: 0.85 - ETA: 12:11 - loss: 0.9230 - acc: 0.85 - ETA: 10:58 - loss: 0.9117 - acc: 0.86 - ETA: 9:45 - loss: 0.9061 - acc: 0.8654 - ETA: 8:31 - loss: 0.9031 - acc: 0.866 - ETA: 7:18 - loss: 0.9046 - acc: 0.860 - ETA: 6:05 - loss: 0.9131 - acc: 0.855 - ETA: 4:52 - loss: 0.9150 - acc: 0.850 - ETA: 3:39 - loss: 0.9184 - acc: 0.848 - ETA: 2:26 - loss: 0.9193 - acc: 0.850 - ETA: 1:13 - loss: 0.9180 - acc: 0.851 - 2624s 8s/step - loss: 0.9198 - acc: 0.8500 - val_loss: 1.3618 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.69017 to 1.36177, saving model to imagemodel4.1.h5\n",
      "Epoch 12/300\n",
      "340/340 [==============================] - ETA: 40:05 - loss: 0.8768 - acc: 0.90 - ETA: 38:50 - loss: 1.0771 - acc: 0.80 - ETA: 37:40 - loss: 0.9754 - acc: 0.86 - ETA: 36:30 - loss: 0.9134 - acc: 0.90 - ETA: 35:14 - loss: 0.8908 - acc: 0.92 - ETA: 34:01 - loss: 0.8824 - acc: 0.91 - ETA: 32:48 - loss: 0.8983 - acc: 0.92 - ETA: 31:36 - loss: 0.8962 - acc: 0.92 - ETA: 30:31 - loss: 0.8711 - acc: 0.92 - ETA: 29:17 - loss: 0.8659 - acc: 0.93 - ETA: 28:03 - loss: 0.8585 - acc: 0.92 - ETA: 26:49 - loss: 0.8719 - acc: 0.92 - ETA: 25:36 - loss: 0.8664 - acc: 0.92 - ETA: 24:22 - loss: 0.8685 - acc: 0.91 - ETA: 23:08 - loss: 0.8664 - acc: 0.92 - ETA: 21:55 - loss: 0.8671 - acc: 0.91 - ETA: 20:41 - loss: 0.8761 - acc: 0.91 - ETA: 19:28 - loss: 0.8678 - acc: 0.91 - ETA: 18:15 - loss: 0.8641 - acc: 0.91 - ETA: 17:02 - loss: 0.8550 - acc: 0.92 - ETA: 15:49 - loss: 0.8554 - acc: 0.91 - ETA: 14:36 - loss: 0.8668 - acc: 0.90 - ETA: 13:23 - loss: 0.8678 - acc: 0.90 - ETA: 12:09 - loss: 0.8677 - acc: 0.90 - ETA: 10:57 - loss: 0.8619 - acc: 0.91 - ETA: 9:44 - loss: 0.8559 - acc: 0.9154 - ETA: 8:30 - loss: 0.8559 - acc: 0.911 - ETA: 7:17 - loss: 0.8526 - acc: 0.910 - ETA: 6:05 - loss: 0.8543 - acc: 0.913 - ETA: 4:52 - loss: 0.8584 - acc: 0.910 - ETA: 3:39 - loss: 0.8553 - acc: 0.909 - ETA: 2:26 - loss: 0.8575 - acc: 0.909 - ETA: 1:13 - loss: 0.8593 - acc: 0.909 - 2626s 8s/step - loss: 0.8543 - acc: 0.9088 - val_loss: 1.7832 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "340/340 [==============================] - ETA: 40:02 - loss: 0.8583 - acc: 0.90 - ETA: 38:46 - loss: 0.8176 - acc: 0.90 - ETA: 37:39 - loss: 0.8443 - acc: 0.86 - ETA: 36:28 - loss: 0.9187 - acc: 0.80 - ETA: 35:18 - loss: 0.8783 - acc: 0.82 - ETA: 34:06 - loss: 0.8729 - acc: 0.83 - ETA: 32:53 - loss: 0.8786 - acc: 0.84 - ETA: 31:39 - loss: 0.8866 - acc: 0.82 - ETA: 30:27 - loss: 0.9647 - acc: 0.80 - ETA: 29:16 - loss: 0.9621 - acc: 0.80 - ETA: 28:02 - loss: 0.9370 - acc: 0.81 - ETA: 26:49 - loss: 0.9293 - acc: 0.83 - ETA: 25:35 - loss: 0.9191 - acc: 0.83 - ETA: 24:22 - loss: 0.9272 - acc: 0.83 - ETA: 23:08 - loss: 0.9305 - acc: 0.83 - ETA: 21:56 - loss: 0.9315 - acc: 0.83 - ETA: 20:43 - loss: 0.9379 - acc: 0.83 - ETA: 19:29 - loss: 0.9376 - acc: 0.83 - ETA: 18:15 - loss: 0.9339 - acc: 0.83 - ETA: 17:02 - loss: 0.9287 - acc: 0.83 - ETA: 15:49 - loss: 0.9252 - acc: 0.82 - ETA: 14:36 - loss: 0.9130 - acc: 0.83 - ETA: 13:24 - loss: 0.9029 - acc: 0.84 - ETA: 12:11 - loss: 0.8954 - acc: 0.85 - ETA: 10:58 - loss: 0.8863 - acc: 0.85 - ETA: 9:45 - loss: 0.8768 - acc: 0.8615 - ETA: 8:32 - loss: 0.8785 - acc: 0.863 - ETA: 7:18 - loss: 0.8815 - acc: 0.860 - ETA: 6:05 - loss: 0.8875 - acc: 0.851 - ETA: 4:52 - loss: 0.8792 - acc: 0.856 - ETA: 3:39 - loss: 0.8721 - acc: 0.861 - ETA: 2:26 - loss: 0.8709 - acc: 0.859 - ETA: 1:13 - loss: 0.8710 - acc: 0.860 - 2625s 8s/step - loss: 0.8636 - acc: 0.8647 - val_loss: 1.9388 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "340/340 [==============================] - ETA: 40:18 - loss: 0.8751 - acc: 1.00 - ETA: 39:14 - loss: 0.9389 - acc: 0.85 - ETA: 37:48 - loss: 0.8762 - acc: 0.83 - ETA: 36:27 - loss: 0.8794 - acc: 0.82 - ETA: 35:09 - loss: 0.8602 - acc: 0.84 - ETA: 33:54 - loss: 0.8313 - acc: 0.86 - ETA: 32:38 - loss: 0.8049 - acc: 0.88 - ETA: 31:22 - loss: 0.7918 - acc: 0.88 - ETA: 30:08 - loss: 0.8318 - acc: 0.85 - ETA: 28:55 - loss: 0.8846 - acc: 0.83 - ETA: 27:42 - loss: 0.8887 - acc: 0.82 - ETA: 26:33 - loss: 0.8793 - acc: 0.83 - ETA: 25:21 - loss: 0.8879 - acc: 0.83 - ETA: 24:09 - loss: 0.8662 - acc: 0.84 - ETA: 22:56 - loss: 0.8576 - acc: 0.85 - ETA: 21:43 - loss: 0.8485 - acc: 0.85 - ETA: 20:30 - loss: 0.8386 - acc: 0.85 - ETA: 19:18 - loss: 0.8466 - acc: 0.85 - ETA: 18:06 - loss: 0.8380 - acc: 0.86 - ETA: 16:53 - loss: 0.8472 - acc: 0.86 - ETA: 15:41 - loss: 0.8433 - acc: 0.85 - ETA: 14:29 - loss: 0.8454 - acc: 0.85 - ETA: 13:16 - loss: 0.8410 - acc: 0.86 - ETA: 12:05 - loss: 0.8360 - acc: 0.86 - ETA: 10:52 - loss: 0.8300 - acc: 0.86 - ETA: 9:39 - loss: 0.8264 - acc: 0.8654 - ETA: 8:27 - loss: 0.8268 - acc: 0.866 - ETA: 7:14 - loss: 0.8455 - acc: 0.867 - ETA: 6:02 - loss: 0.8449 - acc: 0.865 - ETA: 4:49 - loss: 0.8373 - acc: 0.870 - ETA: 3:37 - loss: 0.8428 - acc: 0.871 - ETA: 2:24 - loss: 0.8450 - acc: 0.871 - ETA: 1:12 - loss: 0.8389 - acc: 0.875 - 2600s 8s/step - loss: 0.8421 - acc: 0.8765 - val_loss: 1.5952 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "340/340 [==============================] - ETA: 42:04 - loss: 0.8540 - acc: 0.90 - ETA: 39:33 - loss: 0.8947 - acc: 0.80 - ETA: 37:55 - loss: 0.8826 - acc: 0.76 - ETA: 36:40 - loss: 0.8085 - acc: 0.82 - ETA: 35:20 - loss: 0.8665 - acc: 0.82 - ETA: 34:03 - loss: 0.8826 - acc: 0.81 - ETA: 32:49 - loss: 0.8881 - acc: 0.82 - ETA: 31:34 - loss: 0.8690 - acc: 0.82 - ETA: 30:19 - loss: 0.8703 - acc: 0.83 - ETA: 29:06 - loss: 0.8542 - acc: 0.84 - ETA: 27:52 - loss: 0.8431 - acc: 0.85 - ETA: 26:39 - loss: 0.8631 - acc: 0.85 - ETA: 25:28 - loss: 0.8471 - acc: 0.85 - ETA: 24:14 - loss: 0.8521 - acc: 0.86 - ETA: 23:00 - loss: 0.8542 - acc: 0.86 - ETA: 21:47 - loss: 0.8446 - acc: 0.86 - ETA: 20:34 - loss: 0.8274 - acc: 0.87 - ETA: 19:21 - loss: 0.8280 - acc: 0.88 - ETA: 18:08 - loss: 0.8144 - acc: 0.88 - ETA: 16:55 - loss: 0.8075 - acc: 0.89 - ETA: 15:42 - loss: 0.8015 - acc: 0.89 - ETA: 14:29 - loss: 0.8052 - acc: 0.89 - ETA: 13:16 - loss: 0.7982 - acc: 0.89 - ETA: 12:04 - loss: 0.7927 - acc: 0.89 - ETA: 10:52 - loss: 0.7995 - acc: 0.88 - ETA: 9:40 - loss: 0.8032 - acc: 0.8885 - ETA: 8:28 - loss: 0.8048 - acc: 0.888 - ETA: 7:15 - loss: 0.8177 - acc: 0.885 - ETA: 6:02 - loss: 0.8139 - acc: 0.886 - ETA: 4:50 - loss: 0.8187 - acc: 0.886 - ETA: 3:37 - loss: 0.8155 - acc: 0.890 - ETA: 2:25 - loss: 0.8094 - acc: 0.890 - ETA: 1:12 - loss: 0.8074 - acc: 0.887 - 2604s 8s/step - loss: 0.8050 - acc: 0.8912 - val_loss: 1.0759 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.36177 to 1.07590, saving model to imagemodel4.1.h5\n",
      "Epoch 16/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:52 - loss: 0.6924 - acc: 0.90 - ETA: 38:49 - loss: 0.7642 - acc: 0.90 - ETA: 37:32 - loss: 0.7007 - acc: 0.93 - ETA: 36:15 - loss: 0.7041 - acc: 0.92 - ETA: 35:04 - loss: 0.7667 - acc: 0.88 - ETA: 33:49 - loss: 0.7268 - acc: 0.90 - ETA: 32:37 - loss: 0.7152 - acc: 0.90 - ETA: 31:24 - loss: 0.7052 - acc: 0.91 - ETA: 30:12 - loss: 0.6839 - acc: 0.92 - ETA: 28:59 - loss: 0.6710 - acc: 0.93 - ETA: 27:46 - loss: 0.6757 - acc: 0.92 - ETA: 26:33 - loss: 0.7076 - acc: 0.90 - ETA: 25:20 - loss: 0.6976 - acc: 0.90 - ETA: 24:07 - loss: 0.6882 - acc: 0.91 - ETA: 22:59 - loss: 0.6832 - acc: 0.92 - ETA: 21:46 - loss: 0.6982 - acc: 0.91 - ETA: 20:33 - loss: 0.6940 - acc: 0.91 - ETA: 19:20 - loss: 0.6970 - acc: 0.91 - ETA: 18:07 - loss: 0.6950 - acc: 0.91 - ETA: 16:55 - loss: 0.6879 - acc: 0.92 - ETA: 15:42 - loss: 0.6828 - acc: 0.92 - ETA: 14:30 - loss: 0.6796 - acc: 0.92 - ETA: 13:17 - loss: 0.6753 - acc: 0.92 - ETA: 12:05 - loss: 0.6831 - acc: 0.92 - ETA: 10:52 - loss: 0.6855 - acc: 0.91 - ETA: 9:39 - loss: 0.6886 - acc: 0.9154 - ETA: 8:27 - loss: 0.6938 - acc: 0.918 - ETA: 7:14 - loss: 0.6964 - acc: 0.917 - ETA: 6:02 - loss: 0.6993 - acc: 0.910 - ETA: 4:49 - loss: 0.7031 - acc: 0.910 - ETA: 3:37 - loss: 0.7058 - acc: 0.906 - ETA: 2:24 - loss: 0.7036 - acc: 0.909 - ETA: 1:12 - loss: 0.7017 - acc: 0.909 - 2601s 8s/step - loss: 0.6984 - acc: 0.9118 - val_loss: 1.3731 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "340/340 [==============================] - ETA: 39:31 - loss: 0.6296 - acc: 1.00 - ETA: 38:30 - loss: 0.5951 - acc: 1.00 - ETA: 37:19 - loss: 0.6202 - acc: 1.00 - ETA: 36:38 - loss: 0.6528 - acc: 0.95 - ETA: 35:18 - loss: 0.7906 - acc: 0.92 - ETA: 34:01 - loss: 0.7624 - acc: 0.91 - ETA: 32:43 - loss: 0.7956 - acc: 0.90 - ETA: 31:28 - loss: 0.7836 - acc: 0.91 - ETA: 30:14 - loss: 0.7778 - acc: 0.90 - ETA: 29:00 - loss: 0.7743 - acc: 0.90 - ETA: 27:46 - loss: 0.7679 - acc: 0.90 - ETA: 26:34 - loss: 0.7892 - acc: 0.89 - ETA: 25:21 - loss: 0.7859 - acc: 0.88 - ETA: 24:09 - loss: 0.7772 - acc: 0.89 - ETA: 22:56 - loss: 0.7707 - acc: 0.89 - ETA: 21:44 - loss: 0.7627 - acc: 0.90 - ETA: 20:32 - loss: 0.7574 - acc: 0.90 - ETA: 19:19 - loss: 0.7512 - acc: 0.90 - ETA: 18:06 - loss: 0.7442 - acc: 0.91 - ETA: 16:54 - loss: 0.7365 - acc: 0.91 - ETA: 15:41 - loss: 0.7418 - acc: 0.90 - ETA: 14:29 - loss: 0.7556 - acc: 0.90 - ETA: 13:17 - loss: 0.7512 - acc: 0.90 - ETA: 12:04 - loss: 0.7534 - acc: 0.90 - ETA: 10:52 - loss: 0.7495 - acc: 0.90 - ETA: 9:40 - loss: 0.7523 - acc: 0.9077 - ETA: 8:28 - loss: 0.7501 - acc: 0.907 - ETA: 7:16 - loss: 0.7444 - acc: 0.910 - ETA: 6:03 - loss: 0.7490 - acc: 0.910 - ETA: 4:50 - loss: 0.7462 - acc: 0.913 - ETA: 3:37 - loss: 0.7645 - acc: 0.900 - ETA: 2:25 - loss: 0.7633 - acc: 0.896 - ETA: 1:12 - loss: 0.7671 - acc: 0.893 - 2606s 8s/step - loss: 0.7670 - acc: 0.8882 - val_loss: 6.5525 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "340/340 [==============================] - ETA: 39:38 - loss: 0.9585 - acc: 0.70 - ETA: 38:22 - loss: 0.8454 - acc: 0.80 - ETA: 37:11 - loss: 0.8580 - acc: 0.83 - ETA: 36:02 - loss: 0.8364 - acc: 0.85 - ETA: 34:47 - loss: 0.7804 - acc: 0.88 - ETA: 33:37 - loss: 0.7560 - acc: 0.88 - ETA: 32:30 - loss: 0.7352 - acc: 0.90 - ETA: 31:17 - loss: 0.7233 - acc: 0.91 - ETA: 30:04 - loss: 0.7599 - acc: 0.90 - ETA: 28:51 - loss: 0.7732 - acc: 0.88 - ETA: 27:38 - loss: 0.7753 - acc: 0.89 - ETA: 26:25 - loss: 0.7703 - acc: 0.88 - ETA: 25:13 - loss: 0.7531 - acc: 0.89 - ETA: 24:00 - loss: 0.7423 - acc: 0.89 - ETA: 22:48 - loss: 0.7309 - acc: 0.90 - ETA: 21:36 - loss: 0.7308 - acc: 0.90 - ETA: 20:24 - loss: 0.7452 - acc: 0.90 - ETA: 19:12 - loss: 0.7412 - acc: 0.90 - ETA: 18:00 - loss: 0.7370 - acc: 0.90 - ETA: 16:51 - loss: 0.7296 - acc: 0.90 - ETA: 15:39 - loss: 0.7266 - acc: 0.90 - ETA: 14:26 - loss: 0.7304 - acc: 0.90 - ETA: 13:14 - loss: 0.7230 - acc: 0.90 - ETA: 12:01 - loss: 0.7245 - acc: 0.90 - ETA: 10:49 - loss: 0.7228 - acc: 0.90 - ETA: 9:37 - loss: 0.7173 - acc: 0.9077 - ETA: 8:24 - loss: 0.7211 - acc: 0.903 - ETA: 7:12 - loss: 0.7198 - acc: 0.907 - ETA: 6:00 - loss: 0.7235 - acc: 0.900 - ETA: 4:48 - loss: 0.7217 - acc: 0.903 - ETA: 3:36 - loss: 0.7193 - acc: 0.906 - ETA: 2:24 - loss: 0.7118 - acc: 0.909 - ETA: 1:12 - loss: 0.7088 - acc: 0.912 - 2592s 8s/step - loss: 0.7009 - acc: 0.9147 - val_loss: 1.8264 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "340/340 [==============================] - ETA: 39:35 - loss: 0.5120 - acc: 1.00 - ETA: 38:23 - loss: 0.5831 - acc: 0.95 - ETA: 37:12 - loss: 0.5825 - acc: 0.93 - ETA: 36:03 - loss: 0.5799 - acc: 0.95 - ETA: 34:52 - loss: 0.5882 - acc: 0.94 - ETA: 33:39 - loss: 0.5680 - acc: 0.95 - ETA: 32:28 - loss: 0.6302 - acc: 0.91 - ETA: 31:17 - loss: 0.6242 - acc: 0.91 - ETA: 30:12 - loss: 0.6160 - acc: 0.92 - ETA: 29:00 - loss: 0.6156 - acc: 0.93 - ETA: 27:45 - loss: 0.6068 - acc: 0.93 - ETA: 26:36 - loss: 0.5999 - acc: 0.94 - ETA: 25:29 - loss: 0.6026 - acc: 0.93 - ETA: 24:23 - loss: 0.6000 - acc: 0.94 - ETA: 23:12 - loss: 0.6007 - acc: 0.94 - ETA: 21:58 - loss: 0.6100 - acc: 0.93 - ETA: 20:44 - loss: 0.6107 - acc: 0.92 - ETA: 19:30 - loss: 0.6029 - acc: 0.93 - ETA: 18:16 - loss: 0.5981 - acc: 0.93 - ETA: 17:02 - loss: 0.6034 - acc: 0.93 - ETA: 15:49 - loss: 0.6092 - acc: 0.93 - ETA: 14:35 - loss: 0.6157 - acc: 0.92 - ETA: 13:22 - loss: 0.6146 - acc: 0.92 - ETA: 12:09 - loss: 0.6156 - acc: 0.92 - ETA: 10:55 - loss: 0.6169 - acc: 0.93 - ETA: 9:42 - loss: 0.6197 - acc: 0.9269 - ETA: 8:29 - loss: 0.6170 - acc: 0.929 - ETA: 7:16 - loss: 0.6123 - acc: 0.932 - ETA: 6:03 - loss: 0.6154 - acc: 0.931 - ETA: 4:50 - loss: 0.6143 - acc: 0.930 - ETA: 3:38 - loss: 0.6139 - acc: 0.929 - ETA: 2:25 - loss: 0.6114 - acc: 0.931 - ETA: 1:12 - loss: 0.6099 - acc: 0.933 - 2612s 8s/step - loss: 0.6044 - acc: 0.9353 - val_loss: 1.0819 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "340/340 [==============================] - ETA: 39:29 - loss: 0.5713 - acc: 1.00 - ETA: 38:34 - loss: 0.7565 - acc: 0.85 - ETA: 37:14 - loss: 0.6661 - acc: 0.90 - ETA: 36:03 - loss: 0.6271 - acc: 0.92 - ETA: 34:51 - loss: 0.6009 - acc: 0.94 - ETA: 33:37 - loss: 0.6151 - acc: 0.93 - ETA: 32:26 - loss: 0.6137 - acc: 0.94 - ETA: 31:12 - loss: 0.5918 - acc: 0.95 - ETA: 30:00 - loss: 0.6161 - acc: 0.94 - ETA: 28:50 - loss: 0.5967 - acc: 0.95 - ETA: 27:37 - loss: 0.5878 - acc: 0.95 - ETA: 26:25 - loss: 0.6039 - acc: 0.94 - ETA: 25:13 - loss: 0.5957 - acc: 0.94 - ETA: 24:01 - loss: 0.5887 - acc: 0.95 - ETA: 22:49 - loss: 0.5997 - acc: 0.94 - ETA: 21:37 - loss: 0.5974 - acc: 0.93 - ETA: 20:26 - loss: 0.5994 - acc: 0.93 - ETA: 19:14 - loss: 0.6075 - acc: 0.93 - ETA: 18:01 - loss: 0.6375 - acc: 0.92 - ETA: 16:49 - loss: 0.6409 - acc: 0.92 - ETA: 15:38 - loss: 0.6395 - acc: 0.92 - ETA: 14:26 - loss: 0.6344 - acc: 0.92 - ETA: 13:14 - loss: 0.6269 - acc: 0.93 - ETA: 12:02 - loss: 0.6265 - acc: 0.93 - ETA: 10:50 - loss: 0.6518 - acc: 0.91 - ETA: 9:38 - loss: 0.6481 - acc: 0.9154 - ETA: 8:26 - loss: 0.6492 - acc: 0.914 - ETA: 7:14 - loss: 0.6497 - acc: 0.917 - ETA: 6:02 - loss: 0.6470 - acc: 0.920 - ETA: 4:49 - loss: 0.6492 - acc: 0.920 - ETA: 3:37 - loss: 0.6474 - acc: 0.919 - ETA: 2:24 - loss: 0.6457 - acc: 0.918 - ETA: 1:12 - loss: 0.6484 - acc: 0.915 - 2604s 8s/step - loss: 0.6526 - acc: 0.9147 - val_loss: 2.3905 - val_acc: 0.4833\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:28 - loss: 0.7104 - acc: 0.90 - ETA: 38:29 - loss: 0.8119 - acc: 0.85 - ETA: 37:21 - loss: 0.7256 - acc: 0.86 - ETA: 36:15 - loss: 0.6818 - acc: 0.90 - ETA: 35:08 - loss: 0.6628 - acc: 0.92 - ETA: 33:56 - loss: 0.6571 - acc: 0.93 - ETA: 32:43 - loss: 0.6392 - acc: 0.94 - ETA: 31:30 - loss: 0.6482 - acc: 0.93 - ETA: 30:17 - loss: 0.7136 - acc: 0.91 - ETA: 29:03 - loss: 0.7054 - acc: 0.91 - ETA: 27:51 - loss: 0.6892 - acc: 0.91 - ETA: 26:38 - loss: 0.7501 - acc: 0.88 - ETA: 25:26 - loss: 0.7408 - acc: 0.89 - ETA: 24:13 - loss: 0.7460 - acc: 0.89 - ETA: 23:00 - loss: 0.7441 - acc: 0.89 - ETA: 21:47 - loss: 0.7297 - acc: 0.90 - ETA: 20:38 - loss: 0.7480 - acc: 0.89 - ETA: 19:25 - loss: 0.7399 - acc: 0.88 - ETA: 18:12 - loss: 0.7277 - acc: 0.89 - ETA: 16:59 - loss: 0.7214 - acc: 0.89 - ETA: 15:46 - loss: 0.7104 - acc: 0.90 - ETA: 14:33 - loss: 0.7079 - acc: 0.90 - ETA: 13:20 - loss: 0.7051 - acc: 0.90 - ETA: 12:07 - loss: 0.7240 - acc: 0.90 - ETA: 10:55 - loss: 0.7163 - acc: 0.90 - ETA: 9:42 - loss: 0.7164 - acc: 0.9077 - ETA: 8:29 - loss: 0.7202 - acc: 0.911 - ETA: 7:16 - loss: 0.7223 - acc: 0.910 - ETA: 6:04 - loss: 0.7164 - acc: 0.913 - ETA: 4:51 - loss: 0.7100 - acc: 0.916 - ETA: 3:38 - loss: 0.7078 - acc: 0.916 - ETA: 2:25 - loss: 0.7013 - acc: 0.918 - ETA: 1:12 - loss: 0.6963 - acc: 0.918 - 2613s 8s/step - loss: 0.7038 - acc: 0.9147 - val_loss: 2.5050 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "340/340 [==============================] - ETA: 39:43 - loss: 0.5538 - acc: 1.00 - ETA: 38:40 - loss: 0.6746 - acc: 0.90 - ETA: 37:31 - loss: 0.8010 - acc: 0.86 - ETA: 36:20 - loss: 0.7318 - acc: 0.90 - ETA: 35:06 - loss: 0.6918 - acc: 0.92 - ETA: 34:14 - loss: 0.6780 - acc: 0.91 - ETA: 32:59 - loss: 0.6572 - acc: 0.92 - ETA: 31:41 - loss: 0.6428 - acc: 0.93 - ETA: 30:26 - loss: 0.6678 - acc: 0.92 - ETA: 29:12 - loss: 0.6564 - acc: 0.93 - ETA: 27:58 - loss: 0.6462 - acc: 0.93 - ETA: 26:43 - loss: 0.6404 - acc: 0.94 - ETA: 25:30 - loss: 0.6368 - acc: 0.93 - ETA: 24:16 - loss: 0.6477 - acc: 0.92 - ETA: 23:03 - loss: 0.6492 - acc: 0.92 - ETA: 21:50 - loss: 0.6421 - acc: 0.92 - ETA: 20:37 - loss: 0.6346 - acc: 0.92 - ETA: 19:24 - loss: 0.6316 - acc: 0.93 - ETA: 18:12 - loss: 0.6336 - acc: 0.93 - ETA: 16:59 - loss: 0.6270 - acc: 0.93 - ETA: 15:46 - loss: 0.6325 - acc: 0.93 - ETA: 14:33 - loss: 0.6276 - acc: 0.93 - ETA: 13:20 - loss: 0.6228 - acc: 0.93 - ETA: 12:07 - loss: 0.6229 - acc: 0.92 - ETA: 10:54 - loss: 0.6157 - acc: 0.93 - ETA: 9:41 - loss: 0.6149 - acc: 0.9308 - ETA: 8:28 - loss: 0.6177 - acc: 0.922 - ETA: 7:16 - loss: 0.6148 - acc: 0.925 - ETA: 6:03 - loss: 0.6101 - acc: 0.927 - ETA: 4:50 - loss: 0.6084 - acc: 0.926 - ETA: 3:38 - loss: 0.6152 - acc: 0.919 - ETA: 2:25 - loss: 0.6174 - acc: 0.915 - ETA: 1:12 - loss: 0.6236 - acc: 0.912 - 2614s 8s/step - loss: 0.6174 - acc: 0.9147 - val_loss: 1.3434 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "340/340 [==============================] - ETA: 39:48 - loss: 0.4501 - acc: 1.00 - ETA: 38:41 - loss: 0.4986 - acc: 0.95 - ETA: 37:29 - loss: 0.5145 - acc: 0.93 - ETA: 36:19 - loss: 0.5230 - acc: 0.92 - ETA: 35:07 - loss: 0.5514 - acc: 0.90 - ETA: 33:52 - loss: 0.5377 - acc: 0.91 - ETA: 32:41 - loss: 0.5359 - acc: 0.92 - ETA: 31:27 - loss: 0.5225 - acc: 0.93 - ETA: 30:13 - loss: 0.5273 - acc: 0.93 - ETA: 29:00 - loss: 0.5271 - acc: 0.93 - ETA: 27:48 - loss: 0.5263 - acc: 0.92 - ETA: 26:35 - loss: 0.5286 - acc: 0.93 - ETA: 25:23 - loss: 0.5416 - acc: 0.93 - ETA: 24:11 - loss: 0.5465 - acc: 0.92 - ETA: 22:59 - loss: 0.5540 - acc: 0.92 - ETA: 21:47 - loss: 0.5460 - acc: 0.92 - ETA: 20:34 - loss: 0.5566 - acc: 0.91 - ETA: 19:22 - loss: 0.5549 - acc: 0.91 - ETA: 18:08 - loss: 0.5503 - acc: 0.92 - ETA: 16:59 - loss: 0.5536 - acc: 0.92 - ETA: 15:46 - loss: 0.5609 - acc: 0.91 - ETA: 14:33 - loss: 0.5546 - acc: 0.91 - ETA: 13:20 - loss: 0.5538 - acc: 0.91 - ETA: 12:07 - loss: 0.5589 - acc: 0.91 - ETA: 10:54 - loss: 0.5539 - acc: 0.92 - ETA: 9:41 - loss: 0.5532 - acc: 0.9192 - ETA: 8:29 - loss: 0.5566 - acc: 0.914 - ETA: 7:16 - loss: 0.5547 - acc: 0.917 - ETA: 6:03 - loss: 0.5498 - acc: 0.920 - ETA: 4:50 - loss: 0.5513 - acc: 0.920 - ETA: 3:37 - loss: 0.5641 - acc: 0.916 - ETA: 2:25 - loss: 0.5670 - acc: 0.915 - ETA: 1:12 - loss: 0.5673 - acc: 0.915 - 2610s 8s/step - loss: 0.5683 - acc: 0.9147 - val_loss: 1.0110 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.07590 to 1.01097, saving model to imagemodel4.1.h5\n",
      "Epoch 24/300\n",
      "340/340 [==============================] - ETA: 40:04 - loss: 0.4517 - acc: 1.00 - ETA: 38:42 - loss: 0.4963 - acc: 0.95 - ETA: 37:28 - loss: 0.4800 - acc: 0.93 - ETA: 36:17 - loss: 0.4866 - acc: 0.92 - ETA: 35:03 - loss: 0.4804 - acc: 0.94 - ETA: 33:48 - loss: 0.4817 - acc: 0.95 - ETA: 32:36 - loss: 0.4766 - acc: 0.95 - ETA: 31:24 - loss: 0.4917 - acc: 0.95 - ETA: 30:23 - loss: 0.5008 - acc: 0.95 - ETA: 29:09 - loss: 0.5473 - acc: 0.95 - ETA: 27:56 - loss: 0.5519 - acc: 0.94 - ETA: 26:42 - loss: 0.5588 - acc: 0.94 - ETA: 25:29 - loss: 0.5527 - acc: 0.94 - ETA: 24:16 - loss: 0.5545 - acc: 0.94 - ETA: 23:02 - loss: 0.5777 - acc: 0.92 - ETA: 21:49 - loss: 0.5779 - acc: 0.93 - ETA: 20:36 - loss: 0.5763 - acc: 0.93 - ETA: 19:23 - loss: 0.5791 - acc: 0.93 - ETA: 18:11 - loss: 0.5790 - acc: 0.93 - ETA: 16:58 - loss: 0.5790 - acc: 0.93 - ETA: 15:45 - loss: 0.5850 - acc: 0.93 - ETA: 14:32 - loss: 0.6038 - acc: 0.92 - ETA: 13:19 - loss: 0.6054 - acc: 0.92 - ETA: 12:06 - loss: 0.6073 - acc: 0.92 - ETA: 10:54 - loss: 0.6126 - acc: 0.92 - ETA: 9:41 - loss: 0.6239 - acc: 0.9154 - ETA: 8:28 - loss: 0.6199 - acc: 0.914 - ETA: 7:15 - loss: 0.6177 - acc: 0.914 - ETA: 6:03 - loss: 0.6192 - acc: 0.913 - ETA: 4:50 - loss: 0.6162 - acc: 0.916 - ETA: 3:37 - loss: 0.6141 - acc: 0.919 - ETA: 2:25 - loss: 0.6165 - acc: 0.918 - ETA: 1:12 - loss: 0.6143 - acc: 0.921 - 2615s 8s/step - loss: 0.6173 - acc: 0.9206 - val_loss: 1.7675 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "340/340 [==============================] - ETA: 39:52 - loss: 0.5815 - acc: 0.80 - ETA: 38:44 - loss: 0.5214 - acc: 0.90 - ETA: 37:26 - loss: 0.5395 - acc: 0.90 - ETA: 36:17 - loss: 0.5165 - acc: 0.92 - ETA: 35:04 - loss: 0.4968 - acc: 0.94 - ETA: 33:50 - loss: 0.4853 - acc: 0.95 - ETA: 32:36 - loss: 0.4934 - acc: 0.94 - ETA: 31:22 - loss: 0.5569 - acc: 0.91 - ETA: 30:10 - loss: 0.5585 - acc: 0.91 - ETA: 28:58 - loss: 0.5509 - acc: 0.92 - ETA: 27:46 - loss: 0.5556 - acc: 0.91 - ETA: 26:33 - loss: 0.5477 - acc: 0.92 - ETA: 25:22 - loss: 0.5399 - acc: 0.93 - ETA: 24:10 - loss: 0.5366 - acc: 0.93 - ETA: 22:57 - loss: 0.5305 - acc: 0.94 - ETA: 21:45 - loss: 0.5302 - acc: 0.93 - ETA: 20:32 - loss: 0.5340 - acc: 0.94 - ETA: 19:20 - loss: 0.5526 - acc: 0.93 - ETA: 18:08 - loss: 0.5449 - acc: 0.93 - ETA: 16:55 - loss: 0.5427 - acc: 0.93 - ETA: 15:43 - loss: 0.5453 - acc: 0.93 - ETA: 14:30 - loss: 0.5459 - acc: 0.93 - ETA: 13:20 - loss: 0.5682 - acc: 0.92 - ETA: 12:07 - loss: 0.5616 - acc: 0.92 - ETA: 10:54 - loss: 0.5564 - acc: 0.92 - ETA: 9:42 - loss: 0.5586 - acc: 0.9308 - ETA: 8:29 - loss: 0.5680 - acc: 0.929 - ETA: 7:16 - loss: 0.5659 - acc: 0.928 - ETA: 6:03 - loss: 0.5636 - acc: 0.931 - ETA: 4:50 - loss: 0.5723 - acc: 0.926 - ETA: 3:38 - loss: 0.5693 - acc: 0.925 - ETA: 2:25 - loss: 0.5670 - acc: 0.925 - ETA: 1:12 - loss: 0.5707 - acc: 0.921 - 2611s 8s/step - loss: 0.5834 - acc: 0.9147 - val_loss: 1.8389 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:39 - loss: 0.7117 - acc: 0.90 - ETA: 38:29 - loss: 0.6312 - acc: 0.90 - ETA: 37:20 - loss: 0.7827 - acc: 0.90 - ETA: 36:10 - loss: 0.7940 - acc: 0.87 - ETA: 34:59 - loss: 0.7154 - acc: 0.90 - ETA: 33:47 - loss: 0.6992 - acc: 0.90 - ETA: 32:36 - loss: 0.7113 - acc: 0.90 - ETA: 31:24 - loss: 0.6864 - acc: 0.90 - ETA: 30:13 - loss: 0.6754 - acc: 0.90 - ETA: 28:59 - loss: 0.6590 - acc: 0.91 - ETA: 27:47 - loss: 0.6429 - acc: 0.91 - ETA: 26:46 - loss: 0.6446 - acc: 0.91 - ETA: 25:32 - loss: 0.6451 - acc: 0.91 - ETA: 24:19 - loss: 0.6312 - acc: 0.92 - ETA: 23:11 - loss: 0.6762 - acc: 0.90 - ETA: 22:06 - loss: 0.6775 - acc: 0.89 - ETA: 21:46 - loss: 0.6699 - acc: 0.90 - ETA: 20:42 - loss: 0.6635 - acc: 0.90 - ETA: 19:20 - loss: 0.6639 - acc: 0.90 - ETA: 18:00 - loss: 0.6562 - acc: 0.90 - ETA: 16:40 - loss: 0.6439 - acc: 0.90 - ETA: 15:21 - loss: 0.6391 - acc: 0.91 - ETA: 14:05 - loss: 0.6303 - acc: 0.91 - ETA: 12:49 - loss: 0.6335 - acc: 0.91 - ETA: 11:32 - loss: 0.6318 - acc: 0.91 - ETA: 10:14 - loss: 0.6549 - acc: 0.90 - ETA: 8:56 - loss: 0.6585 - acc: 0.9037 - ETA: 7:39 - loss: 0.6561 - acc: 0.907 - ETA: 6:22 - loss: 0.6524 - acc: 0.906 - ETA: 5:05 - loss: 0.6442 - acc: 0.910 - ETA: 3:49 - loss: 0.6447 - acc: 0.912 - ETA: 2:32 - loss: 0.6468 - acc: 0.909 - ETA: 1:16 - loss: 0.6427 - acc: 0.912 - 2733s 8s/step - loss: 0.6398 - acc: 0.9147 - val_loss: 11.0006 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "340/340 [==============================] - ETA: 40:18 - loss: 0.6030 - acc: 1.00 - ETA: 39:07 - loss: 0.6089 - acc: 1.00 - ETA: 37:56 - loss: 0.5838 - acc: 1.00 - ETA: 36:39 - loss: 0.5770 - acc: 1.00 - ETA: 35:22 - loss: 0.5801 - acc: 1.00 - ETA: 34:06 - loss: 0.5958 - acc: 1.00 - ETA: 32:51 - loss: 0.6040 - acc: 0.97 - ETA: 31:43 - loss: 0.5834 - acc: 0.97 - ETA: 30:35 - loss: 0.5875 - acc: 0.95 - ETA: 29:30 - loss: 0.5735 - acc: 0.96 - ETA: 28:33 - loss: 0.5725 - acc: 0.96 - ETA: 27:28 - loss: 0.5602 - acc: 0.96 - ETA: 26:11 - loss: 0.5546 - acc: 0.96 - ETA: 24:54 - loss: 0.5504 - acc: 0.95 - ETA: 23:38 - loss: 0.5578 - acc: 0.94 - ETA: 22:24 - loss: 0.5513 - acc: 0.94 - ETA: 21:08 - loss: 0.5435 - acc: 0.94 - ETA: 19:53 - loss: 0.5429 - acc: 0.94 - ETA: 18:37 - loss: 0.5495 - acc: 0.94 - ETA: 17:22 - loss: 0.5597 - acc: 0.94 - ETA: 16:07 - loss: 0.5544 - acc: 0.94 - ETA: 14:52 - loss: 0.5667 - acc: 0.93 - ETA: 13:37 - loss: 0.5611 - acc: 0.93 - ETA: 12:22 - loss: 0.5551 - acc: 0.94 - ETA: 11:07 - loss: 0.5502 - acc: 0.94 - ETA: 9:53 - loss: 0.5487 - acc: 0.9462 - ETA: 8:38 - loss: 0.5494 - acc: 0.948 - ETA: 7:24 - loss: 0.5492 - acc: 0.946 - ETA: 6:10 - loss: 0.5523 - acc: 0.948 - ETA: 4:56 - loss: 0.5528 - acc: 0.950 - ETA: 3:42 - loss: 0.5500 - acc: 0.951 - ETA: 2:28 - loss: 0.5445 - acc: 0.953 - ETA: 1:14 - loss: 0.5426 - acc: 0.951 - 2656s 8s/step - loss: 0.5404 - acc: 0.9529 - val_loss: 0.8507 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.01097 to 0.85075, saving model to imagemodel4.1.h5\n",
      "Epoch 28/300\n",
      "340/340 [==============================] - ETA: 40:07 - loss: 0.3997 - acc: 1.00 - ETA: 39:03 - loss: 0.4483 - acc: 1.00 - ETA: 37:49 - loss: 0.5291 - acc: 0.93 - ETA: 36:48 - loss: 0.5053 - acc: 0.95 - ETA: 36:00 - loss: 0.5110 - acc: 0.96 - ETA: 34:42 - loss: 0.4944 - acc: 0.96 - ETA: 33:22 - loss: 0.4851 - acc: 0.97 - ETA: 32:06 - loss: 0.4831 - acc: 0.97 - ETA: 30:50 - loss: 0.4814 - acc: 0.97 - ETA: 29:34 - loss: 0.4890 - acc: 0.97 - ETA: 28:19 - loss: 0.4813 - acc: 0.97 - ETA: 27:04 - loss: 0.4745 - acc: 0.97 - ETA: 25:50 - loss: 0.4714 - acc: 0.97 - ETA: 24:35 - loss: 0.4648 - acc: 0.97 - ETA: 23:21 - loss: 0.4613 - acc: 0.98 - ETA: 22:07 - loss: 0.4750 - acc: 0.96 - ETA: 20:55 - loss: 0.4744 - acc: 0.97 - ETA: 19:41 - loss: 0.4722 - acc: 0.97 - ETA: 18:27 - loss: 0.4692 - acc: 0.97 - ETA: 17:13 - loss: 0.4705 - acc: 0.97 - ETA: 15:59 - loss: 0.4693 - acc: 0.97 - ETA: 14:45 - loss: 0.4649 - acc: 0.97 - ETA: 13:32 - loss: 0.4749 - acc: 0.96 - ETA: 12:18 - loss: 0.4726 - acc: 0.97 - ETA: 11:04 - loss: 0.4689 - acc: 0.97 - ETA: 9:50 - loss: 0.4692 - acc: 0.9692 - ETA: 8:36 - loss: 0.4759 - acc: 0.966 - ETA: 7:22 - loss: 0.4747 - acc: 0.967 - ETA: 6:08 - loss: 0.4717 - acc: 0.969 - ETA: 4:55 - loss: 0.4702 - acc: 0.970 - ETA: 3:41 - loss: 0.4723 - acc: 0.971 - ETA: 2:27 - loss: 0.4708 - acc: 0.968 - ETA: 1:13 - loss: 0.4698 - acc: 0.966 - 2648s 8s/step - loss: 0.4697 - acc: 0.9676 - val_loss: 1.1003 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "340/340 [==============================] - ETA: 40:31 - loss: 0.4572 - acc: 1.00 - ETA: 39:11 - loss: 0.4057 - acc: 1.00 - ETA: 37:59 - loss: 0.3913 - acc: 1.00 - ETA: 36:47 - loss: 0.3792 - acc: 1.00 - ETA: 35:31 - loss: 0.3918 - acc: 1.00 - ETA: 34:33 - loss: 0.3864 - acc: 1.00 - ETA: 33:18 - loss: 0.4023 - acc: 0.98 - ETA: 32:01 - loss: 0.4332 - acc: 0.97 - ETA: 30:46 - loss: 0.4264 - acc: 0.97 - ETA: 29:32 - loss: 0.4256 - acc: 0.98 - ETA: 28:17 - loss: 0.4310 - acc: 0.97 - ETA: 27:04 - loss: 0.4298 - acc: 0.97 - ETA: 25:48 - loss: 0.4261 - acc: 0.97 - ETA: 24:35 - loss: 0.4210 - acc: 0.97 - ETA: 23:21 - loss: 0.4239 - acc: 0.97 - ETA: 22:08 - loss: 0.4680 - acc: 0.94 - ETA: 20:54 - loss: 0.4600 - acc: 0.94 - ETA: 19:42 - loss: 0.4587 - acc: 0.95 - ETA: 18:28 - loss: 0.4529 - acc: 0.95 - ETA: 17:14 - loss: 0.4536 - acc: 0.95 - ETA: 16:00 - loss: 0.4503 - acc: 0.95 - ETA: 14:46 - loss: 0.4465 - acc: 0.95 - ETA: 13:32 - loss: 0.4445 - acc: 0.95 - ETA: 12:18 - loss: 0.4473 - acc: 0.95 - ETA: 11:04 - loss: 0.4517 - acc: 0.95 - ETA: 9:50 - loss: 0.4554 - acc: 0.9500 - ETA: 8:36 - loss: 0.4540 - acc: 0.948 - ETA: 7:22 - loss: 0.4503 - acc: 0.950 - ETA: 6:08 - loss: 0.4473 - acc: 0.951 - ETA: 4:55 - loss: 0.4512 - acc: 0.950 - ETA: 3:41 - loss: 0.4482 - acc: 0.951 - ETA: 2:27 - loss: 0.4485 - acc: 0.950 - ETA: 1:13 - loss: 0.4520 - acc: 0.948 - 2650s 8s/step - loss: 0.4564 - acc: 0.9471 - val_loss: 1.6951 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "340/340 [==============================] - ETA: 40:46 - loss: 0.3261 - acc: 1.00 - ETA: 39:19 - loss: 0.5090 - acc: 0.95 - ETA: 38:03 - loss: 0.4478 - acc: 0.96 - ETA: 36:45 - loss: 0.4247 - acc: 0.97 - ETA: 35:32 - loss: 0.4111 - acc: 0.98 - ETA: 34:17 - loss: 0.4354 - acc: 0.96 - ETA: 33:17 - loss: 0.4303 - acc: 0.97 - ETA: 32:02 - loss: 0.4783 - acc: 0.93 - ETA: 30:45 - loss: 0.4894 - acc: 0.92 - ETA: 29:30 - loss: 0.4747 - acc: 0.93 - ETA: 28:16 - loss: 0.4768 - acc: 0.92 - ETA: 27:02 - loss: 0.4788 - acc: 0.92 - ETA: 25:48 - loss: 0.4722 - acc: 0.93 - ETA: 24:34 - loss: 0.4661 - acc: 0.93 - ETA: 23:20 - loss: 0.4748 - acc: 0.93 - ETA: 22:06 - loss: 0.4688 - acc: 0.93 - ETA: 20:52 - loss: 0.4635 - acc: 0.94 - ETA: 19:39 - loss: 0.4618 - acc: 0.94 - ETA: 18:27 - loss: 0.4720 - acc: 0.93 - ETA: 17:13 - loss: 0.4666 - acc: 0.94 - ETA: 16:00 - loss: 0.4634 - acc: 0.94 - ETA: 14:46 - loss: 0.4624 - acc: 0.94 - ETA: 13:32 - loss: 0.4607 - acc: 0.94 - ETA: 12:18 - loss: 0.4631 - acc: 0.95 - ETA: 11:04 - loss: 0.4625 - acc: 0.95 - ETA: 9:50 - loss: 0.4631 - acc: 0.9500 - ETA: 8:36 - loss: 0.4597 - acc: 0.951 - ETA: 7:22 - loss: 0.4570 - acc: 0.953 - ETA: 6:08 - loss: 0.4537 - acc: 0.955 - ETA: 4:55 - loss: 0.4547 - acc: 0.956 - ETA: 3:41 - loss: 0.4724 - acc: 0.945 - ETA: 2:27 - loss: 0.4711 - acc: 0.946 - ETA: 1:13 - loss: 0.4679 - acc: 0.948 - 2649s 8s/step - loss: 0.4734 - acc: 0.9500 - val_loss: 0.9513 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 43:12 - loss: 0.3952 - acc: 1.00 - ETA: 40:38 - loss: 0.5012 - acc: 1.00 - ETA: 39:31 - loss: 0.4691 - acc: 1.00 - ETA: 38:55 - loss: 0.4573 - acc: 1.00 - ETA: 38:13 - loss: 0.4628 - acc: 1.00 - ETA: 36:59 - loss: 0.4626 - acc: 1.00 - ETA: 35:35 - loss: 0.4462 - acc: 1.00 - ETA: 34:11 - loss: 0.4498 - acc: 1.00 - ETA: 32:44 - loss: 0.4532 - acc: 1.00 - ETA: 31:25 - loss: 0.4597 - acc: 0.99 - ETA: 29:55 - loss: 0.4523 - acc: 0.99 - ETA: 28:34 - loss: 0.4461 - acc: 0.99 - ETA: 27:13 - loss: 0.4447 - acc: 0.99 - ETA: 25:54 - loss: 0.4589 - acc: 0.97 - ETA: 24:33 - loss: 0.4639 - acc: 0.97 - ETA: 23:13 - loss: 0.4629 - acc: 0.97 - ETA: 21:54 - loss: 0.4678 - acc: 0.97 - ETA: 20:35 - loss: 0.4725 - acc: 0.96 - ETA: 19:21 - loss: 0.4739 - acc: 0.96 - ETA: 18:05 - loss: 0.4684 - acc: 0.97 - ETA: 16:51 - loss: 0.4767 - acc: 0.96 - ETA: 15:36 - loss: 0.4756 - acc: 0.96 - ETA: 14:18 - loss: 0.4717 - acc: 0.96 - ETA: 13:01 - loss: 0.4672 - acc: 0.97 - ETA: 11:43 - loss: 0.4758 - acc: 0.96 - ETA: 10:26 - loss: 0.4768 - acc: 0.96 - ETA: 9:08 - loss: 0.4829 - acc: 0.9630 - ETA: 7:49 - loss: 0.4857 - acc: 0.964 - ETA: 6:31 - loss: 0.4878 - acc: 0.958 - ETA: 5:13 - loss: 0.4984 - acc: 0.950 - ETA: 3:55 - loss: 0.5029 - acc: 0.948 - ETA: 2:36 - loss: 0.5131 - acc: 0.940 - ETA: 1:18 - loss: 0.5108 - acc: 0.942 - 2815s 8s/step - loss: 0.5127 - acc: 0.9412 - val_loss: 11.6442 - val_acc: 0.1000\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "340/340 [==============================] - ETA: 42:49 - loss: 0.6034 - acc: 0.90 - ETA: 41:11 - loss: 0.6813 - acc: 0.85 - ETA: 39:48 - loss: 0.5852 - acc: 0.90 - ETA: 38:36 - loss: 0.5876 - acc: 0.87 - ETA: 37:19 - loss: 0.5472 - acc: 0.90 - ETA: 36:00 - loss: 0.5375 - acc: 0.91 - ETA: 34:49 - loss: 0.5464 - acc: 0.92 - ETA: 33:35 - loss: 0.5553 - acc: 0.92 - ETA: 32:20 - loss: 0.5382 - acc: 0.93 - ETA: 31:01 - loss: 0.5278 - acc: 0.94 - ETA: 29:45 - loss: 0.5950 - acc: 0.91 - ETA: 28:29 - loss: 0.5919 - acc: 0.91 - ETA: 27:09 - loss: 0.5874 - acc: 0.91 - ETA: 25:50 - loss: 0.5728 - acc: 0.92 - ETA: 24:34 - loss: 0.5632 - acc: 0.92 - ETA: 23:19 - loss: 0.5692 - acc: 0.92 - ETA: 22:00 - loss: 0.5697 - acc: 0.92 - ETA: 20:50 - loss: 0.5602 - acc: 0.92 - ETA: 19:33 - loss: 0.5574 - acc: 0.93 - ETA: 18:13 - loss: 0.5526 - acc: 0.93 - ETA: 16:54 - loss: 0.5551 - acc: 0.92 - ETA: 15:36 - loss: 0.5543 - acc: 0.93 - ETA: 14:18 - loss: 0.5615 - acc: 0.92 - ETA: 13:01 - loss: 0.5608 - acc: 0.92 - ETA: 11:41 - loss: 0.5727 - acc: 0.92 - ETA: 10:22 - loss: 0.5866 - acc: 0.91 - ETA: 9:03 - loss: 0.6017 - acc: 0.9111 - ETA: 7:46 - loss: 0.5951 - acc: 0.914 - ETA: 6:28 - loss: 0.5940 - acc: 0.913 - ETA: 5:10 - loss: 0.5896 - acc: 0.916 - ETA: 3:53 - loss: 0.5909 - acc: 0.916 - ETA: 2:35 - loss: 0.5910 - acc: 0.918 - ETA: 1:18 - loss: 0.5869 - acc: 0.921 - 2809s 8s/step - loss: 0.5861 - acc: 0.9206 - val_loss: 1.8852 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "340/340 [==============================] - ETA: 42:53 - loss: 0.4702 - acc: 1.00 - ETA: 42:15 - loss: 0.5008 - acc: 0.95 - ETA: 40:46 - loss: 0.4725 - acc: 0.96 - ETA: 38:59 - loss: 0.4664 - acc: 0.97 - ETA: 37:53 - loss: 0.4428 - acc: 0.98 - ETA: 36:27 - loss: 0.4349 - acc: 0.98 - ETA: 35:04 - loss: 0.4420 - acc: 0.98 - ETA: 33:47 - loss: 0.4786 - acc: 0.96 - ETA: 32:20 - loss: 0.4746 - acc: 0.96 - ETA: 31:02 - loss: 0.4701 - acc: 0.97 - ETA: 29:40 - loss: 0.4871 - acc: 0.96 - ETA: 28:16 - loss: 0.4964 - acc: 0.95 - ETA: 26:57 - loss: 0.4872 - acc: 0.96 - ETA: 25:47 - loss: 0.4883 - acc: 0.95 - ETA: 24:41 - loss: 0.4861 - acc: 0.96 - ETA: 23:37 - loss: 0.4909 - acc: 0.95 - ETA: 22:30 - loss: 0.4829 - acc: 0.95 - ETA: 21:18 - loss: 0.4827 - acc: 0.96 - ETA: 20:00 - loss: 0.4777 - acc: 0.96 - ETA: 18:36 - loss: 0.4721 - acc: 0.96 - ETA: 17:14 - loss: 0.4689 - acc: 0.96 - ETA: 15:51 - loss: 0.4646 - acc: 0.96 - ETA: 14:30 - loss: 0.4693 - acc: 0.96 - ETA: 13:08 - loss: 0.4659 - acc: 0.96 - ETA: 11:47 - loss: 0.4791 - acc: 0.96 - ETA: 10:27 - loss: 0.4796 - acc: 0.96 - ETA: 9:08 - loss: 0.4766 - acc: 0.9630 - ETA: 7:48 - loss: 0.4742 - acc: 0.964 - ETA: 6:30 - loss: 0.4725 - acc: 0.965 - ETA: 5:11 - loss: 0.4693 - acc: 0.966 - ETA: 3:53 - loss: 0.4695 - acc: 0.964 - ETA: 2:35 - loss: 0.4686 - acc: 0.965 - ETA: 1:17 - loss: 0.4706 - acc: 0.963 - 2769s 8s/step - loss: 0.4692 - acc: 0.9618 - val_loss: 1.1574 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "340/340 [==============================] - ETA: 39:45 - loss: 0.3431 - acc: 1.00 - ETA: 38:31 - loss: 0.3839 - acc: 1.00 - ETA: 37:55 - loss: 0.3743 - acc: 1.00 - ETA: 36:33 - loss: 0.3899 - acc: 1.00 - ETA: 36:01 - loss: 0.3921 - acc: 1.00 - ETA: 35:15 - loss: 0.3841 - acc: 1.00 - ETA: 34:32 - loss: 0.3781 - acc: 1.00 - ETA: 33:35 - loss: 0.3881 - acc: 0.98 - ETA: 32:42 - loss: 0.3842 - acc: 0.98 - ETA: 31:41 - loss: 0.4027 - acc: 0.98 - ETA: 30:35 - loss: 0.4025 - acc: 0.98 - ETA: 29:25 - loss: 0.3981 - acc: 0.98 - ETA: 28:36 - loss: 0.4173 - acc: 0.96 - ETA: 27:22 - loss: 0.4196 - acc: 0.95 - ETA: 26:05 - loss: 0.4226 - acc: 0.95 - ETA: 24:45 - loss: 0.4183 - acc: 0.95 - ETA: 23:22 - loss: 0.4132 - acc: 0.95 - ETA: 21:56 - loss: 0.4128 - acc: 0.96 - ETA: 20:31 - loss: 0.4111 - acc: 0.96 - ETA: 19:07 - loss: 0.4181 - acc: 0.96 - ETA: 17:43 - loss: 0.4189 - acc: 0.95 - ETA: 16:19 - loss: 0.4177 - acc: 0.95 - ETA: 14:55 - loss: 0.4163 - acc: 0.96 - ETA: 13:33 - loss: 0.4119 - acc: 0.96 - ETA: 12:11 - loss: 0.4131 - acc: 0.96 - ETA: 10:49 - loss: 0.4096 - acc: 0.96 - ETA: 9:27 - loss: 0.4090 - acc: 0.9630 - ETA: 8:06 - loss: 0.4053 - acc: 0.964 - ETA: 6:45 - loss: 0.4040 - acc: 0.965 - ETA: 5:24 - loss: 0.4019 - acc: 0.966 - ETA: 4:03 - loss: 0.4011 - acc: 0.967 - ETA: 2:42 - loss: 0.4000 - acc: 0.968 - ETA: 1:20 - loss: 0.4003 - acc: 0.969 - 2885s 8s/step - loss: 0.4020 - acc: 0.9676 - val_loss: 1.1736 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "340/340 [==============================] - ETA: 40:40 - loss: 0.4224 - acc: 0.90 - ETA: 39:14 - loss: 0.3680 - acc: 0.95 - ETA: 37:58 - loss: 0.3524 - acc: 0.96 - ETA: 36:43 - loss: 0.3584 - acc: 0.97 - ETA: 35:28 - loss: 0.3495 - acc: 0.98 - ETA: 34:16 - loss: 0.3463 - acc: 0.98 - ETA: 33:03 - loss: 0.3396 - acc: 0.98 - ETA: 31:50 - loss: 0.3339 - acc: 0.98 - ETA: 30:35 - loss: 0.3331 - acc: 0.98 - ETA: 29:22 - loss: 0.3358 - acc: 0.99 - ETA: 28:11 - loss: 0.3358 - acc: 0.99 - ETA: 26:57 - loss: 0.3477 - acc: 0.97 - ETA: 25:54 - loss: 0.3437 - acc: 0.97 - ETA: 24:40 - loss: 0.3480 - acc: 0.97 - ETA: 23:26 - loss: 0.3494 - acc: 0.97 - ETA: 22:13 - loss: 0.3466 - acc: 0.97 - ETA: 20:59 - loss: 0.3445 - acc: 0.97 - ETA: 19:45 - loss: 0.3489 - acc: 0.97 - ETA: 18:35 - loss: 0.3509 - acc: 0.97 - ETA: 17:20 - loss: 0.3492 - acc: 0.97 - ETA: 16:04 - loss: 0.3478 - acc: 0.97 - ETA: 14:49 - loss: 0.3461 - acc: 0.97 - ETA: 13:34 - loss: 0.3544 - acc: 0.97 - ETA: 12:20 - loss: 0.3517 - acc: 0.97 - ETA: 11:08 - loss: 0.3520 - acc: 0.97 - ETA: 9:54 - loss: 0.3508 - acc: 0.9769 - ETA: 8:39 - loss: 0.3526 - acc: 0.974 - ETA: 7:25 - loss: 0.3510 - acc: 0.975 - ETA: 6:12 - loss: 0.3519 - acc: 0.972 - ETA: 4:59 - loss: 0.3511 - acc: 0.973 - ETA: 3:44 - loss: 0.3532 - acc: 0.971 - ETA: 2:30 - loss: 0.3578 - acc: 0.968 - ETA: 1:14 - loss: 0.3561 - acc: 0.969 - 2688s 8s/step - loss: 0.3555 - acc: 0.9706 - val_loss: 1.3726 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 40:09 - loss: 0.2881 - acc: 1.00 - ETA: 39:02 - loss: 0.2981 - acc: 1.00 - ETA: 37:45 - loss: 0.2971 - acc: 1.00 - ETA: 36:28 - loss: 0.2943 - acc: 1.00 - ETA: 35:17 - loss: 0.3001 - acc: 1.00 - ETA: 34:01 - loss: 0.2963 - acc: 1.00 - ETA: 32:48 - loss: 0.2987 - acc: 1.00 - ETA: 31:35 - loss: 0.2971 - acc: 1.00 - ETA: 30:21 - loss: 0.3029 - acc: 1.00 - ETA: 29:07 - loss: 0.3014 - acc: 1.00 - ETA: 27:54 - loss: 0.3120 - acc: 1.00 - ETA: 26:46 - loss: 0.3135 - acc: 1.00 - ETA: 25:32 - loss: 0.3136 - acc: 1.00 - ETA: 24:19 - loss: 0.3119 - acc: 1.00 - ETA: 23:06 - loss: 0.3086 - acc: 1.00 - ETA: 21:53 - loss: 0.3173 - acc: 0.99 - ETA: 20:39 - loss: 0.3195 - acc: 0.98 - ETA: 19:26 - loss: 0.3178 - acc: 0.98 - ETA: 18:14 - loss: 0.3234 - acc: 0.98 - ETA: 17:01 - loss: 0.3256 - acc: 0.98 - ETA: 15:48 - loss: 0.3279 - acc: 0.98 - ETA: 14:35 - loss: 0.3260 - acc: 0.98 - ETA: 13:22 - loss: 0.3266 - acc: 0.98 - ETA: 12:10 - loss: 0.3258 - acc: 0.98 - ETA: 10:57 - loss: 0.3278 - acc: 0.98 - ETA: 9:44 - loss: 0.3263 - acc: 0.9808 - ETA: 8:31 - loss: 0.3307 - acc: 0.977 - ETA: 7:17 - loss: 0.3351 - acc: 0.975 - ETA: 6:04 - loss: 0.3371 - acc: 0.972 - ETA: 4:51 - loss: 0.3358 - acc: 0.973 - ETA: 3:38 - loss: 0.3364 - acc: 0.974 - ETA: 2:25 - loss: 0.3388 - acc: 0.975 - ETA: 1:12 - loss: 0.3433 - acc: 0.969 - 2621s 8s/step - loss: 0.3482 - acc: 0.9676 - val_loss: 1.5382 - val_acc: 0.5167\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "340/340 [==============================] - ETA: 40:57 - loss: 0.2942 - acc: 1.00 - ETA: 39:10 - loss: 0.2881 - acc: 1.00 - ETA: 37:52 - loss: 0.2956 - acc: 1.00 - ETA: 36:34 - loss: 0.3425 - acc: 0.97 - ETA: 35:17 - loss: 0.3348 - acc: 0.98 - ETA: 34:06 - loss: 0.4021 - acc: 0.95 - ETA: 32:53 - loss: 0.3918 - acc: 0.95 - ETA: 31:39 - loss: 0.3872 - acc: 0.96 - ETA: 30:25 - loss: 0.3812 - acc: 0.96 - ETA: 29:11 - loss: 0.4035 - acc: 0.95 - ETA: 27:57 - loss: 0.3968 - acc: 0.95 - ETA: 26:44 - loss: 0.3969 - acc: 0.95 - ETA: 25:35 - loss: 0.3884 - acc: 0.96 - ETA: 24:20 - loss: 0.3887 - acc: 0.96 - ETA: 23:07 - loss: 0.3906 - acc: 0.96 - ETA: 21:54 - loss: 0.3844 - acc: 0.96 - ETA: 20:41 - loss: 0.3794 - acc: 0.97 - ETA: 19:27 - loss: 0.3833 - acc: 0.96 - ETA: 18:14 - loss: 0.3807 - acc: 0.96 - ETA: 17:01 - loss: 0.3904 - acc: 0.96 - ETA: 15:47 - loss: 0.3936 - acc: 0.96 - ETA: 14:34 - loss: 0.3927 - acc: 0.96 - ETA: 13:21 - loss: 0.4134 - acc: 0.95 - ETA: 12:09 - loss: 0.4222 - acc: 0.95 - ETA: 10:56 - loss: 0.4214 - acc: 0.95 - ETA: 9:43 - loss: 0.4178 - acc: 0.9538 - ETA: 8:30 - loss: 0.4227 - acc: 0.955 - ETA: 7:17 - loss: 0.4209 - acc: 0.957 - ETA: 6:04 - loss: 0.4288 - acc: 0.948 - ETA: 4:51 - loss: 0.4380 - acc: 0.946 - ETA: 3:38 - loss: 0.4362 - acc: 0.948 - ETA: 2:25 - loss: 0.4400 - acc: 0.946 - ETA: 1:12 - loss: 0.4549 - acc: 0.936 - 2616s 8s/step - loss: 0.4641 - acc: 0.9324 - val_loss: 9.3241 - val_acc: 0.3167\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "340/340 [==============================] - ETA: 40:07 - loss: 0.8280 - acc: 0.80 - ETA: 39:46 - loss: 0.6377 - acc: 0.85 - ETA: 38:12 - loss: 0.8593 - acc: 0.76 - ETA: 36:51 - loss: 0.7803 - acc: 0.80 - ETA: 35:31 - loss: 0.7435 - acc: 0.80 - ETA: 34:15 - loss: 0.7480 - acc: 0.80 - ETA: 32:58 - loss: 0.7694 - acc: 0.78 - ETA: 31:44 - loss: 0.7576 - acc: 0.80 - ETA: 30:30 - loss: 0.7250 - acc: 0.82 - ETA: 29:15 - loss: 0.7407 - acc: 0.81 - ETA: 28:02 - loss: 0.7281 - acc: 0.82 - ETA: 26:48 - loss: 0.7028 - acc: 0.84 - ETA: 25:34 - loss: 0.6991 - acc: 0.84 - ETA: 24:21 - loss: 0.6880 - acc: 0.85 - ETA: 23:08 - loss: 0.6728 - acc: 0.86 - ETA: 21:54 - loss: 0.6618 - acc: 0.86 - ETA: 20:41 - loss: 0.6604 - acc: 0.86 - ETA: 19:28 - loss: 0.6516 - acc: 0.87 - ETA: 18:15 - loss: 0.6434 - acc: 0.87 - ETA: 17:01 - loss: 0.6331 - acc: 0.88 - ETA: 15:48 - loss: 0.6312 - acc: 0.88 - ETA: 14:35 - loss: 0.6267 - acc: 0.89 - ETA: 13:22 - loss: 0.6255 - acc: 0.89 - ETA: 12:09 - loss: 0.6189 - acc: 0.89 - ETA: 10:56 - loss: 0.6157 - acc: 0.89 - ETA: 9:43 - loss: 0.6101 - acc: 0.9000 - ETA: 8:31 - loss: 0.6092 - acc: 0.903 - ETA: 7:18 - loss: 0.6039 - acc: 0.907 - ETA: 6:05 - loss: 0.6032 - acc: 0.903 - ETA: 4:52 - loss: 0.6010 - acc: 0.903 - ETA: 3:39 - loss: 0.6002 - acc: 0.900 - ETA: 2:26 - loss: 0.5954 - acc: 0.903 - ETA: 1:12 - loss: 0.5945 - acc: 0.900 - 2622s 8s/step - loss: 0.5893 - acc: 0.9029 - val_loss: 5.9066 - val_acc: 0.3500\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "340/340 [==============================] - ETA: 40:02 - loss: 0.5254 - acc: 0.90 - ETA: 38:50 - loss: 0.4959 - acc: 0.90 - ETA: 37:39 - loss: 0.4799 - acc: 0.93 - ETA: 36:25 - loss: 0.4809 - acc: 0.92 - ETA: 35:15 - loss: 0.4814 - acc: 0.94 - ETA: 34:01 - loss: 0.4929 - acc: 0.93 - ETA: 32:47 - loss: 0.4923 - acc: 0.92 - ETA: 31:34 - loss: 0.4782 - acc: 0.93 - ETA: 30:20 - loss: 0.4694 - acc: 0.94 - ETA: 29:07 - loss: 0.4769 - acc: 0.95 - ETA: 27:54 - loss: 0.4780 - acc: 0.95 - ETA: 26:41 - loss: 0.4847 - acc: 0.95 - ETA: 25:28 - loss: 0.4798 - acc: 0.95 - ETA: 24:15 - loss: 0.4751 - acc: 0.95 - ETA: 23:03 - loss: 0.4708 - acc: 0.96 - ETA: 21:54 - loss: 0.4867 - acc: 0.95 - ETA: 20:40 - loss: 0.4833 - acc: 0.95 - ETA: 19:27 - loss: 0.4862 - acc: 0.95 - ETA: 18:14 - loss: 0.4829 - acc: 0.95 - ETA: 17:01 - loss: 0.4767 - acc: 0.95 - ETA: 15:48 - loss: 0.4725 - acc: 0.95 - ETA: 14:35 - loss: 0.4677 - acc: 0.95 - ETA: 13:22 - loss: 0.4636 - acc: 0.96 - ETA: 12:09 - loss: 0.4696 - acc: 0.95 - ETA: 10:56 - loss: 0.4657 - acc: 0.96 - ETA: 9:43 - loss: 0.4635 - acc: 0.9615 - ETA: 8:30 - loss: 0.4602 - acc: 0.963 - ETA: 7:17 - loss: 0.4566 - acc: 0.964 - ETA: 6:04 - loss: 0.4535 - acc: 0.965 - ETA: 4:51 - loss: 0.4514 - acc: 0.966 - ETA: 3:38 - loss: 0.4499 - acc: 0.967 - ETA: 2:25 - loss: 0.4513 - acc: 0.965 - ETA: 1:12 - loss: 0.4478 - acc: 0.966 - 2620s 8s/step - loss: 0.4456 - acc: 0.9676 - val_loss: 1.3117 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "340/340 [==============================] - ETA: 40:02 - loss: 0.5556 - acc: 0.90 - ETA: 38:51 - loss: 0.4527 - acc: 0.95 - ETA: 37:39 - loss: 0.4308 - acc: 0.96 - ETA: 36:27 - loss: 0.4247 - acc: 0.97 - ETA: 35:38 - loss: 0.4187 - acc: 0.98 - ETA: 34:20 - loss: 0.4130 - acc: 0.98 - ETA: 33:03 - loss: 0.4028 - acc: 0.98 - ETA: 31:47 - loss: 0.3955 - acc: 0.98 - ETA: 30:31 - loss: 0.3895 - acc: 0.98 - ETA: 29:16 - loss: 0.3860 - acc: 0.99 - ETA: 28:01 - loss: 0.3879 - acc: 0.99 - ETA: 26:47 - loss: 0.3855 - acc: 0.99 - ETA: 25:34 - loss: 0.4038 - acc: 0.97 - ETA: 24:20 - loss: 0.3992 - acc: 0.97 - ETA: 23:07 - loss: 0.3977 - acc: 0.98 - ETA: 21:53 - loss: 0.3988 - acc: 0.97 - ETA: 20:41 - loss: 0.3981 - acc: 0.97 - ETA: 19:28 - loss: 0.3966 - acc: 0.97 - ETA: 18:15 - loss: 0.3943 - acc: 0.97 - ETA: 17:01 - loss: 0.3958 - acc: 0.98 - ETA: 15:48 - loss: 0.3940 - acc: 0.98 - ETA: 14:35 - loss: 0.4022 - acc: 0.97 - ETA: 13:22 - loss: 0.4015 - acc: 0.97 - ETA: 12:09 - loss: 0.3982 - acc: 0.97 - ETA: 10:56 - loss: 0.3980 - acc: 0.98 - ETA: 9:43 - loss: 0.3951 - acc: 0.9808 - ETA: 8:30 - loss: 0.3932 - acc: 0.981 - ETA: 7:17 - loss: 0.3914 - acc: 0.982 - ETA: 6:04 - loss: 0.3927 - acc: 0.979 - ETA: 4:51 - loss: 0.3958 - acc: 0.980 - ETA: 3:38 - loss: 0.3948 - acc: 0.980 - ETA: 2:25 - loss: 0.3938 - acc: 0.981 - ETA: 1:12 - loss: 0.3918 - acc: 0.981 - 2620s 8s/step - loss: 0.3925 - acc: 0.9824 - val_loss: 0.7229 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.85075 to 0.72294, saving model to imagemodel4.1.h5\n",
      "Epoch 41/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:49 - loss: 0.3515 - acc: 1.00 - ETA: 38:49 - loss: 0.4194 - acc: 0.95 - ETA: 37:42 - loss: 0.3938 - acc: 0.96 - ETA: 36:36 - loss: 0.3901 - acc: 0.97 - ETA: 35:21 - loss: 0.3865 - acc: 0.98 - ETA: 34:17 - loss: 0.4387 - acc: 0.96 - ETA: 33:00 - loss: 0.4236 - acc: 0.97 - ETA: 31:45 - loss: 0.4163 - acc: 0.97 - ETA: 30:30 - loss: 0.4080 - acc: 0.97 - ETA: 29:15 - loss: 0.4029 - acc: 0.98 - ETA: 28:01 - loss: 0.4016 - acc: 0.98 - ETA: 26:47 - loss: 0.4023 - acc: 0.98 - ETA: 25:34 - loss: 0.3958 - acc: 0.98 - ETA: 24:21 - loss: 0.3949 - acc: 0.98 - ETA: 23:07 - loss: 0.3922 - acc: 0.98 - ETA: 21:54 - loss: 0.4130 - acc: 0.97 - ETA: 20:41 - loss: 0.4106 - acc: 0.97 - ETA: 19:28 - loss: 0.4133 - acc: 0.97 - ETA: 18:15 - loss: 0.4083 - acc: 0.97 - ETA: 17:02 - loss: 0.4037 - acc: 0.97 - ETA: 15:49 - loss: 0.4004 - acc: 0.97 - ETA: 14:36 - loss: 0.3966 - acc: 0.97 - ETA: 13:23 - loss: 0.4010 - acc: 0.97 - ETA: 12:09 - loss: 0.3981 - acc: 0.97 - ETA: 10:56 - loss: 0.3951 - acc: 0.98 - ETA: 9:43 - loss: 0.3926 - acc: 0.9808 - ETA: 8:30 - loss: 0.3914 - acc: 0.981 - ETA: 7:17 - loss: 0.3892 - acc: 0.982 - ETA: 6:04 - loss: 0.3871 - acc: 0.982 - ETA: 4:51 - loss: 0.3862 - acc: 0.983 - ETA: 3:39 - loss: 0.3840 - acc: 0.983 - ETA: 2:26 - loss: 0.3897 - acc: 0.981 - ETA: 1:13 - loss: 0.3873 - acc: 0.981 - 2623s 8s/step - loss: 0.3857 - acc: 0.9824 - val_loss: 0.7489 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "340/340 [==============================] - ETA: 40:01 - loss: 0.3709 - acc: 1.00 - ETA: 38:55 - loss: 0.3448 - acc: 1.00 - ETA: 37:41 - loss: 0.3401 - acc: 1.00 - ETA: 36:26 - loss: 0.3560 - acc: 1.00 - ETA: 35:13 - loss: 0.3454 - acc: 1.00 - ETA: 34:00 - loss: 0.3393 - acc: 1.00 - ETA: 32:45 - loss: 0.3411 - acc: 1.00 - ETA: 31:31 - loss: 0.3370 - acc: 1.00 - ETA: 30:19 - loss: 0.3383 - acc: 1.00 - ETA: 29:05 - loss: 0.3347 - acc: 1.00 - ETA: 27:51 - loss: 0.3367 - acc: 1.00 - ETA: 26:39 - loss: 0.3350 - acc: 1.00 - ETA: 25:28 - loss: 0.3354 - acc: 1.00 - ETA: 24:15 - loss: 0.3338 - acc: 1.00 - ETA: 23:03 - loss: 0.3323 - acc: 1.00 - ETA: 21:50 - loss: 0.3307 - acc: 1.00 - ETA: 20:37 - loss: 0.3296 - acc: 1.00 - ETA: 19:23 - loss: 0.3324 - acc: 1.00 - ETA: 18:11 - loss: 0.3317 - acc: 1.00 - ETA: 17:01 - loss: 0.3328 - acc: 1.00 - ETA: 15:48 - loss: 0.3317 - acc: 1.00 - ETA: 14:35 - loss: 0.3329 - acc: 1.00 - ETA: 13:22 - loss: 0.3319 - acc: 1.00 - ETA: 12:09 - loss: 0.3318 - acc: 1.00 - ETA: 10:56 - loss: 0.3316 - acc: 1.00 - ETA: 9:43 - loss: 0.3305 - acc: 1.0000 - ETA: 8:30 - loss: 0.3301 - acc: 1.000 - ETA: 7:17 - loss: 0.3294 - acc: 1.000 - ETA: 6:04 - loss: 0.3281 - acc: 1.000 - ETA: 4:51 - loss: 0.3283 - acc: 1.000 - ETA: 3:38 - loss: 0.3272 - acc: 1.000 - ETA: 2:25 - loss: 0.3296 - acc: 0.996 - ETA: 1:12 - loss: 0.3293 - acc: 0.997 - 2618s 8s/step - loss: 0.3295 - acc: 0.9971 - val_loss: 0.7598 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "340/340 [==============================] - ETA: 40:01 - loss: 0.3010 - acc: 1.00 - ETA: 38:45 - loss: 0.3042 - acc: 1.00 - ETA: 37:33 - loss: 0.3243 - acc: 1.00 - ETA: 36:20 - loss: 0.3205 - acc: 1.00 - ETA: 35:09 - loss: 0.3155 - acc: 1.00 - ETA: 33:58 - loss: 0.3133 - acc: 1.00 - ETA: 32:45 - loss: 0.3103 - acc: 1.00 - ETA: 31:33 - loss: 0.3140 - acc: 1.00 - ETA: 30:31 - loss: 0.3184 - acc: 1.00 - ETA: 29:16 - loss: 0.3208 - acc: 1.00 - ETA: 28:01 - loss: 0.3271 - acc: 1.00 - ETA: 26:47 - loss: 0.3241 - acc: 1.00 - ETA: 25:33 - loss: 0.3224 - acc: 1.00 - ETA: 24:19 - loss: 0.3211 - acc: 1.00 - ETA: 23:06 - loss: 0.3456 - acc: 0.99 - ETA: 21:53 - loss: 0.3421 - acc: 0.99 - ETA: 20:40 - loss: 0.3396 - acc: 0.99 - ETA: 19:27 - loss: 0.3374 - acc: 0.99 - ETA: 18:14 - loss: 0.3370 - acc: 0.99 - ETA: 17:00 - loss: 0.3360 - acc: 0.99 - ETA: 15:48 - loss: 0.3376 - acc: 0.99 - ETA: 14:35 - loss: 0.3354 - acc: 0.99 - ETA: 13:22 - loss: 0.3332 - acc: 0.99 - ETA: 12:09 - loss: 0.3329 - acc: 0.99 - ETA: 10:56 - loss: 0.3310 - acc: 0.99 - ETA: 9:43 - loss: 0.3392 - acc: 0.9923 - ETA: 8:30 - loss: 0.3385 - acc: 0.992 - ETA: 7:17 - loss: 0.3369 - acc: 0.992 - ETA: 6:04 - loss: 0.3373 - acc: 0.993 - ETA: 4:51 - loss: 0.3357 - acc: 0.993 - ETA: 3:38 - loss: 0.3353 - acc: 0.993 - ETA: 2:25 - loss: 0.3349 - acc: 0.993 - ETA: 1:12 - loss: 0.3336 - acc: 0.993 - 2622s 8s/step - loss: 0.3329 - acc: 0.9941 - val_loss: 0.6752 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.72294 to 0.67519, saving model to imagemodel4.1.h5\n",
      "Epoch 44/300\n",
      "340/340 [==============================] - ETA: 39:50 - loss: 0.2903 - acc: 1.00 - ETA: 38:40 - loss: 0.2871 - acc: 1.00 - ETA: 37:37 - loss: 0.2905 - acc: 1.00 - ETA: 36:21 - loss: 0.2991 - acc: 1.00 - ETA: 35:08 - loss: 0.2965 - acc: 1.00 - ETA: 33:56 - loss: 0.2963 - acc: 1.00 - ETA: 32:45 - loss: 0.2969 - acc: 1.00 - ETA: 31:31 - loss: 0.3056 - acc: 0.98 - ETA: 30:19 - loss: 0.3027 - acc: 0.98 - ETA: 29:10 - loss: 0.3047 - acc: 0.99 - ETA: 27:57 - loss: 0.3043 - acc: 0.99 - ETA: 26:44 - loss: 0.3020 - acc: 0.99 - ETA: 25:31 - loss: 0.3007 - acc: 0.99 - ETA: 24:18 - loss: 0.3208 - acc: 0.98 - ETA: 23:05 - loss: 0.3189 - acc: 0.98 - ETA: 21:52 - loss: 0.3192 - acc: 0.98 - ETA: 20:39 - loss: 0.3174 - acc: 0.98 - ETA: 19:26 - loss: 0.3159 - acc: 0.98 - ETA: 18:12 - loss: 0.3143 - acc: 0.98 - ETA: 16:59 - loss: 0.3133 - acc: 0.99 - ETA: 15:47 - loss: 0.3125 - acc: 0.99 - ETA: 14:34 - loss: 0.3138 - acc: 0.99 - ETA: 13:22 - loss: 0.3126 - acc: 0.99 - ETA: 12:09 - loss: 0.3138 - acc: 0.99 - ETA: 10:56 - loss: 0.3161 - acc: 0.99 - ETA: 9:43 - loss: 0.3208 - acc: 0.9885 - ETA: 8:30 - loss: 0.3208 - acc: 0.988 - ETA: 7:17 - loss: 0.3195 - acc: 0.989 - ETA: 6:04 - loss: 0.3184 - acc: 0.989 - ETA: 4:51 - loss: 0.3178 - acc: 0.990 - ETA: 3:38 - loss: 0.3168 - acc: 0.990 - ETA: 2:25 - loss: 0.3161 - acc: 0.990 - ETA: 1:12 - loss: 0.3149 - acc: 0.990 - 2621s 8s/step - loss: 0.3164 - acc: 0.9912 - val_loss: 0.8564 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "340/340 [==============================] - ETA: 40:07 - loss: 0.4175 - acc: 1.00 - ETA: 38:52 - loss: 0.3487 - acc: 1.00 - ETA: 37:40 - loss: 0.3450 - acc: 1.00 - ETA: 36:27 - loss: 0.3316 - acc: 1.00 - ETA: 35:12 - loss: 0.3325 - acc: 1.00 - ETA: 33:57 - loss: 0.3243 - acc: 1.00 - ETA: 32:45 - loss: 0.3254 - acc: 1.00 - ETA: 31:33 - loss: 0.3187 - acc: 1.00 - ETA: 30:20 - loss: 0.3191 - acc: 1.00 - ETA: 29:07 - loss: 0.3142 - acc: 1.00 - ETA: 27:54 - loss: 0.3118 - acc: 1.00 - ETA: 26:45 - loss: 0.3083 - acc: 1.00 - ETA: 25:31 - loss: 0.3096 - acc: 1.00 - ETA: 24:18 - loss: 0.3078 - acc: 1.00 - ETA: 23:05 - loss: 0.3058 - acc: 1.00 - ETA: 21:52 - loss: 0.3084 - acc: 1.00 - ETA: 20:40 - loss: 0.3061 - acc: 1.00 - ETA: 19:26 - loss: 0.3040 - acc: 1.00 - ETA: 18:13 - loss: 0.3070 - acc: 1.00 - ETA: 17:00 - loss: 0.3049 - acc: 1.00 - ETA: 15:47 - loss: 0.3039 - acc: 1.00 - ETA: 14:34 - loss: 0.3030 - acc: 1.00 - ETA: 13:21 - loss: 0.3022 - acc: 1.00 - ETA: 12:09 - loss: 0.3008 - acc: 1.00 - ETA: 10:56 - loss: 0.3008 - acc: 1.00 - ETA: 9:43 - loss: 0.3093 - acc: 0.9962 - ETA: 8:30 - loss: 0.3079 - acc: 0.996 - ETA: 7:17 - loss: 0.3066 - acc: 0.996 - ETA: 6:04 - loss: 0.3083 - acc: 0.996 - ETA: 4:51 - loss: 0.3085 - acc: 0.996 - ETA: 3:38 - loss: 0.3072 - acc: 0.996 - ETA: 2:25 - loss: 0.3068 - acc: 0.996 - ETA: 1:12 - loss: 0.3061 - acc: 0.997 - 2619s 8s/step - loss: 0.3051 - acc: 0.9971 - val_loss: 0.7475 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 40:35 - loss: 0.3147 - acc: 1.00 - ETA: 39:08 - loss: 0.3058 - acc: 1.00 - ETA: 37:56 - loss: 0.3323 - acc: 1.00 - ETA: 36:37 - loss: 0.3465 - acc: 0.97 - ETA: 35:19 - loss: 0.3317 - acc: 0.98 - ETA: 34:04 - loss: 0.3199 - acc: 0.98 - ETA: 32:50 - loss: 0.3117 - acc: 0.98 - ETA: 31:35 - loss: 0.3096 - acc: 0.98 - ETA: 30:23 - loss: 0.3062 - acc: 0.98 - ETA: 29:09 - loss: 0.3118 - acc: 0.99 - ETA: 27:57 - loss: 0.3085 - acc: 0.99 - ETA: 26:43 - loss: 0.3062 - acc: 0.99 - ETA: 25:37 - loss: 0.3029 - acc: 0.99 - ETA: 24:23 - loss: 0.3024 - acc: 0.99 - ETA: 23:09 - loss: 0.2999 - acc: 0.99 - ETA: 21:56 - loss: 0.2984 - acc: 0.99 - ETA: 20:42 - loss: 0.3027 - acc: 0.98 - ETA: 19:29 - loss: 0.3008 - acc: 0.98 - ETA: 18:16 - loss: 0.2985 - acc: 0.98 - ETA: 17:03 - loss: 0.2975 - acc: 0.99 - ETA: 15:49 - loss: 0.2972 - acc: 0.99 - ETA: 14:36 - loss: 0.2975 - acc: 0.99 - ETA: 13:23 - loss: 0.2962 - acc: 0.99 - ETA: 12:10 - loss: 0.2956 - acc: 0.99 - ETA: 10:57 - loss: 0.2942 - acc: 0.99 - ETA: 9:44 - loss: 0.2929 - acc: 0.9923 - ETA: 8:31 - loss: 0.2917 - acc: 0.992 - ETA: 7:18 - loss: 0.2910 - acc: 0.992 - ETA: 6:04 - loss: 0.2898 - acc: 0.993 - ETA: 4:52 - loss: 0.2892 - acc: 0.993 - ETA: 3:38 - loss: 0.2886 - acc: 0.993 - ETA: 2:25 - loss: 0.2875 - acc: 0.993 - ETA: 1:12 - loss: 0.2871 - acc: 0.993 - 2622s 8s/step - loss: 0.2864 - acc: 0.9941 - val_loss: 0.8786 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "340/340 [==============================] - ETA: 39:46 - loss: 0.2595 - acc: 1.00 - ETA: 39:43 - loss: 0.2569 - acc: 1.00 - ETA: 38:08 - loss: 0.2700 - acc: 1.00 - ETA: 36:48 - loss: 0.2651 - acc: 1.00 - ETA: 35:29 - loss: 0.2695 - acc: 1.00 - ETA: 34:13 - loss: 0.2676 - acc: 1.00 - ETA: 32:58 - loss: 0.2654 - acc: 1.00 - ETA: 31:43 - loss: 0.2639 - acc: 1.00 - ETA: 30:28 - loss: 0.2638 - acc: 1.00 - ETA: 29:14 - loss: 0.2626 - acc: 1.00 - ETA: 28:01 - loss: 0.2669 - acc: 1.00 - ETA: 26:47 - loss: 0.2658 - acc: 1.00 - ETA: 25:34 - loss: 0.2652 - acc: 1.00 - ETA: 24:22 - loss: 0.2641 - acc: 1.00 - ETA: 23:08 - loss: 0.2655 - acc: 1.00 - ETA: 21:55 - loss: 0.2656 - acc: 1.00 - ETA: 20:42 - loss: 0.2648 - acc: 1.00 - ETA: 19:29 - loss: 0.2646 - acc: 1.00 - ETA: 18:15 - loss: 0.2643 - acc: 1.00 - ETA: 17:02 - loss: 0.2636 - acc: 1.00 - ETA: 15:49 - loss: 0.2636 - acc: 1.00 - ETA: 14:36 - loss: 0.2634 - acc: 1.00 - ETA: 13:23 - loss: 0.2629 - acc: 1.00 - ETA: 12:09 - loss: 0.2632 - acc: 1.00 - ETA: 10:56 - loss: 0.2625 - acc: 1.00 - ETA: 9:43 - loss: 0.2618 - acc: 1.0000 - ETA: 8:31 - loss: 0.2637 - acc: 1.000 - ETA: 7:18 - loss: 0.2638 - acc: 1.000 - ETA: 6:05 - loss: 0.2640 - acc: 1.000 - ETA: 4:52 - loss: 0.2635 - acc: 1.000 - ETA: 3:39 - loss: 0.2630 - acc: 1.000 - ETA: 2:26 - loss: 0.2628 - acc: 1.000 - ETA: 1:13 - loss: 0.2627 - acc: 1.000 - 2639s 8s/step - loss: 0.2632 - acc: 1.0000 - val_loss: 0.9171 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "340/340 [==============================] - ETA: 40:40 - loss: 0.2432 - acc: 1.00 - ETA: 40:09 - loss: 0.2446 - acc: 1.00 - ETA: 38:41 - loss: 0.2445 - acc: 1.00 - ETA: 37:08 - loss: 0.2481 - acc: 1.00 - ETA: 35:44 - loss: 0.2468 - acc: 1.00 - ETA: 34:24 - loss: 0.2464 - acc: 1.00 - ETA: 33:20 - loss: 0.2486 - acc: 1.00 - ETA: 32:06 - loss: 0.2476 - acc: 1.00 - ETA: 31:37 - loss: 0.2483 - acc: 1.00 - ETA: 30:43 - loss: 0.2474 - acc: 1.00 - ETA: 29:42 - loss: 0.2473 - acc: 1.00 - ETA: 29:15 - loss: 0.2520 - acc: 1.00 - ETA: 28:33 - loss: 0.2538 - acc: 1.00 - ETA: 27:24 - loss: 0.2527 - acc: 1.00 - ETA: 25:47 - loss: 0.2522 - acc: 1.00 - ETA: 24:12 - loss: 0.2514 - acc: 1.00 - ETA: 22:41 - loss: 0.2509 - acc: 1.00 - ETA: 21:16 - loss: 0.2504 - acc: 1.00 - ETA: 19:48 - loss: 0.2505 - acc: 1.00 - ETA: 18:23 - loss: 0.2503 - acc: 1.00 - ETA: 16:59 - loss: 0.2500 - acc: 1.00 - ETA: 15:36 - loss: 0.2496 - acc: 1.00 - ETA: 14:14 - loss: 0.2491 - acc: 1.00 - ETA: 12:53 - loss: 0.2493 - acc: 1.00 - ETA: 11:33 - loss: 0.2491 - acc: 1.00 - ETA: 10:14 - loss: 0.2486 - acc: 1.00 - ETA: 8:55 - loss: 0.2483 - acc: 1.0000 - ETA: 7:37 - loss: 0.2480 - acc: 1.000 - ETA: 6:20 - loss: 0.2477 - acc: 1.000 - ETA: 5:03 - loss: 0.2478 - acc: 1.000 - ETA: 3:47 - loss: 0.2478 - acc: 1.000 - ETA: 2:32 - loss: 0.2481 - acc: 1.000 - ETA: 1:15 - loss: 0.2477 - acc: 1.000 - 2715s 8s/step - loss: 0.2476 - acc: 1.0000 - val_loss: 0.9234 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "340/340 [==============================] - ETA: 38:28 - loss: 0.2456 - acc: 1.00 - ETA: 37:17 - loss: 0.2432 - acc: 1.00 - ETA: 36:06 - loss: 0.2409 - acc: 1.00 - ETA: 35:03 - loss: 0.2501 - acc: 1.00 - ETA: 33:51 - loss: 0.2504 - acc: 1.00 - ETA: 32:40 - loss: 0.2480 - acc: 1.00 - ETA: 31:28 - loss: 0.2461 - acc: 1.00 - ETA: 30:22 - loss: 0.2446 - acc: 1.00 - ETA: 29:11 - loss: 0.2433 - acc: 1.00 - ETA: 28:01 - loss: 0.2440 - acc: 1.00 - ETA: 26:50 - loss: 0.2459 - acc: 1.00 - ETA: 25:40 - loss: 0.2453 - acc: 1.00 - ETA: 24:30 - loss: 0.2444 - acc: 1.00 - ETA: 23:20 - loss: 0.2449 - acc: 1.00 - ETA: 22:09 - loss: 0.2450 - acc: 1.00 - ETA: 20:59 - loss: 0.2440 - acc: 1.00 - ETA: 19:49 - loss: 0.2452 - acc: 1.00 - ETA: 18:39 - loss: 0.2445 - acc: 1.00 - ETA: 17:29 - loss: 0.2456 - acc: 1.00 - ETA: 16:19 - loss: 0.2454 - acc: 1.00 - ETA: 15:09 - loss: 0.2450 - acc: 1.00 - ETA: 13:59 - loss: 0.2446 - acc: 1.00 - ETA: 12:49 - loss: 0.2447 - acc: 1.00 - ETA: 11:39 - loss: 0.2443 - acc: 1.00 - ETA: 10:29 - loss: 0.2453 - acc: 1.00 - ETA: 9:19 - loss: 0.2447 - acc: 1.0000 - ETA: 8:09 - loss: 0.2457 - acc: 1.000 - ETA: 6:59 - loss: 0.2451 - acc: 1.000 - ETA: 5:49 - loss: 0.2446 - acc: 1.000 - ETA: 4:39 - loss: 0.2442 - acc: 1.000 - ETA: 3:29 - loss: 0.2441 - acc: 1.000 - ETA: 2:19 - loss: 0.2443 - acc: 1.000 - ETA: 1:09 - loss: 0.2453 - acc: 1.000 - 2515s 7s/step - loss: 0.2467 - acc: 1.0000 - val_loss: 0.9611 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "340/340 [==============================] - ETA: 38:23 - loss: 0.2266 - acc: 1.00 - ETA: 37:07 - loss: 0.2286 - acc: 1.00 - ETA: 35:57 - loss: 0.2292 - acc: 1.00 - ETA: 34:49 - loss: 0.2296 - acc: 1.00 - ETA: 33:40 - loss: 0.2425 - acc: 1.00 - ETA: 32:32 - loss: 0.2420 - acc: 1.00 - ETA: 31:21 - loss: 0.2409 - acc: 1.00 - ETA: 30:10 - loss: 0.2404 - acc: 1.00 - ETA: 29:00 - loss: 0.2401 - acc: 1.00 - ETA: 27:53 - loss: 0.2397 - acc: 1.00 - ETA: 26:44 - loss: 0.2392 - acc: 1.00 - ETA: 25:34 - loss: 0.2436 - acc: 1.00 - ETA: 24:24 - loss: 0.2422 - acc: 1.00 - ETA: 23:14 - loss: 0.2418 - acc: 1.00 - ETA: 22:05 - loss: 0.2422 - acc: 1.00 - ETA: 20:55 - loss: 0.2413 - acc: 1.00 - ETA: 19:46 - loss: 0.2405 - acc: 1.00 - ETA: 18:36 - loss: 0.2399 - acc: 1.00 - ETA: 17:27 - loss: 0.2393 - acc: 1.00 - ETA: 16:17 - loss: 0.2392 - acc: 1.00 - ETA: 15:07 - loss: 0.2400 - acc: 1.00 - ETA: 13:57 - loss: 0.2393 - acc: 1.00 - ETA: 12:48 - loss: 0.2389 - acc: 1.00 - ETA: 11:38 - loss: 0.2386 - acc: 1.00 - ETA: 10:28 - loss: 0.2385 - acc: 1.00 - ETA: 9:18 - loss: 0.2419 - acc: 1.0000 - ETA: 8:08 - loss: 0.2413 - acc: 1.000 - ETA: 6:59 - loss: 0.2408 - acc: 1.000 - ETA: 5:49 - loss: 0.2404 - acc: 1.000 - ETA: 4:39 - loss: 0.2401 - acc: 1.000 - ETA: 3:29 - loss: 0.2396 - acc: 1.000 - ETA: 2:19 - loss: 0.2396 - acc: 1.000 - ETA: 1:09 - loss: 0.2391 - acc: 1.000 - 2510s 7s/step - loss: 0.2387 - acc: 1.0000 - val_loss: 0.8813 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 38:12 - loss: 0.2216 - acc: 1.00 - ETA: 37:01 - loss: 0.2233 - acc: 1.00 - ETA: 35:53 - loss: 0.2275 - acc: 1.00 - ETA: 34:46 - loss: 0.2265 - acc: 1.00 - ETA: 33:37 - loss: 0.2257 - acc: 1.00 - ETA: 32:30 - loss: 0.2260 - acc: 1.00 - ETA: 31:20 - loss: 0.2260 - acc: 1.00 - ETA: 30:10 - loss: 0.2254 - acc: 1.00 - ETA: 29:00 - loss: 0.2252 - acc: 1.00 - ETA: 27:50 - loss: 0.2249 - acc: 1.00 - ETA: 26:42 - loss: 0.2282 - acc: 1.00 - ETA: 25:33 - loss: 0.2285 - acc: 1.00 - ETA: 24:26 - loss: 0.2287 - acc: 1.00 - ETA: 23:16 - loss: 0.2296 - acc: 1.00 - ETA: 22:06 - loss: 0.2300 - acc: 1.00 - ETA: 20:56 - loss: 0.2353 - acc: 1.00 - ETA: 19:45 - loss: 0.2351 - acc: 1.00 - ETA: 18:36 - loss: 0.2347 - acc: 1.00 - ETA: 17:26 - loss: 0.2341 - acc: 1.00 - ETA: 16:16 - loss: 0.2344 - acc: 1.00 - ETA: 15:06 - loss: 0.2339 - acc: 1.00 - ETA: 13:56 - loss: 0.2343 - acc: 1.00 - ETA: 12:47 - loss: 0.2338 - acc: 1.00 - ETA: 11:37 - loss: 0.2333 - acc: 1.00 - ETA: 10:27 - loss: 0.2334 - acc: 1.00 - ETA: 9:18 - loss: 0.2328 - acc: 1.0000 - ETA: 8:08 - loss: 0.2332 - acc: 1.000 - ETA: 6:58 - loss: 0.2340 - acc: 1.000 - ETA: 5:49 - loss: 0.2338 - acc: 1.000 - ETA: 4:39 - loss: 0.2337 - acc: 1.000 - ETA: 3:29 - loss: 0.2334 - acc: 1.000 - ETA: 2:19 - loss: 0.2347 - acc: 1.000 - ETA: 1:09 - loss: 0.2342 - acc: 1.000 - 2509s 7s/step - loss: 0.2338 - acc: 1.0000 - val_loss: 0.8721 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "340/340 [==============================] - ETA: 38:18 - loss: 0.2290 - acc: 1.00 - ETA: 37:40 - loss: 0.2254 - acc: 1.00 - ETA: 37:30 - loss: 0.2232 - acc: 1.00 - ETA: 36:50 - loss: 0.2403 - acc: 1.00 - ETA: 35:41 - loss: 0.2363 - acc: 1.00 - ETA: 34:13 - loss: 0.2333 - acc: 1.00 - ETA: 32:46 - loss: 0.2345 - acc: 1.00 - ETA: 31:23 - loss: 0.2366 - acc: 1.00 - ETA: 30:02 - loss: 0.2356 - acc: 1.00 - ETA: 28:45 - loss: 0.2337 - acc: 1.00 - ETA: 27:28 - loss: 0.2326 - acc: 1.00 - ETA: 26:13 - loss: 0.2365 - acc: 1.00 - ETA: 24:59 - loss: 0.2363 - acc: 1.00 - ETA: 23:45 - loss: 0.2351 - acc: 1.00 - ETA: 22:32 - loss: 0.2345 - acc: 1.00 - ETA: 21:23 - loss: 0.2346 - acc: 1.00 - ETA: 20:10 - loss: 0.2340 - acc: 1.00 - ETA: 18:58 - loss: 0.2336 - acc: 1.00 - ETA: 17:45 - loss: 0.2355 - acc: 1.00 - ETA: 16:34 - loss: 0.2351 - acc: 1.00 - ETA: 15:22 - loss: 0.2345 - acc: 1.00 - ETA: 14:10 - loss: 0.2345 - acc: 1.00 - ETA: 12:59 - loss: 0.2342 - acc: 1.00 - ETA: 11:47 - loss: 0.2351 - acc: 1.00 - ETA: 10:36 - loss: 0.2360 - acc: 1.00 - ETA: 9:25 - loss: 0.2379 - acc: 1.0000 - ETA: 8:14 - loss: 0.2390 - acc: 1.000 - ETA: 7:03 - loss: 0.2381 - acc: 1.000 - ETA: 5:53 - loss: 0.2379 - acc: 1.000 - ETA: 4:42 - loss: 0.2384 - acc: 1.000 - ETA: 3:32 - loss: 0.2379 - acc: 1.000 - ETA: 2:21 - loss: 0.2417 - acc: 0.996 - ETA: 1:10 - loss: 0.2416 - acc: 0.997 - 2536s 7s/step - loss: 0.2436 - acc: 0.9971 - val_loss: 0.9412 - val_acc: 0.7833\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "340/340 [==============================] - ETA: 38:28 - loss: 0.4571 - acc: 0.90 - ETA: 37:33 - loss: 0.3372 - acc: 0.95 - ETA: 36:18 - loss: 0.2998 - acc: 0.96 - ETA: 35:06 - loss: 0.2860 - acc: 0.97 - ETA: 33:52 - loss: 0.2727 - acc: 0.98 - ETA: 32:59 - loss: 0.2645 - acc: 0.98 - ETA: 31:45 - loss: 0.2704 - acc: 0.98 - ETA: 30:31 - loss: 0.2642 - acc: 0.98 - ETA: 29:19 - loss: 0.2587 - acc: 0.98 - ETA: 28:08 - loss: 0.2546 - acc: 0.99 - ETA: 26:56 - loss: 0.2517 - acc: 0.99 - ETA: 25:45 - loss: 0.2488 - acc: 0.99 - ETA: 24:34 - loss: 0.2507 - acc: 0.99 - ETA: 23:23 - loss: 0.2491 - acc: 0.99 - ETA: 22:13 - loss: 0.2499 - acc: 0.99 - ETA: 21:02 - loss: 0.2490 - acc: 0.99 - ETA: 19:52 - loss: 0.2516 - acc: 0.98 - ETA: 18:41 - loss: 0.2500 - acc: 0.98 - ETA: 17:34 - loss: 0.2497 - acc: 0.98 - ETA: 16:24 - loss: 0.2489 - acc: 0.99 - ETA: 15:13 - loss: 0.2481 - acc: 0.99 - ETA: 14:03 - loss: 0.2482 - acc: 0.99 - ETA: 12:53 - loss: 0.2489 - acc: 0.99 - ETA: 11:43 - loss: 0.2492 - acc: 0.99 - ETA: 10:33 - loss: 0.2513 - acc: 0.99 - ETA: 9:22 - loss: 0.2556 - acc: 0.9923 - ETA: 8:12 - loss: 0.2554 - acc: 0.992 - ETA: 7:02 - loss: 0.2541 - acc: 0.992 - ETA: 5:51 - loss: 0.2530 - acc: 0.993 - ETA: 4:41 - loss: 0.2519 - acc: 0.993 - ETA: 3:32 - loss: 0.2511 - acc: 0.993 - ETA: 2:21 - loss: 0.2500 - acc: 0.993 - ETA: 1:11 - loss: 0.2497 - acc: 0.993 - 2558s 8s/step - loss: 0.2489 - acc: 0.9941 - val_loss: 0.6858 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "340/340 [==============================] - ETA: 39:19 - loss: 0.2904 - acc: 1.00 - ETA: 37:43 - loss: 0.2554 - acc: 1.00 - ETA: 36:24 - loss: 0.2418 - acc: 1.00 - ETA: 35:09 - loss: 0.2419 - acc: 1.00 - ETA: 33:54 - loss: 0.2396 - acc: 1.00 - ETA: 32:42 - loss: 0.2374 - acc: 1.00 - ETA: 31:31 - loss: 0.2353 - acc: 1.00 - ETA: 30:20 - loss: 0.2356 - acc: 1.00 - ETA: 29:09 - loss: 0.2339 - acc: 1.00 - ETA: 27:59 - loss: 0.2342 - acc: 1.00 - ETA: 26:50 - loss: 0.2367 - acc: 1.00 - ETA: 25:40 - loss: 0.2360 - acc: 1.00 - ETA: 24:30 - loss: 0.2348 - acc: 1.00 - ETA: 23:22 - loss: 0.2342 - acc: 1.00 - ETA: 22:11 - loss: 0.2330 - acc: 1.00 - ETA: 21:01 - loss: 0.2320 - acc: 1.00 - ETA: 19:50 - loss: 0.2331 - acc: 1.00 - ETA: 18:40 - loss: 0.2338 - acc: 1.00 - ETA: 17:30 - loss: 0.2359 - acc: 1.00 - ETA: 16:20 - loss: 0.2349 - acc: 1.00 - ETA: 15:10 - loss: 0.2341 - acc: 1.00 - ETA: 14:00 - loss: 0.2331 - acc: 1.00 - ETA: 12:50 - loss: 0.2327 - acc: 1.00 - ETA: 11:40 - loss: 0.2320 - acc: 1.00 - ETA: 10:30 - loss: 0.2316 - acc: 1.00 - ETA: 9:20 - loss: 0.2317 - acc: 1.0000 - ETA: 8:10 - loss: 0.2309 - acc: 1.000 - ETA: 7:00 - loss: 0.2302 - acc: 1.000 - ETA: 5:50 - loss: 0.2298 - acc: 1.000 - ETA: 4:40 - loss: 0.2291 - acc: 1.000 - ETA: 3:30 - loss: 0.2286 - acc: 1.000 - ETA: 2:20 - loss: 0.2281 - acc: 1.000 - ETA: 1:10 - loss: 0.2277 - acc: 1.000 - 2517s 7s/step - loss: 0.2271 - acc: 1.0000 - val_loss: 0.7677 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "340/340 [==============================] - ETA: 38:27 - loss: 0.2102 - acc: 1.00 - ETA: 37:11 - loss: 0.2161 - acc: 1.00 - ETA: 35:58 - loss: 0.2395 - acc: 1.00 - ETA: 35:10 - loss: 0.2402 - acc: 1.00 - ETA: 33:56 - loss: 0.2349 - acc: 1.00 - ETA: 32:45 - loss: 0.2336 - acc: 1.00 - ETA: 31:34 - loss: 0.2297 - acc: 1.00 - ETA: 30:23 - loss: 0.2283 - acc: 1.00 - ETA: 29:11 - loss: 0.2262 - acc: 1.00 - ETA: 28:01 - loss: 0.2255 - acc: 1.00 - ETA: 26:51 - loss: 0.2245 - acc: 1.00 - ETA: 25:40 - loss: 0.2234 - acc: 1.00 - ETA: 24:30 - loss: 0.2221 - acc: 1.00 - ETA: 23:19 - loss: 0.2211 - acc: 1.00 - ETA: 22:09 - loss: 0.2214 - acc: 1.00 - ETA: 20:59 - loss: 0.2205 - acc: 1.00 - ETA: 19:51 - loss: 0.2205 - acc: 1.00 - ETA: 18:41 - loss: 0.2199 - acc: 1.00 - ETA: 17:30 - loss: 0.2196 - acc: 1.00 - ETA: 16:20 - loss: 0.2193 - acc: 1.00 - ETA: 15:10 - loss: 0.2202 - acc: 1.00 - ETA: 13:59 - loss: 0.2199 - acc: 1.00 - ETA: 12:49 - loss: 0.2194 - acc: 1.00 - ETA: 11:40 - loss: 0.2189 - acc: 1.00 - ETA: 10:29 - loss: 0.2184 - acc: 1.00 - ETA: 9:19 - loss: 0.2185 - acc: 1.0000 - ETA: 8:09 - loss: 0.2184 - acc: 1.000 - ETA: 6:59 - loss: 0.2184 - acc: 1.000 - ETA: 5:49 - loss: 0.2180 - acc: 1.000 - ETA: 4:40 - loss: 0.2177 - acc: 1.000 - ETA: 3:30 - loss: 0.2174 - acc: 1.000 - ETA: 2:20 - loss: 0.2173 - acc: 1.000 - ETA: 1:09 - loss: 0.2169 - acc: 1.000 - 2516s 7s/step - loss: 0.2167 - acc: 1.0000 - val_loss: 0.6827 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 38:19 - loss: 0.2049 - acc: 1.00 - ETA: 37:09 - loss: 0.2066 - acc: 1.00 - ETA: 36:01 - loss: 0.2074 - acc: 1.00 - ETA: 34:50 - loss: 0.2065 - acc: 1.00 - ETA: 33:41 - loss: 0.2069 - acc: 1.00 - ETA: 32:31 - loss: 0.2085 - acc: 1.00 - ETA: 31:33 - loss: 0.2084 - acc: 1.00 - ETA: 30:22 - loss: 0.2078 - acc: 1.00 - ETA: 29:11 - loss: 0.2095 - acc: 1.00 - ETA: 28:00 - loss: 0.2095 - acc: 1.00 - ETA: 26:49 - loss: 0.2092 - acc: 1.00 - ETA: 25:38 - loss: 0.2092 - acc: 1.00 - ETA: 24:28 - loss: 0.2100 - acc: 1.00 - ETA: 23:17 - loss: 0.2099 - acc: 1.00 - ETA: 22:07 - loss: 0.2102 - acc: 1.00 - ETA: 20:57 - loss: 0.2119 - acc: 1.00 - ETA: 19:48 - loss: 0.2114 - acc: 1.00 - ETA: 18:38 - loss: 0.2112 - acc: 1.00 - ETA: 17:28 - loss: 0.2110 - acc: 1.00 - ETA: 16:20 - loss: 0.2107 - acc: 1.00 - ETA: 15:10 - loss: 0.2109 - acc: 1.00 - ETA: 13:59 - loss: 0.2109 - acc: 1.00 - ETA: 12:49 - loss: 0.2106 - acc: 1.00 - ETA: 11:39 - loss: 0.2109 - acc: 1.00 - ETA: 10:30 - loss: 0.2112 - acc: 1.00 - ETA: 9:19 - loss: 0.2112 - acc: 1.0000 - ETA: 8:09 - loss: 0.2113 - acc: 1.000 - ETA: 6:59 - loss: 0.2110 - acc: 1.000 - ETA: 5:49 - loss: 0.2109 - acc: 1.000 - ETA: 4:39 - loss: 0.2107 - acc: 1.000 - ETA: 3:29 - loss: 0.2105 - acc: 1.000 - ETA: 2:19 - loss: 0.2103 - acc: 1.000 - ETA: 1:10 - loss: 0.2101 - acc: 1.000 - 2516s 7s/step - loss: 0.2100 - acc: 1.0000 - val_loss: 0.6300 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.67519 to 0.63001, saving model to imagemodel4.1.h5\n",
      "Epoch 57/300\n",
      "340/340 [==============================] - ETA: 38:30 - loss: 0.2114 - acc: 1.00 - ETA: 37:18 - loss: 0.2088 - acc: 1.00 - ETA: 36:06 - loss: 0.2138 - acc: 1.00 - ETA: 34:59 - loss: 0.2115 - acc: 1.00 - ETA: 33:48 - loss: 0.2111 - acc: 1.00 - ETA: 32:37 - loss: 0.2194 - acc: 1.00 - ETA: 31:27 - loss: 0.2174 - acc: 1.00 - ETA: 30:18 - loss: 0.2159 - acc: 1.00 - ETA: 29:08 - loss: 0.2159 - acc: 1.00 - ETA: 28:06 - loss: 0.2154 - acc: 1.00 - ETA: 26:54 - loss: 0.2145 - acc: 1.00 - ETA: 25:43 - loss: 0.2136 - acc: 1.00 - ETA: 24:32 - loss: 0.2130 - acc: 1.00 - ETA: 23:22 - loss: 0.2129 - acc: 1.00 - ETA: 22:12 - loss: 0.2126 - acc: 1.00 - ETA: 21:02 - loss: 0.2121 - acc: 1.00 - ETA: 19:51 - loss: 0.2116 - acc: 1.00 - ETA: 18:41 - loss: 0.2114 - acc: 1.00 - ETA: 17:30 - loss: 0.2110 - acc: 1.00 - ETA: 16:20 - loss: 0.2106 - acc: 1.00 - ETA: 15:10 - loss: 0.2103 - acc: 1.00 - ETA: 14:00 - loss: 0.2103 - acc: 1.00 - ETA: 12:51 - loss: 0.2103 - acc: 1.00 - ETA: 11:41 - loss: 0.2100 - acc: 1.00 - ETA: 10:31 - loss: 0.2098 - acc: 1.00 - ETA: 9:20 - loss: 0.2095 - acc: 1.0000 - ETA: 8:10 - loss: 0.2096 - acc: 1.000 - ETA: 7:00 - loss: 0.2097 - acc: 1.000 - ETA: 5:50 - loss: 0.2094 - acc: 1.000 - ETA: 4:40 - loss: 0.2094 - acc: 1.000 - ETA: 3:30 - loss: 0.2092 - acc: 1.000 - ETA: 2:20 - loss: 0.2090 - acc: 1.000 - ETA: 1:10 - loss: 0.2090 - acc: 1.000 - 2518s 7s/step - loss: 0.2089 - acc: 1.0000 - val_loss: 0.6206 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.63001 to 0.62061, saving model to imagemodel4.1.h5\n",
      "Epoch 58/300\n",
      "340/340 [==============================] - ETA: 38:27 - loss: 0.2031 - acc: 1.00 - ETA: 37:18 - loss: 0.2053 - acc: 1.00 - ETA: 36:07 - loss: 0.2060 - acc: 1.00 - ETA: 34:58 - loss: 0.2076 - acc: 1.00 - ETA: 33:48 - loss: 0.2070 - acc: 1.00 - ETA: 32:36 - loss: 0.2067 - acc: 1.00 - ETA: 31:25 - loss: 0.2101 - acc: 1.00 - ETA: 30:16 - loss: 0.2090 - acc: 1.00 - ETA: 29:06 - loss: 0.2099 - acc: 1.00 - ETA: 27:56 - loss: 0.2095 - acc: 1.00 - ETA: 26:46 - loss: 0.2093 - acc: 1.00 - ETA: 25:36 - loss: 0.2088 - acc: 1.00 - ETA: 24:32 - loss: 0.2090 - acc: 1.00 - ETA: 23:22 - loss: 0.2096 - acc: 1.00 - ETA: 22:11 - loss: 0.2102 - acc: 1.00 - ETA: 21:01 - loss: 0.2096 - acc: 1.00 - ETA: 19:51 - loss: 0.2094 - acc: 1.00 - ETA: 18:40 - loss: 0.2095 - acc: 1.00 - ETA: 17:30 - loss: 0.2091 - acc: 1.00 - ETA: 16:20 - loss: 0.2087 - acc: 1.00 - ETA: 15:10 - loss: 0.2083 - acc: 1.00 - ETA: 14:00 - loss: 0.2079 - acc: 1.00 - ETA: 12:50 - loss: 0.2076 - acc: 1.00 - ETA: 11:39 - loss: 0.2074 - acc: 1.00 - ETA: 10:29 - loss: 0.2071 - acc: 1.00 - ETA: 9:21 - loss: 0.2070 - acc: 1.0000 - ETA: 8:10 - loss: 0.2067 - acc: 1.000 - ETA: 7:00 - loss: 0.2070 - acc: 1.000 - ETA: 5:50 - loss: 0.2071 - acc: 1.000 - ETA: 4:40 - loss: 0.2071 - acc: 1.000 - ETA: 3:30 - loss: 0.2069 - acc: 1.000 - ETA: 2:20 - loss: 0.2067 - acc: 1.000 - ETA: 1:10 - loss: 0.2067 - acc: 1.000 - 2519s 7s/step - loss: 0.2067 - acc: 1.0000 - val_loss: 0.6241 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "340/340 [==============================] - ETA: 38:31 - loss: 0.1994 - acc: 1.00 - ETA: 37:20 - loss: 0.2006 - acc: 1.00 - ETA: 36:46 - loss: 0.2016 - acc: 1.00 - ETA: 35:24 - loss: 0.2015 - acc: 1.00 - ETA: 34:09 - loss: 0.2011 - acc: 1.00 - ETA: 32:54 - loss: 0.2014 - acc: 1.00 - ETA: 31:42 - loss: 0.2060 - acc: 1.00 - ETA: 30:29 - loss: 0.2051 - acc: 1.00 - ETA: 29:17 - loss: 0.2050 - acc: 1.00 - ETA: 28:06 - loss: 0.2050 - acc: 1.00 - ETA: 26:55 - loss: 0.2045 - acc: 1.00 - ETA: 25:44 - loss: 0.2039 - acc: 1.00 - ETA: 24:33 - loss: 0.2035 - acc: 1.00 - ETA: 23:23 - loss: 0.2037 - acc: 1.00 - ETA: 22:12 - loss: 0.2036 - acc: 1.00 - ETA: 21:05 - loss: 0.2035 - acc: 1.00 - ETA: 19:54 - loss: 0.2032 - acc: 1.00 - ETA: 18:44 - loss: 0.2035 - acc: 1.00 - ETA: 17:35 - loss: 0.2033 - acc: 1.00 - ETA: 16:25 - loss: 0.2032 - acc: 1.00 - ETA: 15:14 - loss: 0.2030 - acc: 1.00 - ETA: 14:03 - loss: 0.2028 - acc: 1.00 - ETA: 12:53 - loss: 0.2025 - acc: 1.00 - ETA: 11:42 - loss: 0.2024 - acc: 1.00 - ETA: 10:32 - loss: 0.2028 - acc: 1.00 - ETA: 9:21 - loss: 0.2026 - acc: 1.0000 - ETA: 8:11 - loss: 0.2032 - acc: 1.000 - ETA: 7:01 - loss: 0.2030 - acc: 1.000 - ETA: 5:51 - loss: 0.2028 - acc: 1.000 - ETA: 4:41 - loss: 0.2029 - acc: 1.000 - ETA: 3:30 - loss: 0.2029 - acc: 1.000 - ETA: 2:20 - loss: 0.2031 - acc: 1.000 - ETA: 1:10 - loss: 0.2030 - acc: 1.000 - 2527s 7s/step - loss: 0.2029 - acc: 1.0000 - val_loss: 0.5945 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.62061 to 0.59445, saving model to imagemodel4.1.h5\n",
      "Epoch 60/300\n",
      "340/340 [==============================] - ETA: 38:27 - loss: 0.2006 - acc: 1.00 - ETA: 37:16 - loss: 0.2023 - acc: 1.00 - ETA: 36:01 - loss: 0.2016 - acc: 1.00 - ETA: 34:51 - loss: 0.2009 - acc: 1.00 - ETA: 33:43 - loss: 0.2005 - acc: 1.00 - ETA: 32:49 - loss: 0.2008 - acc: 1.00 - ETA: 31:36 - loss: 0.2007 - acc: 1.00 - ETA: 30:23 - loss: 0.2014 - acc: 1.00 - ETA: 29:12 - loss: 0.2008 - acc: 1.00 - ETA: 28:01 - loss: 0.2015 - acc: 1.00 - ETA: 26:50 - loss: 0.2014 - acc: 1.00 - ETA: 25:40 - loss: 0.2012 - acc: 1.00 - ETA: 24:30 - loss: 0.2011 - acc: 1.00 - ETA: 23:20 - loss: 0.2021 - acc: 1.00 - ETA: 22:10 - loss: 0.2020 - acc: 1.00 - ETA: 21:00 - loss: 0.2018 - acc: 1.00 - ETA: 19:50 - loss: 0.2015 - acc: 1.00 - ETA: 18:39 - loss: 0.2016 - acc: 1.00 - ETA: 17:32 - loss: 0.2013 - acc: 1.00 - ETA: 16:22 - loss: 0.2013 - acc: 1.00 - ETA: 15:11 - loss: 0.2011 - acc: 1.00 - ETA: 14:01 - loss: 0.2009 - acc: 1.00 - ETA: 12:51 - loss: 0.2009 - acc: 1.00 - ETA: 11:40 - loss: 0.2010 - acc: 1.00 - ETA: 10:30 - loss: 0.2009 - acc: 1.00 - ETA: 9:20 - loss: 0.2013 - acc: 1.0000 - ETA: 8:10 - loss: 0.2014 - acc: 1.000 - ETA: 7:00 - loss: 0.2012 - acc: 1.000 - ETA: 5:50 - loss: 0.2012 - acc: 1.000 - ETA: 4:40 - loss: 0.2011 - acc: 1.000 - ETA: 3:30 - loss: 0.2010 - acc: 1.000 - ETA: 2:20 - loss: 0.2008 - acc: 1.000 - ETA: 1:10 - loss: 0.2007 - acc: 1.000 - 2520s 7s/step - loss: 0.2007 - acc: 1.0000 - val_loss: 0.5878 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.59445 to 0.58781, saving model to imagemodel4.1.h5\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 38:17 - loss: 0.2015 - acc: 1.00 - ETA: 37:07 - loss: 0.1984 - acc: 1.00 - ETA: 35:59 - loss: 0.1986 - acc: 1.00 - ETA: 34:53 - loss: 0.1985 - acc: 1.00 - ETA: 33:46 - loss: 0.1979 - acc: 1.00 - ETA: 32:37 - loss: 0.1985 - acc: 1.00 - ETA: 31:27 - loss: 0.1983 - acc: 1.00 - ETA: 30:17 - loss: 0.1983 - acc: 1.00 - ETA: 29:15 - loss: 0.1980 - acc: 1.00 - ETA: 28:03 - loss: 0.1978 - acc: 1.00 - ETA: 26:53 - loss: 0.1977 - acc: 1.00 - ETA: 25:42 - loss: 0.1986 - acc: 1.00 - ETA: 24:32 - loss: 0.1990 - acc: 1.00 - ETA: 23:22 - loss: 0.1990 - acc: 1.00 - ETA: 22:11 - loss: 0.1990 - acc: 1.00 - ETA: 21:01 - loss: 0.1993 - acc: 1.00 - ETA: 19:50 - loss: 0.1991 - acc: 1.00 - ETA: 18:40 - loss: 0.1989 - acc: 1.00 - ETA: 17:30 - loss: 0.2001 - acc: 1.00 - ETA: 16:20 - loss: 0.2002 - acc: 1.00 - ETA: 15:09 - loss: 0.1999 - acc: 1.00 - ETA: 14:00 - loss: 0.1999 - acc: 1.00 - ETA: 12:50 - loss: 0.1997 - acc: 1.00 - ETA: 11:40 - loss: 0.1998 - acc: 1.00 - ETA: 10:30 - loss: 0.1996 - acc: 1.00 - ETA: 9:20 - loss: 0.1994 - acc: 1.0000 - ETA: 8:10 - loss: 0.1993 - acc: 1.000 - ETA: 7:00 - loss: 0.1993 - acc: 1.000 - ETA: 5:50 - loss: 0.1991 - acc: 1.000 - ETA: 4:40 - loss: 0.1990 - acc: 1.000 - ETA: 3:30 - loss: 0.1991 - acc: 1.000 - ETA: 2:19 - loss: 0.1989 - acc: 1.000 - ETA: 1:10 - loss: 0.1993 - acc: 1.000 - 2516s 7s/step - loss: 0.1991 - acc: 1.0000 - val_loss: 0.5729 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.58781 to 0.57288, saving model to imagemodel4.1.h5\n",
      "Epoch 62/300\n",
      "340/340 [==============================] - ETA: 38:29 - loss: 0.1963 - acc: 1.00 - ETA: 37:16 - loss: 0.1948 - acc: 1.00 - ETA: 36:06 - loss: 0.1957 - acc: 1.00 - ETA: 34:58 - loss: 0.1967 - acc: 1.00 - ETA: 33:46 - loss: 0.1966 - acc: 1.00 - ETA: 32:35 - loss: 0.1963 - acc: 1.00 - ETA: 31:25 - loss: 0.1960 - acc: 1.00 - ETA: 30:15 - loss: 0.1960 - acc: 1.00 - ETA: 29:05 - loss: 0.1957 - acc: 1.00 - ETA: 27:55 - loss: 0.1956 - acc: 1.00 - ETA: 26:46 - loss: 0.1958 - acc: 1.00 - ETA: 25:39 - loss: 0.1963 - acc: 1.00 - ETA: 24:30 - loss: 0.1967 - acc: 1.00 - ETA: 23:20 - loss: 0.1964 - acc: 1.00 - ETA: 22:10 - loss: 0.1965 - acc: 1.00 - ETA: 21:00 - loss: 0.1966 - acc: 1.00 - ETA: 19:49 - loss: 0.1988 - acc: 1.00 - ETA: 18:39 - loss: 0.1985 - acc: 1.00 - ETA: 17:29 - loss: 0.1989 - acc: 1.00 - ETA: 16:19 - loss: 0.1992 - acc: 1.00 - ETA: 15:09 - loss: 0.2007 - acc: 1.00 - ETA: 13:59 - loss: 0.2004 - acc: 1.00 - ETA: 12:49 - loss: 0.2003 - acc: 1.00 - ETA: 11:39 - loss: 0.2000 - acc: 1.00 - ETA: 10:29 - loss: 0.1998 - acc: 1.00 - ETA: 9:19 - loss: 0.1995 - acc: 1.0000 - ETA: 8:09 - loss: 0.1993 - acc: 1.000 - ETA: 6:59 - loss: 0.1993 - acc: 1.000 - ETA: 5:49 - loss: 0.1992 - acc: 1.000 - ETA: 4:39 - loss: 0.1990 - acc: 1.000 - ETA: 3:29 - loss: 0.1988 - acc: 1.000 - ETA: 2:19 - loss: 0.1991 - acc: 1.000 - ETA: 1:09 - loss: 0.1989 - acc: 1.000 - 2515s 7s/step - loss: 0.1987 - acc: 1.0000 - val_loss: 0.5592 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.57288 to 0.55918, saving model to imagemodel4.1.h5\n",
      "Epoch 63/300\n",
      "340/340 [==============================] - ETA: 38:16 - loss: 0.1927 - acc: 1.00 - ETA: 37:22 - loss: 0.1926 - acc: 1.00 - ETA: 36:09 - loss: 0.1964 - acc: 1.00 - ETA: 35:00 - loss: 0.1956 - acc: 1.00 - ETA: 33:48 - loss: 0.1960 - acc: 1.00 - ETA: 32:39 - loss: 0.1953 - acc: 1.00 - ETA: 31:28 - loss: 0.1948 - acc: 1.00 - ETA: 30:17 - loss: 0.1947 - acc: 1.00 - ETA: 29:07 - loss: 0.1944 - acc: 1.00 - ETA: 27:57 - loss: 0.1958 - acc: 1.00 - ETA: 26:47 - loss: 0.1957 - acc: 1.00 - ETA: 25:37 - loss: 0.1955 - acc: 1.00 - ETA: 24:26 - loss: 0.1958 - acc: 1.00 - ETA: 23:17 - loss: 0.1956 - acc: 1.00 - ETA: 22:07 - loss: 0.1954 - acc: 1.00 - ETA: 20:57 - loss: 0.1954 - acc: 1.00 - ETA: 19:47 - loss: 0.1952 - acc: 1.00 - ETA: 18:37 - loss: 0.1950 - acc: 1.00 - ETA: 17:27 - loss: 0.1948 - acc: 1.00 - ETA: 16:18 - loss: 0.1947 - acc: 1.00 - ETA: 15:08 - loss: 0.1958 - acc: 1.00 - ETA: 13:58 - loss: 0.1957 - acc: 1.00 - ETA: 12:48 - loss: 0.1956 - acc: 1.00 - ETA: 11:38 - loss: 0.1955 - acc: 1.00 - ETA: 10:28 - loss: 0.1953 - acc: 1.00 - ETA: 9:18 - loss: 0.1953 - acc: 1.0000 - ETA: 8:09 - loss: 0.1951 - acc: 1.000 - ETA: 6:59 - loss: 0.1950 - acc: 1.000 - ETA: 5:49 - loss: 0.1950 - acc: 1.000 - ETA: 4:39 - loss: 0.1949 - acc: 1.000 - ETA: 3:29 - loss: 0.1949 - acc: 1.000 - ETA: 2:19 - loss: 0.1948 - acc: 1.000 - ETA: 1:09 - loss: 0.1950 - acc: 1.000 - 2512s 7s/step - loss: 0.1950 - acc: 1.0000 - val_loss: 0.5838 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "340/340 [==============================] - ETA: 38:37 - loss: 0.1900 - acc: 1.00 - ETA: 37:19 - loss: 0.1902 - acc: 1.00 - ETA: 36:05 - loss: 0.1939 - acc: 1.00 - ETA: 35:03 - loss: 0.1978 - acc: 1.00 - ETA: 33:51 - loss: 0.2094 - acc: 1.00 - ETA: 32:38 - loss: 0.2072 - acc: 1.00 - ETA: 31:28 - loss: 0.2048 - acc: 1.00 - ETA: 30:18 - loss: 0.2030 - acc: 1.00 - ETA: 29:09 - loss: 0.2016 - acc: 1.00 - ETA: 27:58 - loss: 0.2005 - acc: 1.00 - ETA: 26:48 - loss: 0.1995 - acc: 1.00 - ETA: 25:38 - loss: 0.1996 - acc: 1.00 - ETA: 24:28 - loss: 0.1989 - acc: 1.00 - ETA: 23:18 - loss: 0.1987 - acc: 1.00 - ETA: 22:07 - loss: 0.1982 - acc: 1.00 - ETA: 20:57 - loss: 0.1977 - acc: 1.00 - ETA: 19:49 - loss: 0.1991 - acc: 1.00 - ETA: 18:39 - loss: 0.1987 - acc: 1.00 - ETA: 17:29 - loss: 0.1983 - acc: 1.00 - ETA: 16:19 - loss: 0.1980 - acc: 1.00 - ETA: 15:09 - loss: 0.1975 - acc: 1.00 - ETA: 13:59 - loss: 0.1984 - acc: 1.00 - ETA: 12:49 - loss: 0.1984 - acc: 1.00 - ETA: 11:39 - loss: 0.1983 - acc: 1.00 - ETA: 10:29 - loss: 0.1985 - acc: 1.00 - ETA: 9:19 - loss: 0.1984 - acc: 1.0000 - ETA: 8:09 - loss: 0.1983 - acc: 1.000 - ETA: 6:59 - loss: 0.1979 - acc: 1.000 - ETA: 5:49 - loss: 0.1982 - acc: 1.000 - ETA: 4:39 - loss: 0.1979 - acc: 1.000 - ETA: 3:29 - loss: 0.1977 - acc: 1.000 - ETA: 2:19 - loss: 0.1975 - acc: 1.000 - ETA: 1:09 - loss: 0.1972 - acc: 1.000 - 2515s 7s/step - loss: 0.1970 - acc: 1.0000 - val_loss: 0.5951 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "340/340 [==============================] - ETA: 38:15 - loss: 0.1993 - acc: 1.00 - ETA: 37:13 - loss: 0.1944 - acc: 1.00 - ETA: 36:07 - loss: 0.1982 - acc: 1.00 - ETA: 34:58 - loss: 0.1966 - acc: 1.00 - ETA: 33:46 - loss: 0.1998 - acc: 1.00 - ETA: 32:36 - loss: 0.2027 - acc: 1.00 - ETA: 31:38 - loss: 0.2010 - acc: 1.00 - ETA: 30:25 - loss: 0.2000 - acc: 1.00 - ETA: 29:13 - loss: 0.1990 - acc: 1.00 - ETA: 28:02 - loss: 0.1981 - acc: 1.00 - ETA: 26:51 - loss: 0.1973 - acc: 1.00 - ETA: 25:41 - loss: 0.1967 - acc: 1.00 - ETA: 24:31 - loss: 0.1962 - acc: 1.00 - ETA: 23:21 - loss: 0.1958 - acc: 1.00 - ETA: 22:10 - loss: 0.1953 - acc: 1.00 - ETA: 21:01 - loss: 0.1951 - acc: 1.00 - ETA: 19:51 - loss: 0.1950 - acc: 1.00 - ETA: 18:40 - loss: 0.1946 - acc: 1.00 - ETA: 17:30 - loss: 0.1942 - acc: 1.00 - ETA: 16:22 - loss: 0.1939 - acc: 1.00 - ETA: 15:11 - loss: 0.1937 - acc: 1.00 - ETA: 14:01 - loss: 0.1937 - acc: 1.00 - ETA: 12:51 - loss: 0.1937 - acc: 1.00 - ETA: 11:41 - loss: 0.1935 - acc: 1.00 - ETA: 10:30 - loss: 0.1933 - acc: 1.00 - ETA: 9:20 - loss: 0.1932 - acc: 1.0000 - ETA: 8:13 - loss: 0.1930 - acc: 1.000 - ETA: 7:03 - loss: 0.1931 - acc: 1.000 - ETA: 5:53 - loss: 0.1931 - acc: 1.000 - ETA: 4:43 - loss: 0.1950 - acc: 1.000 - ETA: 3:33 - loss: 0.1951 - acc: 1.000 - ETA: 2:24 - loss: 0.1949 - acc: 1.000 - ETA: 1:12 - loss: 0.1951 - acc: 1.000 - 2632s 8s/step - loss: 0.1949 - acc: 1.0000 - val_loss: 0.6060 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 45:07 - loss: 0.1883 - acc: 1.00 - ETA: 43:56 - loss: 0.1903 - acc: 1.00 - ETA: 42:59 - loss: 0.1897 - acc: 1.00 - ETA: 41:40 - loss: 0.1889 - acc: 1.00 - ETA: 39:30 - loss: 0.1890 - acc: 1.00 - ETA: 37:34 - loss: 0.1889 - acc: 1.00 - ETA: 36:01 - loss: 0.1887 - acc: 1.00 - ETA: 34:40 - loss: 0.1885 - acc: 1.00 - ETA: 33:15 - loss: 0.1884 - acc: 1.00 - ETA: 31:40 - loss: 0.1888 - acc: 1.00 - ETA: 30:37 - loss: 0.1888 - acc: 1.00 - ETA: 29:39 - loss: 0.1894 - acc: 1.00 - ETA: 28:30 - loss: 0.1895 - acc: 1.00 - ETA: 27:18 - loss: 0.1899 - acc: 1.00 - ETA: 26:03 - loss: 0.1898 - acc: 1.00 - ETA: 24:49 - loss: 0.1899 - acc: 1.00 - ETA: 23:31 - loss: 0.1898 - acc: 1.00 - ETA: 22:04 - loss: 0.1900 - acc: 1.00 - ETA: 20:37 - loss: 0.1900 - acc: 1.00 - ETA: 19:10 - loss: 0.1899 - acc: 1.00 - ETA: 17:47 - loss: 0.1898 - acc: 1.00 - ETA: 16:20 - loss: 0.1896 - acc: 1.00 - ETA: 14:53 - loss: 0.1897 - acc: 1.00 - ETA: 13:28 - loss: 0.1895 - acc: 1.00 - ETA: 12:04 - loss: 0.1894 - acc: 1.00 - ETA: 10:41 - loss: 0.1894 - acc: 1.00 - ETA: 9:19 - loss: 0.1895 - acc: 1.0000 - ETA: 7:57 - loss: 0.1897 - acc: 1.000 - ETA: 6:36 - loss: 0.1896 - acc: 1.000 - ETA: 5:16 - loss: 0.1899 - acc: 1.000 - ETA: 3:56 - loss: 0.1898 - acc: 1.000 - ETA: 2:37 - loss: 0.1897 - acc: 1.000 - ETA: 1:18 - loss: 0.1897 - acc: 1.000 - 2796s 8s/step - loss: 0.1897 - acc: 1.0000 - val_loss: 0.6025 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/300\n",
      "340/340 [==============================] - ETA: 39:24 - loss: 0.1947 - acc: 1.00 - ETA: 39:01 - loss: 0.1900 - acc: 1.00 - ETA: 37:33 - loss: 0.1885 - acc: 1.00 - ETA: 36:36 - loss: 0.1887 - acc: 1.00 - ETA: 35:13 - loss: 0.1886 - acc: 1.00 - ETA: 33:51 - loss: 0.1892 - acc: 1.00 - ETA: 32:33 - loss: 0.1892 - acc: 1.00 - ETA: 31:16 - loss: 0.1893 - acc: 1.00 - ETA: 30:01 - loss: 0.1889 - acc: 1.00 - ETA: 28:47 - loss: 0.1890 - acc: 1.00 - ETA: 27:33 - loss: 0.1887 - acc: 1.00 - ETA: 26:24 - loss: 0.1887 - acc: 1.00 - ETA: 25:13 - loss: 0.1885 - acc: 1.00 - ETA: 24:02 - loss: 0.1884 - acc: 1.00 - ETA: 22:50 - loss: 0.1884 - acc: 1.00 - ETA: 21:39 - loss: 0.1882 - acc: 1.00 - ETA: 20:27 - loss: 0.1881 - acc: 1.00 - ETA: 19:15 - loss: 0.1881 - acc: 1.00 - ETA: 18:03 - loss: 0.1880 - acc: 1.00 - ETA: 16:51 - loss: 0.1878 - acc: 1.00 - ETA: 15:39 - loss: 0.1877 - acc: 1.00 - ETA: 14:27 - loss: 0.1876 - acc: 1.00 - ETA: 13:15 - loss: 0.1875 - acc: 1.00 - ETA: 12:03 - loss: 0.1875 - acc: 1.00 - ETA: 10:51 - loss: 0.1873 - acc: 1.00 - ETA: 9:38 - loss: 0.1874 - acc: 1.0000 - ETA: 8:26 - loss: 0.1872 - acc: 1.000 - ETA: 7:14 - loss: 0.1872 - acc: 1.000 - ETA: 6:02 - loss: 0.1871 - acc: 1.000 - ETA: 4:49 - loss: 0.1873 - acc: 1.000 - ETA: 3:37 - loss: 0.1872 - acc: 1.000 - ETA: 2:24 - loss: 0.1872 - acc: 1.000 - ETA: 1:12 - loss: 0.1872 - acc: 1.000 - 2605s 8s/step - loss: 0.1873 - acc: 1.0000 - val_loss: 0.5902 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/300\n",
      "340/340 [==============================] - ETA: 39:37 - loss: 0.1902 - acc: 1.00 - ETA: 38:37 - loss: 0.1918 - acc: 1.00 - ETA: 37:52 - loss: 0.1892 - acc: 1.00 - ETA: 37:52 - loss: 0.1880 - acc: 1.00 - ETA: 37:56 - loss: 0.1888 - acc: 1.00 - ETA: 37:19 - loss: 0.1881 - acc: 1.00 - ETA: 36:30 - loss: 0.1876 - acc: 1.00 - ETA: 35:26 - loss: 0.1870 - acc: 1.00 - ETA: 34:16 - loss: 0.1870 - acc: 1.00 - ETA: 33:04 - loss: 0.1870 - acc: 1.00 - ETA: 31:53 - loss: 0.1867 - acc: 1.00 - ETA: 30:33 - loss: 0.1865 - acc: 1.00 - ETA: 28:55 - loss: 0.1863 - acc: 1.00 - ETA: 27:18 - loss: 0.1864 - acc: 1.00 - ETA: 25:44 - loss: 0.1862 - acc: 1.00 - ETA: 24:22 - loss: 0.1862 - acc: 1.00 - ETA: 23:09 - loss: 0.1860 - acc: 1.00 - ETA: 21:44 - loss: 0.1860 - acc: 1.00 - ETA: 20:22 - loss: 0.1860 - acc: 1.00 - ETA: 19:01 - loss: 0.1859 - acc: 1.00 - ETA: 17:39 - loss: 0.1859 - acc: 1.00 - ETA: 16:17 - loss: 0.1858 - acc: 1.00 - ETA: 15:00 - loss: 0.1857 - acc: 1.00 - ETA: 13:43 - loss: 0.1858 - acc: 1.00 - ETA: 12:23 - loss: 0.1858 - acc: 1.00 - ETA: 11:01 - loss: 0.1858 - acc: 1.00 - ETA: 9:41 - loss: 0.1859 - acc: 1.0000 - ETA: 8:20 - loss: 0.1883 - acc: 1.000 - ETA: 6:59 - loss: 0.1886 - acc: 1.000 - ETA: 5:36 - loss: 0.1889 - acc: 1.000 - ETA: 4:13 - loss: 0.1887 - acc: 1.000 - ETA: 2:49 - loss: 0.1885 - acc: 1.000 - ETA: 1:24 - loss: 0.1884 - acc: 1.000 - 3050s 9s/step - loss: 0.1883 - acc: 1.0000 - val_loss: 0.6191 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/300\n",
      "340/340 [==============================] - ETA: 45:33 - loss: 0.2003 - acc: 1.00 - ETA: 44:15 - loss: 0.2466 - acc: 1.00 - ETA: 43:52 - loss: 0.2277 - acc: 1.00 - ETA: 42:48 - loss: 0.2165 - acc: 1.00 - ETA: 41:48 - loss: 0.2100 - acc: 1.00 - ETA: 40:38 - loss: 0.2054 - acc: 1.00 - ETA: 39:21 - loss: 0.2045 - acc: 1.00 - ETA: 37:55 - loss: 0.2018 - acc: 1.00 - ETA: 36:35 - loss: 0.1998 - acc: 1.00 - ETA: 35:09 - loss: 0.1985 - acc: 1.00 - ETA: 33:27 - loss: 0.1972 - acc: 1.00 - ETA: 31:46 - loss: 0.1960 - acc: 1.00 - ETA: 29:58 - loss: 0.1953 - acc: 1.00 - ETA: 28:18 - loss: 0.1945 - acc: 1.00 - ETA: 26:50 - loss: 0.1940 - acc: 1.00 - ETA: 25:24 - loss: 0.1944 - acc: 1.00 - ETA: 23:59 - loss: 0.1939 - acc: 1.00 - ETA: 22:25 - loss: 0.1935 - acc: 1.00 - ETA: 21:01 - loss: 0.1932 - acc: 1.00 - ETA: 19:36 - loss: 0.1929 - acc: 1.00 - ETA: 18:11 - loss: 0.1924 - acc: 1.00 - ETA: 16:46 - loss: 0.1926 - acc: 1.00 - ETA: 15:22 - loss: 0.1922 - acc: 1.00 - ETA: 13:57 - loss: 0.1919 - acc: 1.00 - ETA: 12:29 - loss: 0.1917 - acc: 1.00 - ETA: 11:03 - loss: 0.1916 - acc: 1.00 - ETA: 9:37 - loss: 0.1912 - acc: 1.0000 - ETA: 8:12 - loss: 0.1909 - acc: 1.000 - ETA: 6:50 - loss: 0.1906 - acc: 1.000 - ETA: 5:28 - loss: 0.1907 - acc: 1.000 - ETA: 4:06 - loss: 0.1906 - acc: 1.000 - ETA: 2:44 - loss: 0.1913 - acc: 1.000 - ETA: 1:22 - loss: 0.1910 - acc: 1.000 - 2947s 9s/step - loss: 0.1908 - acc: 1.0000 - val_loss: 0.6681 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/300\n",
      "340/340 [==============================] - ETA: 40:02 - loss: 0.1812 - acc: 1.00 - ETA: 38:40 - loss: 0.1817 - acc: 1.00 - ETA: 37:23 - loss: 0.1865 - acc: 1.00 - ETA: 36:10 - loss: 0.1853 - acc: 1.00 - ETA: 34:55 - loss: 0.1856 - acc: 1.00 - ETA: 33:44 - loss: 0.1852 - acc: 1.00 - ETA: 32:30 - loss: 0.1847 - acc: 1.00 - ETA: 31:17 - loss: 0.1851 - acc: 1.00 - ETA: 30:04 - loss: 0.1871 - acc: 1.00 - ETA: 28:51 - loss: 0.1867 - acc: 1.00 - ETA: 27:39 - loss: 0.1865 - acc: 1.00 - ETA: 26:26 - loss: 0.1867 - acc: 1.00 - ETA: 25:14 - loss: 0.1872 - acc: 1.00 - ETA: 24:02 - loss: 0.1869 - acc: 1.00 - ETA: 22:50 - loss: 0.1875 - acc: 1.00 - ETA: 21:42 - loss: 0.1875 - acc: 1.00 - ETA: 20:29 - loss: 0.1871 - acc: 1.00 - ETA: 19:17 - loss: 0.1868 - acc: 1.00 - ETA: 18:04 - loss: 0.1865 - acc: 1.00 - ETA: 16:52 - loss: 0.1865 - acc: 1.00 - ETA: 15:39 - loss: 0.1868 - acc: 1.00 - ETA: 14:26 - loss: 0.1867 - acc: 1.00 - ETA: 13:14 - loss: 0.1866 - acc: 1.00 - ETA: 12:02 - loss: 0.1875 - acc: 1.00 - ETA: 10:50 - loss: 0.1872 - acc: 1.00 - ETA: 9:37 - loss: 0.1870 - acc: 1.0000 - ETA: 8:25 - loss: 0.1869 - acc: 1.000 - ETA: 7:13 - loss: 0.1869 - acc: 1.000 - ETA: 6:01 - loss: 0.1869 - acc: 1.000 - ETA: 4:49 - loss: 0.1871 - acc: 1.000 - ETA: 3:36 - loss: 0.1869 - acc: 1.000 - ETA: 2:24 - loss: 0.1868 - acc: 1.000 - ETA: 1:12 - loss: 0.1867 - acc: 1.000 - 2595s 8s/step - loss: 0.1865 - acc: 1.0000 - val_loss: 0.6330 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:46 - loss: 0.1813 - acc: 1.00 - ETA: 38:24 - loss: 0.1855 - acc: 1.00 - ETA: 37:06 - loss: 0.1839 - acc: 1.00 - ETA: 35:52 - loss: 0.1843 - acc: 1.00 - ETA: 34:55 - loss: 0.1835 - acc: 1.00 - ETA: 33:39 - loss: 0.1833 - acc: 1.00 - ETA: 32:25 - loss: 0.1835 - acc: 1.00 - ETA: 31:12 - loss: 0.1832 - acc: 1.00 - ETA: 29:59 - loss: 0.1835 - acc: 1.00 - ETA: 28:45 - loss: 0.1850 - acc: 1.00 - ETA: 27:32 - loss: 0.1846 - acc: 1.00 - ETA: 26:20 - loss: 0.1844 - acc: 1.00 - ETA: 25:07 - loss: 0.1843 - acc: 1.00 - ETA: 23:55 - loss: 0.1839 - acc: 1.00 - ETA: 22:43 - loss: 0.1837 - acc: 1.00 - ETA: 21:31 - loss: 0.1834 - acc: 1.00 - ETA: 20:19 - loss: 0.1854 - acc: 1.00 - ETA: 19:12 - loss: 0.1852 - acc: 1.00 - ETA: 17:59 - loss: 0.1851 - acc: 1.00 - ETA: 16:47 - loss: 0.1849 - acc: 1.00 - ETA: 15:35 - loss: 0.1847 - acc: 1.00 - ETA: 14:22 - loss: 0.1846 - acc: 1.00 - ETA: 13:10 - loss: 0.1847 - acc: 1.00 - ETA: 11:58 - loss: 0.1844 - acc: 1.00 - ETA: 10:46 - loss: 0.1842 - acc: 1.00 - ETA: 9:34 - loss: 0.1841 - acc: 1.0000 - ETA: 8:22 - loss: 0.1840 - acc: 1.000 - ETA: 7:10 - loss: 0.1839 - acc: 1.000 - ETA: 5:59 - loss: 0.1839 - acc: 1.000 - ETA: 4:47 - loss: 0.1838 - acc: 1.000 - ETA: 3:35 - loss: 0.1837 - acc: 1.000 - ETA: 2:23 - loss: 0.1837 - acc: 1.000 - ETA: 1:11 - loss: 0.1836 - acc: 1.000 - 2581s 8s/step - loss: 0.1834 - acc: 1.0000 - val_loss: 0.6113 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/300\n",
      "340/340 [==============================] - ETA: 39:22 - loss: 0.1802 - acc: 1.00 - ETA: 38:16 - loss: 0.1802 - acc: 1.00 - ETA: 37:03 - loss: 0.1851 - acc: 1.00 - ETA: 35:52 - loss: 0.1837 - acc: 1.00 - ETA: 34:39 - loss: 0.1830 - acc: 1.00 - ETA: 33:25 - loss: 0.1825 - acc: 1.00 - ETA: 32:24 - loss: 0.1819 - acc: 1.00 - ETA: 31:11 - loss: 0.1817 - acc: 1.00 - ETA: 29:57 - loss: 0.1814 - acc: 1.00 - ETA: 28:44 - loss: 0.1811 - acc: 1.00 - ETA: 27:31 - loss: 0.1810 - acc: 1.00 - ETA: 26:19 - loss: 0.1827 - acc: 1.00 - ETA: 25:07 - loss: 0.1825 - acc: 1.00 - ETA: 23:55 - loss: 0.1823 - acc: 1.00 - ETA: 22:43 - loss: 0.1820 - acc: 1.00 - ETA: 21:31 - loss: 0.1817 - acc: 1.00 - ETA: 20:19 - loss: 0.1815 - acc: 1.00 - ETA: 19:07 - loss: 0.1817 - acc: 1.00 - ETA: 17:55 - loss: 0.1815 - acc: 1.00 - ETA: 16:46 - loss: 0.1813 - acc: 1.00 - ETA: 15:34 - loss: 0.1812 - acc: 1.00 - ETA: 14:23 - loss: 0.1810 - acc: 1.00 - ETA: 13:11 - loss: 0.1809 - acc: 1.00 - ETA: 11:59 - loss: 0.1807 - acc: 1.00 - ETA: 10:47 - loss: 0.1806 - acc: 1.00 - ETA: 9:35 - loss: 0.1817 - acc: 1.0000 - ETA: 8:23 - loss: 0.1818 - acc: 1.000 - ETA: 7:11 - loss: 0.1817 - acc: 1.000 - ETA: 5:59 - loss: 0.1816 - acc: 1.000 - ETA: 4:47 - loss: 0.1817 - acc: 1.000 - ETA: 3:35 - loss: 0.1817 - acc: 1.000 - ETA: 2:23 - loss: 0.1817 - acc: 1.000 - ETA: 1:11 - loss: 0.1816 - acc: 1.000 - 2581s 8s/step - loss: 0.1816 - acc: 1.0000 - val_loss: 0.6002 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/300\n",
      "340/340 [==============================] - ETA: 39:20 - loss: 0.1817 - acc: 1.00 - ETA: 38:07 - loss: 0.1807 - acc: 1.00 - ETA: 36:53 - loss: 0.1794 - acc: 1.00 - ETA: 35:45 - loss: 0.1818 - acc: 1.00 - ETA: 34:32 - loss: 0.1815 - acc: 1.00 - ETA: 33:21 - loss: 0.1808 - acc: 1.00 - ETA: 32:10 - loss: 0.1804 - acc: 1.00 - ETA: 30:59 - loss: 0.1798 - acc: 1.00 - ETA: 29:56 - loss: 0.1796 - acc: 1.00 - ETA: 28:44 - loss: 0.1802 - acc: 1.00 - ETA: 27:31 - loss: 0.1799 - acc: 1.00 - ETA: 26:20 - loss: 0.1806 - acc: 1.00 - ETA: 25:07 - loss: 0.1803 - acc: 1.00 - ETA: 23:55 - loss: 0.1800 - acc: 1.00 - ETA: 22:42 - loss: 0.1800 - acc: 1.00 - ETA: 21:31 - loss: 0.1799 - acc: 1.00 - ETA: 20:19 - loss: 0.1797 - acc: 1.00 - ETA: 19:07 - loss: 0.1795 - acc: 1.00 - ETA: 17:55 - loss: 0.1795 - acc: 1.00 - ETA: 16:44 - loss: 0.1795 - acc: 1.00 - ETA: 15:32 - loss: 0.1794 - acc: 1.00 - ETA: 14:21 - loss: 0.1794 - acc: 1.00 - ETA: 13:09 - loss: 0.1795 - acc: 1.00 - ETA: 11:58 - loss: 0.1794 - acc: 1.00 - ETA: 10:46 - loss: 0.1793 - acc: 1.00 - ETA: 9:34 - loss: 0.1792 - acc: 1.0000 - ETA: 8:22 - loss: 0.1791 - acc: 1.000 - ETA: 7:10 - loss: 0.1790 - acc: 1.000 - ETA: 5:59 - loss: 0.1789 - acc: 1.000 - ETA: 4:47 - loss: 0.1788 - acc: 1.000 - ETA: 3:35 - loss: 0.1788 - acc: 1.000 - ETA: 2:23 - loss: 0.1787 - acc: 1.000 - ETA: 1:11 - loss: 0.1786 - acc: 1.000 - 2580s 8s/step - loss: 0.1786 - acc: 1.0000 - val_loss: 0.5901 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/300\n",
      "340/340 [==============================] - ETA: 39:30 - loss: 0.1789 - acc: 1.00 - ETA: 38:14 - loss: 0.1778 - acc: 1.00 - ETA: 37:03 - loss: 0.1773 - acc: 1.00 - ETA: 35:52 - loss: 0.1773 - acc: 1.00 - ETA: 34:41 - loss: 0.1773 - acc: 1.00 - ETA: 33:29 - loss: 0.1770 - acc: 1.00 - ETA: 32:17 - loss: 0.1778 - acc: 1.00 - ETA: 31:05 - loss: 0.1780 - acc: 1.00 - ETA: 29:53 - loss: 0.1777 - acc: 1.00 - ETA: 28:41 - loss: 0.1774 - acc: 1.00 - ETA: 27:37 - loss: 0.1773 - acc: 1.00 - ETA: 26:24 - loss: 0.1776 - acc: 1.00 - ETA: 25:11 - loss: 0.1776 - acc: 1.00 - ETA: 24:00 - loss: 0.1789 - acc: 1.00 - ETA: 22:47 - loss: 0.1788 - acc: 1.00 - ETA: 21:35 - loss: 0.1786 - acc: 1.00 - ETA: 20:22 - loss: 0.1784 - acc: 1.00 - ETA: 19:10 - loss: 0.1783 - acc: 1.00 - ETA: 17:58 - loss: 0.1781 - acc: 1.00 - ETA: 16:46 - loss: 0.1780 - acc: 1.00 - ETA: 15:34 - loss: 0.1779 - acc: 1.00 - ETA: 14:22 - loss: 0.1779 - acc: 1.00 - ETA: 13:10 - loss: 0.1780 - acc: 1.00 - ETA: 11:59 - loss: 0.1779 - acc: 1.00 - ETA: 10:47 - loss: 0.1778 - acc: 1.00 - ETA: 9:35 - loss: 0.1779 - acc: 1.0000 - ETA: 8:23 - loss: 0.1778 - acc: 1.000 - ETA: 7:11 - loss: 0.1777 - acc: 1.000 - ETA: 5:59 - loss: 0.1777 - acc: 1.000 - ETA: 4:47 - loss: 0.1777 - acc: 1.000 - ETA: 3:35 - loss: 0.1776 - acc: 1.000 - ETA: 2:23 - loss: 0.1776 - acc: 1.000 - ETA: 1:11 - loss: 0.1779 - acc: 1.000 - 2582s 8s/step - loss: 0.1780 - acc: 1.0000 - val_loss: 0.5760 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/300\n",
      "340/340 [==============================] - ETA: 39:23 - loss: 0.1773 - acc: 1.00 - ETA: 38:09 - loss: 0.1760 - acc: 1.00 - ETA: 36:57 - loss: 0.1812 - acc: 1.00 - ETA: 35:47 - loss: 0.1797 - acc: 1.00 - ETA: 34:37 - loss: 0.1791 - acc: 1.00 - ETA: 33:28 - loss: 0.1906 - acc: 1.00 - ETA: 32:16 - loss: 0.1895 - acc: 1.00 - ETA: 31:04 - loss: 0.1877 - acc: 1.00 - ETA: 29:52 - loss: 0.1864 - acc: 1.00 - ETA: 28:40 - loss: 0.1853 - acc: 1.00 - ETA: 27:28 - loss: 0.1848 - acc: 1.00 - ETA: 26:17 - loss: 0.1839 - acc: 1.00 - ETA: 25:12 - loss: 0.1838 - acc: 1.00 - ETA: 23:59 - loss: 0.1848 - acc: 1.00 - ETA: 22:47 - loss: 0.1843 - acc: 1.00 - ETA: 21:34 - loss: 0.1837 - acc: 1.00 - ETA: 20:22 - loss: 0.1832 - acc: 1.00 - ETA: 19:09 - loss: 0.1828 - acc: 1.00 - ETA: 17:58 - loss: 0.1827 - acc: 1.00 - ETA: 16:45 - loss: 0.1823 - acc: 1.00 - ETA: 15:33 - loss: 0.1820 - acc: 1.00 - ETA: 14:22 - loss: 0.1817 - acc: 1.00 - ETA: 13:10 - loss: 0.1814 - acc: 1.00 - ETA: 11:58 - loss: 0.1815 - acc: 1.00 - ETA: 10:46 - loss: 0.1812 - acc: 1.00 - ETA: 9:34 - loss: 0.1810 - acc: 1.0000 - ETA: 8:22 - loss: 0.1812 - acc: 1.000 - ETA: 7:10 - loss: 0.1810 - acc: 1.000 - ETA: 5:58 - loss: 0.1828 - acc: 1.000 - ETA: 4:47 - loss: 0.1828 - acc: 1.000 - ETA: 3:35 - loss: 0.1825 - acc: 1.000 - ETA: 2:23 - loss: 0.1822 - acc: 1.000 - ETA: 1:11 - loss: 0.1822 - acc: 1.000 - 2580s 8s/step - loss: 0.1819 - acc: 1.0000 - val_loss: 0.5910 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:34 - loss: 0.1743 - acc: 1.00 - ETA: 38:58 - loss: 0.1748 - acc: 1.00 - ETA: 37:30 - loss: 0.1744 - acc: 1.00 - ETA: 36:13 - loss: 0.1748 - acc: 1.00 - ETA: 34:57 - loss: 0.1752 - acc: 1.00 - ETA: 33:42 - loss: 0.1764 - acc: 1.00 - ETA: 32:28 - loss: 0.1767 - acc: 1.00 - ETA: 31:15 - loss: 0.1767 - acc: 1.00 - ETA: 30:01 - loss: 0.1772 - acc: 1.00 - ETA: 28:49 - loss: 0.1770 - acc: 1.00 - ETA: 27:35 - loss: 0.1768 - acc: 1.00 - ETA: 26:23 - loss: 0.1767 - acc: 1.00 - ETA: 25:11 - loss: 0.1766 - acc: 1.00 - ETA: 23:59 - loss: 0.1764 - acc: 1.00 - ETA: 22:50 - loss: 0.1772 - acc: 1.00 - ETA: 21:38 - loss: 0.1771 - acc: 1.00 - ETA: 20:26 - loss: 0.1780 - acc: 1.00 - ETA: 19:13 - loss: 0.1778 - acc: 1.00 - ETA: 18:01 - loss: 0.1784 - acc: 1.00 - ETA: 16:49 - loss: 0.1782 - acc: 1.00 - ETA: 15:36 - loss: 0.1782 - acc: 1.00 - ETA: 14:24 - loss: 0.1780 - acc: 1.00 - ETA: 13:12 - loss: 0.1780 - acc: 1.00 - ETA: 11:59 - loss: 0.1779 - acc: 1.00 - ETA: 10:47 - loss: 0.1779 - acc: 1.00 - ETA: 9:35 - loss: 0.1778 - acc: 1.0000 - ETA: 8:24 - loss: 0.1780 - acc: 1.000 - ETA: 7:12 - loss: 0.1778 - acc: 1.000 - ETA: 5:59 - loss: 0.1777 - acc: 1.000 - ETA: 4:47 - loss: 0.1776 - acc: 1.000 - ETA: 3:35 - loss: 0.1775 - acc: 1.000 - ETA: 2:23 - loss: 0.1774 - acc: 1.000 - ETA: 1:11 - loss: 0.1774 - acc: 1.000 - 2586s 8s/step - loss: 0.1775 - acc: 1.0000 - val_loss: 0.5814 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/300\n",
      "340/340 [==============================] - ETA: 39:32 - loss: 0.1745 - acc: 1.00 - ETA: 38:23 - loss: 0.1749 - acc: 1.00 - ETA: 37:16 - loss: 0.1751 - acc: 1.00 - ETA: 36:25 - loss: 0.1746 - acc: 1.00 - ETA: 35:07 - loss: 0.1751 - acc: 1.00 - ETA: 33:49 - loss: 0.1769 - acc: 1.00 - ETA: 32:34 - loss: 0.1766 - acc: 1.00 - ETA: 31:20 - loss: 0.1764 - acc: 1.00 - ETA: 30:06 - loss: 0.1762 - acc: 1.00 - ETA: 28:52 - loss: 0.1764 - acc: 1.00 - ETA: 27:39 - loss: 0.1762 - acc: 1.00 - ETA: 26:27 - loss: 0.1760 - acc: 1.00 - ETA: 25:14 - loss: 0.1759 - acc: 1.00 - ETA: 24:02 - loss: 0.1760 - acc: 1.00 - ETA: 22:50 - loss: 0.1758 - acc: 1.00 - ETA: 21:37 - loss: 0.1759 - acc: 1.00 - ETA: 20:27 - loss: 0.1758 - acc: 1.00 - ETA: 19:15 - loss: 0.1757 - acc: 1.00 - ETA: 18:02 - loss: 0.1756 - acc: 1.00 - ETA: 16:49 - loss: 0.1758 - acc: 1.00 - ETA: 15:37 - loss: 0.1759 - acc: 1.00 - ETA: 14:25 - loss: 0.1758 - acc: 1.00 - ETA: 13:12 - loss: 0.1758 - acc: 1.00 - ETA: 12:00 - loss: 0.1757 - acc: 1.00 - ETA: 10:48 - loss: 0.1757 - acc: 1.00 - ETA: 9:36 - loss: 0.1756 - acc: 1.0000 - ETA: 8:24 - loss: 0.1756 - acc: 1.000 - ETA: 7:12 - loss: 0.1756 - acc: 1.000 - ETA: 6:00 - loss: 0.1755 - acc: 1.000 - ETA: 4:48 - loss: 0.1754 - acc: 1.000 - ETA: 3:36 - loss: 0.1754 - acc: 1.000 - ETA: 2:24 - loss: 0.1753 - acc: 1.000 - ETA: 1:12 - loss: 0.1754 - acc: 1.000 - 2591s 8s/step - loss: 0.1756 - acc: 1.0000 - val_loss: 0.5868 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/300\n",
      "340/340 [==============================] - ETA: 39:39 - loss: 0.1750 - acc: 1.00 - ETA: 38:18 - loss: 0.1747 - acc: 1.00 - ETA: 37:04 - loss: 0.1744 - acc: 1.00 - ETA: 35:50 - loss: 0.1743 - acc: 1.00 - ETA: 34:41 - loss: 0.1740 - acc: 1.00 - ETA: 33:48 - loss: 0.1738 - acc: 1.00 - ETA: 32:32 - loss: 0.1741 - acc: 1.00 - ETA: 31:17 - loss: 0.1752 - acc: 1.00 - ETA: 30:05 - loss: 0.1751 - acc: 1.00 - ETA: 28:51 - loss: 0.1751 - acc: 1.00 - ETA: 27:38 - loss: 0.1750 - acc: 1.00 - ETA: 26:26 - loss: 0.1750 - acc: 1.00 - ETA: 25:12 - loss: 0.1749 - acc: 1.00 - ETA: 24:00 - loss: 0.1750 - acc: 1.00 - ETA: 22:48 - loss: 0.1749 - acc: 1.00 - ETA: 21:35 - loss: 0.1749 - acc: 1.00 - ETA: 20:23 - loss: 0.1747 - acc: 1.00 - ETA: 19:11 - loss: 0.1749 - acc: 1.00 - ETA: 17:59 - loss: 0.1748 - acc: 1.00 - ETA: 16:47 - loss: 0.1748 - acc: 1.00 - ETA: 15:35 - loss: 0.1747 - acc: 1.00 - ETA: 14:23 - loss: 0.1751 - acc: 1.00 - ETA: 13:11 - loss: 0.1751 - acc: 1.00 - ETA: 11:59 - loss: 0.1751 - acc: 1.00 - ETA: 10:47 - loss: 0.1752 - acc: 1.00 - ETA: 9:35 - loss: 0.1751 - acc: 1.0000 - ETA: 8:23 - loss: 0.1751 - acc: 1.000 - ETA: 7:11 - loss: 0.1750 - acc: 1.000 - ETA: 5:59 - loss: 0.1749 - acc: 1.000 - ETA: 4:47 - loss: 0.1749 - acc: 1.000 - ETA: 3:36 - loss: 0.1748 - acc: 1.000 - ETA: 2:24 - loss: 0.1748 - acc: 1.000 - ETA: 1:12 - loss: 0.1747 - acc: 1.000 - 2588s 8s/step - loss: 0.1747 - acc: 1.0000 - val_loss: 0.5769 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/300\n",
      "340/340 [==============================] - ETA: 39:49 - loss: 0.1723 - acc: 1.00 - ETA: 38:29 - loss: 0.1766 - acc: 1.00 - ETA: 37:15 - loss: 0.1756 - acc: 1.00 - ETA: 36:05 - loss: 0.1754 - acc: 1.00 - ETA: 34:53 - loss: 0.1751 - acc: 1.00 - ETA: 33:39 - loss: 0.1757 - acc: 1.00 - ETA: 32:26 - loss: 0.1755 - acc: 1.00 - ETA: 31:21 - loss: 0.1754 - acc: 1.00 - ETA: 30:08 - loss: 0.1752 - acc: 1.00 - ETA: 28:56 - loss: 0.1750 - acc: 1.00 - ETA: 27:43 - loss: 0.1759 - acc: 1.00 - ETA: 26:31 - loss: 0.1756 - acc: 1.00 - ETA: 25:18 - loss: 0.1756 - acc: 1.00 - ETA: 24:07 - loss: 0.1755 - acc: 1.00 - ETA: 22:54 - loss: 0.1754 - acc: 1.00 - ETA: 21:42 - loss: 0.1753 - acc: 1.00 - ETA: 20:29 - loss: 0.1753 - acc: 1.00 - ETA: 19:16 - loss: 0.1752 - acc: 1.00 - ETA: 18:04 - loss: 0.1754 - acc: 1.00 - ETA: 16:53 - loss: 0.1753 - acc: 1.00 - ETA: 15:41 - loss: 0.1751 - acc: 1.00 - ETA: 14:28 - loss: 0.1750 - acc: 1.00 - ETA: 13:16 - loss: 0.1750 - acc: 1.00 - ETA: 12:04 - loss: 0.1750 - acc: 1.00 - ETA: 10:51 - loss: 0.1749 - acc: 1.00 - ETA: 9:38 - loss: 0.1749 - acc: 1.0000 - ETA: 8:26 - loss: 0.1748 - acc: 1.000 - ETA: 7:14 - loss: 0.1747 - acc: 1.000 - ETA: 6:01 - loss: 0.1746 - acc: 1.000 - ETA: 4:49 - loss: 0.1751 - acc: 1.000 - ETA: 3:37 - loss: 0.1750 - acc: 1.000 - ETA: 2:24 - loss: 0.1750 - acc: 1.000 - ETA: 1:12 - loss: 0.1749 - acc: 1.000 - 2599s 8s/step - loss: 0.1748 - acc: 1.0000 - val_loss: 0.5901 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/300\n",
      "340/340 [==============================] - ETA: 39:19 - loss: 0.1726 - acc: 1.00 - ETA: 38:13 - loss: 0.1723 - acc: 1.00 - ETA: 37:06 - loss: 0.1728 - acc: 1.00 - ETA: 35:55 - loss: 0.1727 - acc: 1.00 - ETA: 34:41 - loss: 0.1725 - acc: 1.00 - ETA: 33:31 - loss: 0.1735 - acc: 1.00 - ETA: 32:21 - loss: 0.1733 - acc: 1.00 - ETA: 31:08 - loss: 0.1734 - acc: 1.00 - ETA: 29:58 - loss: 0.1734 - acc: 1.00 - ETA: 28:46 - loss: 0.1749 - acc: 1.00 - ETA: 27:33 - loss: 0.1746 - acc: 1.00 - ETA: 26:21 - loss: 0.1747 - acc: 1.00 - ETA: 25:09 - loss: 0.1758 - acc: 1.00 - ETA: 23:58 - loss: 0.1757 - acc: 1.00 - ETA: 22:46 - loss: 0.1755 - acc: 1.00 - ETA: 21:34 - loss: 0.1755 - acc: 1.00 - ETA: 20:22 - loss: 0.1753 - acc: 1.00 - ETA: 19:10 - loss: 0.1752 - acc: 1.00 - ETA: 17:58 - loss: 0.1750 - acc: 1.00 - ETA: 16:46 - loss: 0.1751 - acc: 1.00 - ETA: 15:34 - loss: 0.1757 - acc: 1.00 - ETA: 14:25 - loss: 0.1756 - acc: 1.00 - ETA: 13:13 - loss: 0.1754 - acc: 1.00 - ETA: 12:01 - loss: 0.1754 - acc: 1.00 - ETA: 10:48 - loss: 0.1756 - acc: 1.00 - ETA: 9:36 - loss: 0.1754 - acc: 1.0000 - ETA: 8:24 - loss: 0.1754 - acc: 1.000 - ETA: 7:12 - loss: 0.1754 - acc: 1.000 - ETA: 6:00 - loss: 0.1755 - acc: 1.000 - ETA: 4:48 - loss: 0.1754 - acc: 1.000 - ETA: 3:36 - loss: 0.1753 - acc: 1.000 - ETA: 2:24 - loss: 0.1752 - acc: 1.000 - ETA: 1:12 - loss: 0.1751 - acc: 1.000 - 2588s 8s/step - loss: 0.1750 - acc: 1.0000 - val_loss: 0.6132 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:24 - loss: 0.1752 - acc: 1.00 - ETA: 38:18 - loss: 0.1743 - acc: 1.00 - ETA: 37:06 - loss: 0.1743 - acc: 1.00 - ETA: 35:57 - loss: 0.1738 - acc: 1.00 - ETA: 34:49 - loss: 0.1738 - acc: 1.00 - ETA: 33:39 - loss: 0.1735 - acc: 1.00 - ETA: 32:24 - loss: 0.1734 - acc: 1.00 - ETA: 31:14 - loss: 0.1737 - acc: 1.00 - ETA: 30:02 - loss: 0.1737 - acc: 1.00 - ETA: 28:49 - loss: 0.1735 - acc: 1.00 - ETA: 27:41 - loss: 0.1734 - acc: 1.00 - ETA: 26:28 - loss: 0.1738 - acc: 1.00 - ETA: 25:15 - loss: 0.1736 - acc: 1.00 - ETA: 24:03 - loss: 0.1735 - acc: 1.00 - ETA: 22:50 - loss: 0.1735 - acc: 1.00 - ETA: 21:38 - loss: 0.1734 - acc: 1.00 - ETA: 20:25 - loss: 0.1733 - acc: 1.00 - ETA: 19:13 - loss: 0.1732 - acc: 1.00 - ETA: 18:01 - loss: 0.1731 - acc: 1.00 - ETA: 16:49 - loss: 0.1734 - acc: 1.00 - ETA: 15:37 - loss: 0.1734 - acc: 1.00 - ETA: 14:24 - loss: 0.1733 - acc: 1.00 - ETA: 13:12 - loss: 0.1735 - acc: 1.00 - ETA: 12:01 - loss: 0.1734 - acc: 1.00 - ETA: 10:49 - loss: 0.1734 - acc: 1.00 - ETA: 9:37 - loss: 0.1734 - acc: 1.0000 - ETA: 8:24 - loss: 0.1734 - acc: 1.000 - ETA: 7:12 - loss: 0.1734 - acc: 1.000 - ETA: 6:00 - loss: 0.1734 - acc: 1.000 - ETA: 4:48 - loss: 0.1734 - acc: 1.000 - ETA: 3:36 - loss: 0.1734 - acc: 1.000 - ETA: 2:24 - loss: 0.1733 - acc: 1.000 - ETA: 1:12 - loss: 0.1736 - acc: 1.000 - 2592s 8s/step - loss: 0.1736 - acc: 1.0000 - val_loss: 0.6162 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/300\n",
      "340/340 [==============================] - ETA: 39:49 - loss: 0.1783 - acc: 1.00 - ETA: 39:22 - loss: 0.1761 - acc: 1.00 - ETA: 38:10 - loss: 0.1751 - acc: 1.00 - ETA: 36:40 - loss: 0.1748 - acc: 1.00 - ETA: 35:15 - loss: 0.1750 - acc: 1.00 - ETA: 33:58 - loss: 0.1745 - acc: 1.00 - ETA: 32:47 - loss: 0.1739 - acc: 1.00 - ETA: 31:30 - loss: 0.1743 - acc: 1.00 - ETA: 30:15 - loss: 0.1741 - acc: 1.00 - ETA: 29:03 - loss: 0.1763 - acc: 1.00 - ETA: 27:48 - loss: 0.1759 - acc: 1.00 - ETA: 26:34 - loss: 0.1756 - acc: 1.00 - ETA: 25:21 - loss: 0.1754 - acc: 1.00 - ETA: 24:12 - loss: 0.1752 - acc: 1.00 - ETA: 22:58 - loss: 0.1752 - acc: 1.00 - ETA: 21:45 - loss: 0.1749 - acc: 1.00 - ETA: 20:31 - loss: 0.1755 - acc: 1.00 - ETA: 19:18 - loss: 0.1753 - acc: 1.00 - ETA: 18:05 - loss: 0.1751 - acc: 1.00 - ETA: 16:52 - loss: 0.1749 - acc: 1.00 - ETA: 15:39 - loss: 0.1757 - acc: 1.00 - ETA: 14:27 - loss: 0.1755 - acc: 1.00 - ETA: 13:14 - loss: 0.1754 - acc: 1.00 - ETA: 12:01 - loss: 0.1753 - acc: 1.00 - ETA: 10:49 - loss: 0.1751 - acc: 1.00 - ETA: 9:37 - loss: 0.1750 - acc: 1.0000 - ETA: 8:24 - loss: 0.1782 - acc: 1.000 - ETA: 7:12 - loss: 0.1781 - acc: 1.000 - ETA: 6:00 - loss: 0.1779 - acc: 1.000 - ETA: 4:48 - loss: 0.1794 - acc: 1.000 - ETA: 3:36 - loss: 0.1791 - acc: 1.000 - ETA: 2:24 - loss: 0.1789 - acc: 1.000 - ETA: 1:11 - loss: 0.1788 - acc: 1.000 - 2586s 8s/step - loss: 0.1786 - acc: 1.0000 - val_loss: 0.6043 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/300\n",
      "340/340 [==============================] - ETA: 39:17 - loss: 0.1735 - acc: 1.00 - ETA: 38:00 - loss: 0.1735 - acc: 1.00 - ETA: 37:26 - loss: 0.1729 - acc: 1.00 - ETA: 36:04 - loss: 0.1725 - acc: 1.00 - ETA: 34:48 - loss: 0.1722 - acc: 1.00 - ETA: 33:33 - loss: 0.1727 - acc: 1.00 - ETA: 32:19 - loss: 0.1725 - acc: 1.00 - ETA: 31:04 - loss: 0.1723 - acc: 1.00 - ETA: 29:51 - loss: 0.1721 - acc: 1.00 - ETA: 28:39 - loss: 0.1720 - acc: 1.00 - ETA: 27:28 - loss: 0.1720 - acc: 1.00 - ETA: 26:16 - loss: 0.1740 - acc: 1.00 - ETA: 25:04 - loss: 0.1737 - acc: 1.00 - ETA: 23:51 - loss: 0.1739 - acc: 1.00 - ETA: 22:39 - loss: 0.1737 - acc: 1.00 - ETA: 21:33 - loss: 0.1735 - acc: 1.00 - ETA: 20:22 - loss: 0.1736 - acc: 1.00 - ETA: 19:10 - loss: 0.1738 - acc: 1.00 - ETA: 17:58 - loss: 0.1738 - acc: 1.00 - ETA: 16:46 - loss: 0.1737 - acc: 1.00 - ETA: 15:34 - loss: 0.1735 - acc: 1.00 - ETA: 14:22 - loss: 0.1734 - acc: 1.00 - ETA: 13:11 - loss: 0.1736 - acc: 1.00 - ETA: 11:59 - loss: 0.1735 - acc: 1.00 - ETA: 10:47 - loss: 0.1735 - acc: 1.00 - ETA: 9:35 - loss: 0.1734 - acc: 1.0000 - ETA: 8:23 - loss: 0.1735 - acc: 1.000 - ETA: 7:11 - loss: 0.1735 - acc: 1.000 - ETA: 5:59 - loss: 0.1734 - acc: 1.000 - ETA: 4:47 - loss: 0.1740 - acc: 1.000 - ETA: 3:35 - loss: 0.1741 - acc: 1.000 - ETA: 2:23 - loss: 0.1741 - acc: 1.000 - ETA: 1:11 - loss: 0.1740 - acc: 1.000 - 2585s 8s/step - loss: 0.1739 - acc: 1.0000 - val_loss: 0.6100 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/300\n",
      "340/340 [==============================] - ETA: 39:37 - loss: 0.1715 - acc: 1.00 - ETA: 38:24 - loss: 0.1768 - acc: 1.00 - ETA: 37:08 - loss: 0.1751 - acc: 1.00 - ETA: 35:56 - loss: 0.1745 - acc: 1.00 - ETA: 35:05 - loss: 0.1743 - acc: 1.00 - ETA: 33:50 - loss: 0.1736 - acc: 1.00 - ETA: 32:33 - loss: 0.1733 - acc: 1.00 - ETA: 31:19 - loss: 0.1738 - acc: 1.00 - ETA: 30:05 - loss: 0.1737 - acc: 1.00 - ETA: 28:51 - loss: 0.1738 - acc: 1.00 - ETA: 27:38 - loss: 0.1734 - acc: 1.00 - ETA: 26:25 - loss: 0.1731 - acc: 1.00 - ETA: 25:13 - loss: 0.1729 - acc: 1.00 - ETA: 24:01 - loss: 0.1727 - acc: 1.00 - ETA: 22:49 - loss: 0.1727 - acc: 1.00 - ETA: 21:36 - loss: 0.1733 - acc: 1.00 - ETA: 20:24 - loss: 0.1734 - acc: 1.00 - ETA: 19:11 - loss: 0.1734 - acc: 1.00 - ETA: 17:59 - loss: 0.1734 - acc: 1.00 - ETA: 16:46 - loss: 0.1733 - acc: 1.00 - ETA: 15:33 - loss: 0.1732 - acc: 1.00 - ETA: 14:21 - loss: 0.1732 - acc: 1.00 - ETA: 13:09 - loss: 0.1731 - acc: 1.00 - ETA: 11:57 - loss: 0.1735 - acc: 1.00 - ETA: 10:45 - loss: 0.1733 - acc: 1.00 - ETA: 9:33 - loss: 0.1739 - acc: 1.0000 - ETA: 8:21 - loss: 0.1738 - acc: 1.000 - ETA: 7:09 - loss: 0.1737 - acc: 1.000 - ETA: 5:58 - loss: 0.1737 - acc: 1.000 - ETA: 4:46 - loss: 0.1735 - acc: 1.000 - ETA: 3:35 - loss: 0.1736 - acc: 1.000 - ETA: 2:23 - loss: 0.1735 - acc: 1.000 - ETA: 1:11 - loss: 0.1737 - acc: 1.000 - 2574s 8s/step - loss: 0.1736 - acc: 1.0000 - val_loss: 0.6122 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/300\n",
      "340/340 [==============================] - ETA: 39:22 - loss: 0.1747 - acc: 1.00 - ETA: 38:07 - loss: 0.1731 - acc: 1.00 - ETA: 36:51 - loss: 0.1774 - acc: 1.00 - ETA: 35:38 - loss: 0.1755 - acc: 1.00 - ETA: 34:27 - loss: 0.1749 - acc: 1.00 - ETA: 33:16 - loss: 0.1747 - acc: 1.00 - ETA: 32:18 - loss: 0.1741 - acc: 1.00 - ETA: 31:03 - loss: 0.1751 - acc: 1.00 - ETA: 29:51 - loss: 0.1759 - acc: 1.00 - ETA: 28:54 - loss: 0.1753 - acc: 1.00 - ETA: 27:51 - loss: 0.1752 - acc: 1.00 - ETA: 26:51 - loss: 0.1755 - acc: 1.00 - ETA: 25:53 - loss: 0.1754 - acc: 1.00 - ETA: 24:56 - loss: 0.1757 - acc: 1.00 - ETA: 23:55 - loss: 0.1755 - acc: 1.00 - ETA: 22:48 - loss: 0.1767 - acc: 1.00 - ETA: 21:35 - loss: 0.1765 - acc: 1.00 - ETA: 20:21 - loss: 0.1763 - acc: 1.00 - ETA: 19:07 - loss: 0.1759 - acc: 1.00 - ETA: 17:56 - loss: 0.1756 - acc: 1.00 - ETA: 16:42 - loss: 0.1754 - acc: 1.00 - ETA: 15:24 - loss: 0.1751 - acc: 1.00 - ETA: 14:07 - loss: 0.1749 - acc: 1.00 - ETA: 12:49 - loss: 0.1748 - acc: 1.00 - ETA: 11:32 - loss: 0.1746 - acc: 1.00 - ETA: 10:15 - loss: 0.1745 - acc: 1.00 - ETA: 9:00 - loss: 0.1745 - acc: 1.0000 - ETA: 7:43 - loss: 0.1743 - acc: 1.000 - ETA: 6:27 - loss: 0.1742 - acc: 1.000 - ETA: 5:10 - loss: 0.1740 - acc: 1.000 - ETA: 3:52 - loss: 0.1739 - acc: 1.000 - ETA: 2:35 - loss: 0.1738 - acc: 1.000 - ETA: 1:17 - loss: 0.1737 - acc: 1.000 - 2778s 8s/step - loss: 0.1736 - acc: 1.0000 - val_loss: 0.6174 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:37 - loss: 0.1745 - acc: 1.00 - ETA: 38:19 - loss: 0.1725 - acc: 1.00 - ETA: 37:41 - loss: 0.1721 - acc: 1.00 - ETA: 36:17 - loss: 0.1714 - acc: 1.00 - ETA: 35:00 - loss: 0.1713 - acc: 1.00 - ETA: 33:43 - loss: 0.1711 - acc: 1.00 - ETA: 32:28 - loss: 0.1715 - acc: 1.00 - ETA: 31:13 - loss: 0.1713 - acc: 1.00 - ETA: 29:59 - loss: 0.1712 - acc: 1.00 - ETA: 28:58 - loss: 0.1713 - acc: 1.00 - ETA: 27:58 - loss: 0.1714 - acc: 1.00 - ETA: 26:55 - loss: 0.1713 - acc: 1.00 - ETA: 25:44 - loss: 0.1714 - acc: 1.00 - ETA: 24:28 - loss: 0.1713 - acc: 1.00 - ETA: 23:14 - loss: 0.1717 - acc: 1.00 - ETA: 21:59 - loss: 0.1716 - acc: 1.00 - ETA: 20:44 - loss: 0.1715 - acc: 1.00 - ETA: 19:30 - loss: 0.1716 - acc: 1.00 - ETA: 18:17 - loss: 0.1718 - acc: 1.00 - ETA: 17:02 - loss: 0.1718 - acc: 1.00 - ETA: 15:49 - loss: 0.1717 - acc: 1.00 - ETA: 14:35 - loss: 0.1716 - acc: 1.00 - ETA: 13:22 - loss: 0.1718 - acc: 1.00 - ETA: 12:08 - loss: 0.1717 - acc: 1.00 - ETA: 10:55 - loss: 0.1718 - acc: 1.00 - ETA: 9:42 - loss: 0.1717 - acc: 1.0000 - ETA: 8:29 - loss: 0.1717 - acc: 1.000 - ETA: 7:16 - loss: 0.1718 - acc: 1.000 - ETA: 6:03 - loss: 0.1720 - acc: 1.000 - ETA: 4:50 - loss: 0.1721 - acc: 1.000 - ETA: 3:38 - loss: 0.1721 - acc: 1.000 - ETA: 2:25 - loss: 0.1721 - acc: 1.000 - ETA: 1:12 - loss: 0.1720 - acc: 1.000 - 2608s 8s/step - loss: 0.1719 - acc: 1.0000 - val_loss: 0.6219 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/300\n",
      "340/340 [==============================] - ETA: 39:39 - loss: 0.1701 - acc: 1.00 - ETA: 38:13 - loss: 0.1703 - acc: 1.00 - ETA: 37:04 - loss: 0.1710 - acc: 1.00 - ETA: 35:50 - loss: 0.1706 - acc: 1.00 - ETA: 34:35 - loss: 0.1706 - acc: 1.00 - ETA: 33:24 - loss: 0.1713 - acc: 1.00 - ETA: 32:11 - loss: 0.1712 - acc: 1.00 - ETA: 31:07 - loss: 0.1712 - acc: 1.00 - ETA: 29:55 - loss: 0.1711 - acc: 1.00 - ETA: 28:41 - loss: 0.1710 - acc: 1.00 - ETA: 27:29 - loss: 0.1709 - acc: 1.00 - ETA: 26:17 - loss: 0.1710 - acc: 1.00 - ETA: 25:05 - loss: 0.1715 - acc: 1.00 - ETA: 23:52 - loss: 0.1714 - acc: 1.00 - ETA: 22:40 - loss: 0.1713 - acc: 1.00 - ETA: 21:28 - loss: 0.1712 - acc: 1.00 - ETA: 20:16 - loss: 0.1712 - acc: 1.00 - ETA: 19:04 - loss: 0.1713 - acc: 1.00 - ETA: 17:53 - loss: 0.1714 - acc: 1.00 - ETA: 16:41 - loss: 0.1714 - acc: 1.00 - ETA: 15:29 - loss: 0.1719 - acc: 1.00 - ETA: 14:18 - loss: 0.1719 - acc: 1.00 - ETA: 13:06 - loss: 0.1721 - acc: 1.00 - ETA: 11:54 - loss: 0.1722 - acc: 1.00 - ETA: 10:43 - loss: 0.1721 - acc: 1.00 - ETA: 9:31 - loss: 0.1721 - acc: 1.0000 - ETA: 8:20 - loss: 0.1720 - acc: 1.000 - ETA: 7:08 - loss: 0.1724 - acc: 1.000 - ETA: 5:57 - loss: 0.1725 - acc: 1.000 - ETA: 4:45 - loss: 0.1725 - acc: 1.000 - ETA: 3:34 - loss: 0.1730 - acc: 1.000 - ETA: 2:22 - loss: 0.1729 - acc: 1.000 - ETA: 1:11 - loss: 0.1729 - acc: 1.000 - 2568s 8s/step - loss: 0.1728 - acc: 1.0000 - val_loss: 0.6275 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/300\n",
      "340/340 [==============================] - ETA: 39:04 - loss: 0.1850 - acc: 1.00 - ETA: 38:00 - loss: 0.1785 - acc: 1.00 - ETA: 36:50 - loss: 0.1833 - acc: 1.00 - ETA: 35:37 - loss: 0.1797 - acc: 1.00 - ETA: 34:26 - loss: 0.1792 - acc: 1.00 - ETA: 33:13 - loss: 0.1783 - acc: 1.00 - ETA: 32:01 - loss: 0.1769 - acc: 1.00 - ETA: 30:50 - loss: 0.1764 - acc: 1.00 - ETA: 29:38 - loss: 0.1756 - acc: 1.00 - ETA: 28:36 - loss: 0.1750 - acc: 1.00 - ETA: 27:24 - loss: 0.1747 - acc: 1.00 - ETA: 26:12 - loss: 0.1767 - acc: 1.00 - ETA: 25:00 - loss: 0.1764 - acc: 1.00 - ETA: 23:48 - loss: 0.1760 - acc: 1.00 - ETA: 22:36 - loss: 0.1756 - acc: 1.00 - ETA: 21:24 - loss: 0.1754 - acc: 1.00 - ETA: 20:13 - loss: 0.1754 - acc: 1.00 - ETA: 19:03 - loss: 0.1753 - acc: 1.00 - ETA: 17:51 - loss: 0.1750 - acc: 1.00 - ETA: 16:39 - loss: 0.1748 - acc: 1.00 - ETA: 15:28 - loss: 0.1747 - acc: 1.00 - ETA: 14:16 - loss: 0.1744 - acc: 1.00 - ETA: 13:05 - loss: 0.1743 - acc: 1.00 - ETA: 11:54 - loss: 0.1743 - acc: 1.00 - ETA: 10:42 - loss: 0.1741 - acc: 1.00 - ETA: 9:31 - loss: 0.1740 - acc: 1.0000 - ETA: 8:19 - loss: 0.1739 - acc: 1.000 - ETA: 7:08 - loss: 0.1739 - acc: 1.000 - ETA: 5:56 - loss: 0.1738 - acc: 1.000 - ETA: 4:45 - loss: 0.1737 - acc: 1.000 - ETA: 3:34 - loss: 0.1739 - acc: 1.000 - ETA: 2:22 - loss: 0.1738 - acc: 1.000 - ETA: 1:11 - loss: 0.1737 - acc: 1.000 - 2563s 8s/step - loss: 0.1736 - acc: 1.0000 - val_loss: 0.6256 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/300\n",
      "340/340 [==============================] - ETA: 39:15 - loss: 0.1698 - acc: 1.00 - ETA: 38:04 - loss: 0.1697 - acc: 1.00 - ETA: 36:49 - loss: 0.1695 - acc: 1.00 - ETA: 35:34 - loss: 0.1694 - acc: 1.00 - ETA: 34:23 - loss: 0.1735 - acc: 1.00 - ETA: 33:14 - loss: 0.1738 - acc: 1.00 - ETA: 32:02 - loss: 0.1734 - acc: 1.00 - ETA: 30:51 - loss: 0.1735 - acc: 1.00 - ETA: 29:40 - loss: 0.1732 - acc: 1.00 - ETA: 28:29 - loss: 0.1728 - acc: 1.00 - ETA: 27:18 - loss: 0.1726 - acc: 1.00 - ETA: 26:13 - loss: 0.1723 - acc: 1.00 - ETA: 25:01 - loss: 0.1722 - acc: 1.00 - ETA: 23:48 - loss: 0.1721 - acc: 1.00 - ETA: 22:36 - loss: 0.1723 - acc: 1.00 - ETA: 21:25 - loss: 0.1726 - acc: 1.00 - ETA: 20:13 - loss: 0.1724 - acc: 1.00 - ETA: 19:02 - loss: 0.1722 - acc: 1.00 - ETA: 17:50 - loss: 0.1721 - acc: 1.00 - ETA: 16:39 - loss: 0.1721 - acc: 1.00 - ETA: 15:27 - loss: 0.1723 - acc: 1.00 - ETA: 14:16 - loss: 0.1722 - acc: 1.00 - ETA: 13:04 - loss: 0.1723 - acc: 1.00 - ETA: 11:53 - loss: 0.1722 - acc: 1.00 - ETA: 10:43 - loss: 0.1721 - acc: 1.00 - ETA: 9:31 - loss: 0.1720 - acc: 1.0000 - ETA: 8:20 - loss: 0.1720 - acc: 1.000 - ETA: 7:08 - loss: 0.1720 - acc: 1.000 - ETA: 5:56 - loss: 0.1719 - acc: 1.000 - ETA: 4:45 - loss: 0.1718 - acc: 1.000 - ETA: 3:34 - loss: 0.1717 - acc: 1.000 - ETA: 2:22 - loss: 0.1716 - acc: 1.000 - ETA: 1:11 - loss: 0.1716 - acc: 1.000 - 2565s 8s/step - loss: 0.1716 - acc: 1.0000 - val_loss: 0.6268 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/300\n",
      "340/340 [==============================] - ETA: 39:03 - loss: 0.1721 - acc: 1.00 - ETA: 37:56 - loss: 0.1774 - acc: 1.00 - ETA: 36:44 - loss: 0.1745 - acc: 1.00 - ETA: 35:34 - loss: 0.1737 - acc: 1.00 - ETA: 34:21 - loss: 0.1737 - acc: 1.00 - ETA: 33:12 - loss: 0.1729 - acc: 1.00 - ETA: 32:01 - loss: 0.1724 - acc: 1.00 - ETA: 30:51 - loss: 0.1722 - acc: 1.00 - ETA: 29:40 - loss: 0.1725 - acc: 1.00 - ETA: 28:29 - loss: 0.1723 - acc: 1.00 - ETA: 27:17 - loss: 0.1720 - acc: 1.00 - ETA: 26:06 - loss: 0.1719 - acc: 1.00 - ETA: 24:55 - loss: 0.1717 - acc: 1.00 - ETA: 23:47 - loss: 0.1716 - acc: 1.00 - ETA: 22:36 - loss: 0.1714 - acc: 1.00 - ETA: 21:24 - loss: 0.1713 - acc: 1.00 - ETA: 20:13 - loss: 0.1711 - acc: 1.00 - ETA: 19:01 - loss: 0.1711 - acc: 1.00 - ETA: 17:50 - loss: 0.1711 - acc: 1.00 - ETA: 16:38 - loss: 0.1710 - acc: 1.00 - ETA: 15:27 - loss: 0.1710 - acc: 1.00 - ETA: 14:15 - loss: 0.1710 - acc: 1.00 - ETA: 13:04 - loss: 0.1710 - acc: 1.00 - ETA: 11:53 - loss: 0.1711 - acc: 1.00 - ETA: 10:41 - loss: 0.1710 - acc: 1.00 - ETA: 9:30 - loss: 0.1764 - acc: 1.0000 - ETA: 8:20 - loss: 0.1761 - acc: 1.000 - ETA: 7:08 - loss: 0.1760 - acc: 1.000 - ETA: 5:57 - loss: 0.1758 - acc: 1.000 - ETA: 4:45 - loss: 0.1756 - acc: 1.000 - ETA: 3:34 - loss: 0.1754 - acc: 1.000 - ETA: 2:22 - loss: 0.1752 - acc: 1.000 - ETA: 1:11 - loss: 0.1751 - acc: 1.000 - 2565s 8s/step - loss: 0.1749 - acc: 1.0000 - val_loss: 0.6420 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:08 - loss: 0.1688 - acc: 1.00 - ETA: 37:58 - loss: 0.1692 - acc: 1.00 - ETA: 36:48 - loss: 0.1697 - acc: 1.00 - ETA: 35:57 - loss: 0.1695 - acc: 1.00 - ETA: 34:40 - loss: 0.1695 - acc: 1.00 - ETA: 33:25 - loss: 0.1702 - acc: 1.00 - ETA: 32:11 - loss: 0.1700 - acc: 1.00 - ETA: 30:59 - loss: 0.1700 - acc: 1.00 - ETA: 29:46 - loss: 0.1699 - acc: 1.00 - ETA: 28:35 - loss: 0.1698 - acc: 1.00 - ETA: 27:24 - loss: 0.1699 - acc: 1.00 - ETA: 26:12 - loss: 0.1698 - acc: 1.00 - ETA: 25:00 - loss: 0.1698 - acc: 1.00 - ETA: 23:48 - loss: 0.1698 - acc: 1.00 - ETA: 22:36 - loss: 0.1699 - acc: 1.00 - ETA: 21:26 - loss: 0.1701 - acc: 1.00 - ETA: 20:14 - loss: 0.1701 - acc: 1.00 - ETA: 19:03 - loss: 0.1700 - acc: 1.00 - ETA: 17:51 - loss: 0.1700 - acc: 1.00 - ETA: 16:39 - loss: 0.1732 - acc: 1.00 - ETA: 15:28 - loss: 0.1730 - acc: 1.00 - ETA: 14:16 - loss: 0.1728 - acc: 1.00 - ETA: 13:05 - loss: 0.1727 - acc: 1.00 - ETA: 11:53 - loss: 0.1726 - acc: 1.00 - ETA: 10:42 - loss: 0.1725 - acc: 1.00 - ETA: 9:30 - loss: 0.1723 - acc: 1.0000 - ETA: 8:19 - loss: 0.1722 - acc: 1.000 - ETA: 7:08 - loss: 0.1722 - acc: 1.000 - ETA: 5:57 - loss: 0.1722 - acc: 1.000 - ETA: 4:45 - loss: 0.1721 - acc: 1.000 - ETA: 3:34 - loss: 0.1721 - acc: 1.000 - ETA: 2:22 - loss: 0.1722 - acc: 1.000 - ETA: 1:11 - loss: 0.1721 - acc: 1.000 - 2566s 8s/step - loss: 0.1721 - acc: 1.0000 - val_loss: 0.6178 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/300\n",
      "340/340 [==============================] - ETA: 39:05 - loss: 0.1691 - acc: 1.00 - ETA: 37:57 - loss: 0.1689 - acc: 1.00 - ETA: 36:46 - loss: 0.1698 - acc: 1.00 - ETA: 35:35 - loss: 0.1696 - acc: 1.00 - ETA: 34:21 - loss: 0.1698 - acc: 1.00 - ETA: 33:29 - loss: 0.1709 - acc: 1.00 - ETA: 32:14 - loss: 0.1707 - acc: 1.00 - ETA: 31:01 - loss: 0.1706 - acc: 1.00 - ETA: 29:50 - loss: 0.1704 - acc: 1.00 - ETA: 28:37 - loss: 0.1704 - acc: 1.00 - ETA: 27:24 - loss: 0.1702 - acc: 1.00 - ETA: 26:11 - loss: 0.1702 - acc: 1.00 - ETA: 25:00 - loss: 0.1703 - acc: 1.00 - ETA: 23:48 - loss: 0.1707 - acc: 1.00 - ETA: 22:36 - loss: 0.1706 - acc: 1.00 - ETA: 21:25 - loss: 0.1706 - acc: 1.00 - ETA: 20:13 - loss: 0.1705 - acc: 1.00 - ETA: 19:01 - loss: 0.1705 - acc: 1.00 - ETA: 17:51 - loss: 0.1705 - acc: 1.00 - ETA: 16:39 - loss: 0.1705 - acc: 1.00 - ETA: 15:27 - loss: 0.1706 - acc: 1.00 - ETA: 14:17 - loss: 0.1706 - acc: 1.00 - ETA: 13:07 - loss: 0.1705 - acc: 1.00 - ETA: 11:55 - loss: 0.1705 - acc: 1.00 - ETA: 10:43 - loss: 0.1705 - acc: 1.00 - ETA: 9:32 - loss: 0.1704 - acc: 1.0000 - ETA: 8:20 - loss: 0.1703 - acc: 1.000 - ETA: 7:09 - loss: 0.1703 - acc: 1.000 - ETA: 5:57 - loss: 0.1702 - acc: 1.000 - ETA: 4:45 - loss: 0.1702 - acc: 1.000 - ETA: 3:34 - loss: 0.1701 - acc: 1.000 - ETA: 2:23 - loss: 0.1701 - acc: 1.000 - ETA: 1:11 - loss: 0.1701 - acc: 1.000 - 2572s 8s/step - loss: 0.1700 - acc: 1.0000 - val_loss: 0.6151 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/300\n",
      "340/340 [==============================] - ETA: 38:58 - loss: 0.1710 - acc: 1.00 - ETA: 37:53 - loss: 0.1715 - acc: 1.00 - ETA: 36:44 - loss: 0.1713 - acc: 1.00 - ETA: 35:34 - loss: 0.1707 - acc: 1.00 - ETA: 34:25 - loss: 0.1728 - acc: 1.00 - ETA: 33:14 - loss: 0.1735 - acc: 1.00 - ETA: 32:01 - loss: 0.1731 - acc: 1.00 - ETA: 31:02 - loss: 0.1729 - acc: 1.00 - ETA: 29:50 - loss: 0.1725 - acc: 1.00 - ETA: 28:37 - loss: 0.1722 - acc: 1.00 - ETA: 27:25 - loss: 0.1724 - acc: 1.00 - ETA: 26:13 - loss: 0.1722 - acc: 1.00 - ETA: 25:01 - loss: 0.1719 - acc: 1.00 - ETA: 23:49 - loss: 0.1718 - acc: 1.00 - ETA: 22:37 - loss: 0.1717 - acc: 1.00 - ETA: 21:25 - loss: 0.1714 - acc: 1.00 - ETA: 20:13 - loss: 0.1714 - acc: 1.00 - ETA: 19:02 - loss: 0.1713 - acc: 1.00 - ETA: 17:50 - loss: 0.1715 - acc: 1.00 - ETA: 16:38 - loss: 0.1714 - acc: 1.00 - ETA: 15:28 - loss: 0.1713 - acc: 1.00 - ETA: 14:17 - loss: 0.1712 - acc: 1.00 - ETA: 13:05 - loss: 0.1711 - acc: 1.00 - ETA: 11:53 - loss: 0.1712 - acc: 1.00 - ETA: 10:42 - loss: 0.1711 - acc: 1.00 - ETA: 9:30 - loss: 0.1712 - acc: 1.0000 - ETA: 8:19 - loss: 0.1711 - acc: 1.000 - ETA: 7:08 - loss: 0.1710 - acc: 1.000 - ETA: 5:56 - loss: 0.1710 - acc: 1.000 - ETA: 4:45 - loss: 0.1710 - acc: 1.000 - ETA: 3:34 - loss: 0.1710 - acc: 1.000 - ETA: 2:22 - loss: 0.1717 - acc: 1.000 - ETA: 1:11 - loss: 0.1716 - acc: 1.000 - 2564s 8s/step - loss: 0.1715 - acc: 1.0000 - val_loss: 0.6262 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/300\n",
      "340/340 [==============================] - ETA: 39:09 - loss: 0.1703 - acc: 1.00 - ETA: 37:51 - loss: 0.1697 - acc: 1.00 - ETA: 36:45 - loss: 0.1713 - acc: 1.00 - ETA: 35:35 - loss: 0.1706 - acc: 1.00 - ETA: 34:22 - loss: 0.1706 - acc: 1.00 - ETA: 33:12 - loss: 0.1705 - acc: 1.00 - ETA: 32:00 - loss: 0.1703 - acc: 1.00 - ETA: 30:48 - loss: 0.1701 - acc: 1.00 - ETA: 29:37 - loss: 0.1699 - acc: 1.00 - ETA: 28:32 - loss: 0.1707 - acc: 1.00 - ETA: 27:19 - loss: 0.1704 - acc: 1.00 - ETA: 26:08 - loss: 0.1703 - acc: 1.00 - ETA: 24:56 - loss: 0.1702 - acc: 1.00 - ETA: 23:45 - loss: 0.1701 - acc: 1.00 - ETA: 22:33 - loss: 0.1700 - acc: 1.00 - ETA: 21:22 - loss: 0.1699 - acc: 1.00 - ETA: 20:10 - loss: 0.1698 - acc: 1.00 - ETA: 18:59 - loss: 0.1697 - acc: 1.00 - ETA: 17:48 - loss: 0.1697 - acc: 1.00 - ETA: 16:36 - loss: 0.1697 - acc: 1.00 - ETA: 15:25 - loss: 0.1698 - acc: 1.00 - ETA: 14:14 - loss: 0.1698 - acc: 1.00 - ETA: 13:05 - loss: 0.1697 - acc: 1.00 - ETA: 11:53 - loss: 0.1697 - acc: 1.00 - ETA: 10:42 - loss: 0.1700 - acc: 1.00 - ETA: 9:30 - loss: 0.1700 - acc: 1.0000 - ETA: 8:19 - loss: 0.1700 - acc: 1.000 - ETA: 7:07 - loss: 0.1699 - acc: 1.000 - ETA: 5:56 - loss: 0.1699 - acc: 1.000 - ETA: 4:45 - loss: 0.1700 - acc: 1.000 - ETA: 3:34 - loss: 0.1699 - acc: 1.000 - ETA: 2:22 - loss: 0.1699 - acc: 1.000 - ETA: 1:11 - loss: 0.1698 - acc: 1.000 - 2562s 8s/step - loss: 0.1698 - acc: 1.0000 - val_loss: 0.6295 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/300\n",
      "340/340 [==============================] - ETA: 39:12 - loss: 0.1789 - acc: 1.00 - ETA: 37:59 - loss: 0.1758 - acc: 1.00 - ETA: 36:46 - loss: 0.1737 - acc: 1.00 - ETA: 35:36 - loss: 0.1726 - acc: 1.00 - ETA: 34:23 - loss: 0.1719 - acc: 1.00 - ETA: 33:11 - loss: 0.1716 - acc: 1.00 - ETA: 32:00 - loss: 0.1714 - acc: 1.00 - ETA: 30:49 - loss: 0.1710 - acc: 1.00 - ETA: 29:38 - loss: 0.1708 - acc: 1.00 - ETA: 28:26 - loss: 0.1706 - acc: 1.00 - ETA: 27:14 - loss: 0.1712 - acc: 1.00 - ETA: 26:07 - loss: 0.1712 - acc: 1.00 - ETA: 24:55 - loss: 0.1710 - acc: 1.00 - ETA: 23:43 - loss: 0.1709 - acc: 1.00 - ETA: 22:32 - loss: 0.1710 - acc: 1.00 - ETA: 21:21 - loss: 0.1709 - acc: 1.00 - ETA: 20:10 - loss: 0.1708 - acc: 1.00 - ETA: 18:58 - loss: 0.1706 - acc: 1.00 - ETA: 17:47 - loss: 0.1705 - acc: 1.00 - ETA: 16:36 - loss: 0.1705 - acc: 1.00 - ETA: 15:25 - loss: 0.1704 - acc: 1.00 - ETA: 14:14 - loss: 0.1703 - acc: 1.00 - ETA: 13:03 - loss: 0.1703 - acc: 1.00 - ETA: 11:51 - loss: 0.1702 - acc: 1.00 - ETA: 10:41 - loss: 0.1702 - acc: 1.00 - ETA: 9:30 - loss: 0.1702 - acc: 1.0000 - ETA: 8:18 - loss: 0.1703 - acc: 1.000 - ETA: 7:07 - loss: 0.1703 - acc: 1.000 - ETA: 5:56 - loss: 0.1703 - acc: 1.000 - ETA: 4:44 - loss: 0.1703 - acc: 1.000 - ETA: 3:33 - loss: 0.1703 - acc: 1.000 - ETA: 2:22 - loss: 0.1702 - acc: 1.000 - ETA: 1:11 - loss: 0.1702 - acc: 1.000 - 2561s 8s/step - loss: 0.1702 - acc: 1.0000 - val_loss: 0.6318 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 39:02 - loss: 0.1689 - acc: 1.00 - ETA: 38:56 - loss: 0.1732 - acc: 1.00 - ETA: 37:15 - loss: 0.1717 - acc: 1.00 - ETA: 35:56 - loss: 0.1710 - acc: 1.00 - ETA: 34:42 - loss: 0.1706 - acc: 1.00 - ETA: 33:25 - loss: 0.1705 - acc: 1.00 - ETA: 32:10 - loss: 0.1703 - acc: 1.00 - ETA: 30:56 - loss: 0.1700 - acc: 1.00 - ETA: 29:43 - loss: 0.1700 - acc: 1.00 - ETA: 28:31 - loss: 0.1702 - acc: 1.00 - ETA: 27:19 - loss: 0.1700 - acc: 1.00 - ETA: 26:07 - loss: 0.1700 - acc: 1.00 - ETA: 24:55 - loss: 0.1701 - acc: 1.00 - ETA: 23:43 - loss: 0.1701 - acc: 1.00 - ETA: 22:34 - loss: 0.1700 - acc: 1.00 - ETA: 21:22 - loss: 0.1699 - acc: 1.00 - ETA: 20:10 - loss: 0.1698 - acc: 1.00 - ETA: 18:59 - loss: 0.1698 - acc: 1.00 - ETA: 17:47 - loss: 0.1698 - acc: 1.00 - ETA: 16:36 - loss: 0.1698 - acc: 1.00 - ETA: 15:24 - loss: 0.1697 - acc: 1.00 - ETA: 14:13 - loss: 0.1696 - acc: 1.00 - ETA: 13:02 - loss: 0.1696 - acc: 1.00 - ETA: 11:50 - loss: 0.1696 - acc: 1.00 - ETA: 10:39 - loss: 0.1695 - acc: 1.00 - ETA: 9:28 - loss: 0.1695 - acc: 1.0000 - ETA: 8:17 - loss: 0.1702 - acc: 1.000 - ETA: 7:06 - loss: 0.1701 - acc: 1.000 - ETA: 5:55 - loss: 0.1701 - acc: 1.000 - ETA: 4:44 - loss: 0.1701 - acc: 1.000 - ETA: 3:33 - loss: 0.1700 - acc: 1.000 - ETA: 2:22 - loss: 0.1700 - acc: 1.000 - ETA: 1:11 - loss: 0.1699 - acc: 1.000 - 2553s 8s/step - loss: 0.1699 - acc: 1.0000 - val_loss: 0.6318 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/300\n",
      "340/340 [==============================] - ETA: 39:00 - loss: 0.1691 - acc: 1.00 - ETA: 37:46 - loss: 0.1694 - acc: 1.00 - ETA: 36:34 - loss: 0.1694 - acc: 1.00 - ETA: 35:41 - loss: 0.1694 - acc: 1.00 - ETA: 34:25 - loss: 0.1693 - acc: 1.00 - ETA: 33:12 - loss: 0.1693 - acc: 1.00 - ETA: 31:59 - loss: 0.1699 - acc: 1.00 - ETA: 30:47 - loss: 0.1696 - acc: 1.00 - ETA: 29:35 - loss: 0.1694 - acc: 1.00 - ETA: 28:25 - loss: 0.1693 - acc: 1.00 - ETA: 27:13 - loss: 0.1692 - acc: 1.00 - ETA: 26:02 - loss: 0.1692 - acc: 1.00 - ETA: 24:51 - loss: 0.1692 - acc: 1.00 - ETA: 23:40 - loss: 0.1692 - acc: 1.00 - ETA: 22:29 - loss: 0.1692 - acc: 1.00 - ETA: 21:18 - loss: 0.1691 - acc: 1.00 - ETA: 20:11 - loss: 0.1692 - acc: 1.00 - ETA: 18:59 - loss: 0.1691 - acc: 1.00 - ETA: 17:48 - loss: 0.1692 - acc: 1.00 - ETA: 16:36 - loss: 0.1692 - acc: 1.00 - ETA: 15:25 - loss: 0.1692 - acc: 1.00 - ETA: 14:14 - loss: 0.1691 - acc: 1.00 - ETA: 13:03 - loss: 0.1691 - acc: 1.00 - ETA: 11:51 - loss: 0.1691 - acc: 1.00 - ETA: 10:40 - loss: 0.1691 - acc: 1.00 - ETA: 9:29 - loss: 0.1691 - acc: 1.0000 - ETA: 8:17 - loss: 0.1690 - acc: 1.000 - ETA: 7:06 - loss: 0.1691 - acc: 1.000 - ETA: 5:55 - loss: 0.1691 - acc: 1.000 - ETA: 4:44 - loss: 0.1691 - acc: 1.000 - ETA: 3:33 - loss: 0.1691 - acc: 1.000 - ETA: 2:22 - loss: 0.1693 - acc: 1.000 - ETA: 1:11 - loss: 0.1694 - acc: 1.000 - 2555s 8s/step - loss: 0.1697 - acc: 1.0000 - val_loss: 0.6340 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/300\n",
      "340/340 [==============================] - ETA: 38:56 - loss: 0.1706 - acc: 1.00 - ETA: 37:46 - loss: 0.1715 - acc: 1.00 - ETA: 36:33 - loss: 0.1704 - acc: 1.00 - ETA: 35:24 - loss: 0.1699 - acc: 1.00 - ETA: 34:13 - loss: 0.1729 - acc: 1.00 - ETA: 33:07 - loss: 0.1721 - acc: 1.00 - ETA: 31:57 - loss: 0.1717 - acc: 1.00 - ETA: 30:45 - loss: 0.1717 - acc: 1.00 - ETA: 29:34 - loss: 0.1715 - acc: 1.00 - ETA: 28:22 - loss: 0.1711 - acc: 1.00 - ETA: 27:11 - loss: 0.1728 - acc: 1.00 - ETA: 26:00 - loss: 0.1725 - acc: 1.00 - ETA: 24:49 - loss: 0.1723 - acc: 1.00 - ETA: 23:38 - loss: 0.1722 - acc: 1.00 - ETA: 22:27 - loss: 0.1719 - acc: 1.00 - ETA: 21:16 - loss: 0.1718 - acc: 1.00 - ETA: 20:04 - loss: 0.1719 - acc: 1.00 - ETA: 18:54 - loss: 0.1717 - acc: 1.00 - ETA: 17:45 - loss: 0.1715 - acc: 1.00 - ETA: 16:34 - loss: 0.1714 - acc: 1.00 - ETA: 15:23 - loss: 0.1713 - acc: 1.00 - ETA: 14:11 - loss: 0.1712 - acc: 1.00 - ETA: 13:01 - loss: 0.1711 - acc: 1.00 - ETA: 11:50 - loss: 0.1710 - acc: 1.00 - ETA: 10:39 - loss: 0.1708 - acc: 1.00 - ETA: 9:28 - loss: 0.1707 - acc: 1.0000 - ETA: 8:17 - loss: 0.1707 - acc: 1.000 - ETA: 7:06 - loss: 0.1706 - acc: 1.000 - ETA: 5:55 - loss: 0.1706 - acc: 1.000 - ETA: 4:44 - loss: 0.1705 - acc: 1.000 - ETA: 3:33 - loss: 0.1705 - acc: 1.000 - ETA: 2:22 - loss: 0.1705 - acc: 1.000 - ETA: 1:11 - loss: 0.1706 - acc: 1.000 - 2557s 8s/step - loss: 0.1705 - acc: 1.0000 - val_loss: 0.6366 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/300\n",
      "310/340 [==========================>...] - ETA: 38:54 - loss: 0.1683 - acc: 1.00 - ETA: 37:47 - loss: 0.1681 - acc: 1.00 - ETA: 36:40 - loss: 0.1687 - acc: 1.00 - ETA: 35:30 - loss: 0.1692 - acc: 1.00 - ETA: 34:51 - loss: 0.1690 - acc: 1.00 - ETA: 34:30 - loss: 0.1690 - acc: 1.00 - ETA: 34:19 - loss: 0.1692 - acc: 1.00 - ETA: 33:39 - loss: 0.1691 - acc: 1.00 - ETA: 32:50 - loss: 0.1691 - acc: 1.00 - ETA: 31:47 - loss: 0.1690 - acc: 1.00 - ETA: 30:44 - loss: 0.1694 - acc: 1.00 - ETA: 29:38 - loss: 0.1694 - acc: 1.00 - ETA: 28:26 - loss: 0.1699 - acc: 1.00 - ETA: 27:12 - loss: 0.1697 - acc: 1.00 - ETA: 25:59 - loss: 0.1702 - acc: 1.00 - ETA: 24:41 - loss: 0.1701 - acc: 1.00 - ETA: 23:22 - loss: 0.1700 - acc: 1.00 - ETA: 22:03 - loss: 0.1708 - acc: 1.00 - ETA: 20:43 - loss: 0.1708 - acc: 1.00 - ETA: 19:25 - loss: 0.1707 - acc: 1.00 - ETA: 18:03 - loss: 0.1706 - acc: 1.00 - ETA: 16:42 - loss: 0.1705 - acc: 1.00 - ETA: 15:19 - loss: 0.1704 - acc: 1.00 - ETA: 13:55 - loss: 0.1705 - acc: 1.00 - ETA: 12:32 - loss: 0.1704 - acc: 1.00 - ETA: 11:08 - loss: 0.1704 - acc: 1.00 - ETA: 9:44 - loss: 0.1703 - acc: 1.0000 - ETA: 8:21 - loss: 0.1704 - acc: 1.000 - ETA: 6:56 - loss: 0.1703 - acc: 1.000 - ETA: 5:31 - loss: 0.1702 - acc: 1.000 - ETA: 4:07 - loss: 0.1702 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-6174c9e43021>\", line 9, in <module>\n",
      "    callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1712, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2475, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1128, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1344, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1329, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              verbose=1,\n",
    "              validation_data=(X_val, Y_val),\n",
    "              shuffle=True,\n",
    "              callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=True,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=True,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=False,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=False,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        epochs=nb_epoch, verbose=1, max_q_size=100,\n",
    "                        callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "八、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997\n"
     ]
    }
   ],
   "source": [
    "for im in enumerate(vid):\n",
    "    len_video=im[0]+1\n",
    "print(len_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model('imagemodel4.1.h5')\n",
    "#del X\n",
    "#del PRED\n",
    "#del pred\n",
    "PRED=np.zeros([len_video,1])\n",
    "sample=np.zeros([1,img_rows,img_cols,img_channels])\n",
    "indice=0\n",
    "for im in enumerate(vid):\n",
    "    sample[0,:,:,:]=im[1]\n",
    "    sample -=mean_X\n",
    "    sample /= 128\n",
    "    pred=model.predict(sample)\n",
    "    PRED[indice]=np.argmax(pred)\n",
    "    #print(PRED[indice])\n",
    "    indice +=1\n",
    "np.save('PRED',PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAF/tJREFUeJzt3XuYHXV9x/HPN5sLCSSQEEAaslnAAMYbhFVDKegDiNwEpSioCGo1j1Ypai+G0gJ9WkVrS+uFSqOiogGpCI+0SAUFb2BCc4MAARJICAkxITEJhIRc2G//OLPJ2SW7Z/bs/GbOb+b9ep59cnYy58z3zNnzmd98Z84Zc3cBAOIxpOgCAAADQ3ADQGQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABAZghsAIjM0xIOOHz/eOzo6Qjw0AJTSvHnz1rn7AWnmDRLcHR0dmjt3boiHBoBSMrOn085LqwQAIkNwA0BkCG4AiAzBDQCRIbgBIDIENwBEhuAGgMgEOY8b1bJywxYtXbtZbzvywKJLaeiFl3bonsfW6pyjJ+S63IdXbdJvlqzT+s3b9HdnTcl12UX47wef1YlHHKB9Rw7Lfdlznlqv82fO1vmdE3XQmBF9zrezy/Ufv3xSkrTvyGG6+LhJg172qBFD9fG3Hj7ox2mE4Magnfpvv9aW7S9r+RfPLLqUhmb8eJHuWLRakw8crSl/NCa35Z71td/uuv2504/SsLby7uwuW/eiLrlpgU466kBd/6E35b7882fOliTdPPcZmfU9X/3ldjdt3aGv3bt00Msev88Ightx2LL95aJLSO3ZTVslSVt3xFNzbLYmfw/PbtxacCXSsqv7Hkx0zLgj9bytprybfaBF1Y/0gGYQ3AAQGYIbQKb66ysjGwQ3AESG4EZF0WhGvAhuAIgMwY2KKq4R6xUZ7XP2TDgENyqKVAmFg5PhEdyoFDIFZUBwAzmjhYDBIrgBBFGVXn4RCG4AmTIaUsER3AAQGYIbyBk9bgwWwQ0AkSG4gZxV5aAdexbhENwAMsUHcMJLFdxm9hkze8TMHjazm8xsr9CFAWXFSBSD1TC4zWyCpL+Q1Onur5PUJumC0IUBIRGeiFnaa04OlTTSzHZIGiXp2XAlAeFYsh9/3nW/0xP/dLqGD82/W1j2bcbmbTslSUvWbtZlty7Spq3btXrTS3pTx7iCKyuPhsHt7qvM7F8krZC0VdJd7n5X7/nMbLqk6ZLU3t6edZ1AJrxuqP2fv3pSl5w8ucBqyunzdyzedfumB1bsur149fO5fjjnqFeN7vf/v3zeG/TXtzyUUzXZahjcZjZW0jmSDpW0UdKPzOxCd/9B/XzuPlPSTEnq7Ows+6ACJbBp646iSyilDS9u3+P0R//hNA0ZEj64L79tkWbNWaEPTJvU73zv6Zyo93RODF5PCGn2E0+RtMzdn3P3HZJulfTHYcsCwrAWOOXBabAHVYW1mya4V0iaZmajrPZXf7KkxQ3uA7QkQjMHfWwb89pmdr/ExW+iw2kY3O4+R9ItkuZLWpTcZ2bguoDSKvumo6/AzHtvpwV2roJJdVaJu18p6crAtQDB1YdH2QO0usr/yvLJSSBndGvyUeavlyW4AZRKFTaMBDeQt5IHS9Fn7uw6OFneATfBjeqqwsisCK2Sl61SRwgEN5Czqnyta1GqsH4JbgClRKsEQGZo0YRVhfVLcAPIVNEj3e7c5nRAoISq0AstQssEZouUEQLBDeSMzUVYtEoAYICKbpV0a5EygiC4USnW43Yxb22+oTCsKrTACG5USvnf0tCuT06Wd8xNcKOyihqZsfHIR3ljm+BGxZT5zYyaKmwY017lHSiFVnhT59niXrLmBc2as0JdKRbqLt089xmd3zlxUAcYH/v9C83fOQPrk2telrhTQnADZXbL/JX67v3LNXbUsIbzbthSu3jy92c/nWr+VrX2+ZckSaOGtxVcSTgENyqlFQZhefbWu7pcI4e1acEVpzac95q7HtdX71mqz5xyhC49ZfKgltsx445B3X8whiRD7UPGjiqshtDocaOyqnBWnvvAWwZVOJ0udgQ3UGKu3SNQlAfBDeQtxwFtl3tLtIeQLYIbKDF3tUZjH5kiuIGc5d1BrlqrpAodeoIbKLEu91Kfz1xVBDeQszzPZnGv3oi7Cs+W4EalVCzDODhZUgQ3KqUVzt3O8zxp1wA2ViXZqrXASxwcwQ2UWO0DOCkDOdmqtcLGLQsl2Q7tEcGNSmmFN3O+PW5aJWVEcAMlNqCPvLfCVg2pENxAzvLsRLi8cmeVVAHBDZRYl1fj9LiqIbiBEhvQwUlEg+BGZRV1tfU8l+sV/ORkUa9rnlIFt5ntZ2a3mNljZrbYzI4LXRiAwRvQedyIRtor4HxF0v+6+3lmNlxSeS8tAQSW9+mAaQ9OliXfq9AaahjcZjZG0omSPiRJ7r5d0vawZQHIwkAOTnqvf2NVhVZJmhH3YZKek/QdM3ujpHmSLnX3F4NWhlTuXLRaB+27l6a2jy26FKR09Z2LtffwfC73uuCZDRo6pJqHsqw0+xCvlOavZ6ikqZIucfc5ZvYVSTMk/X39TGY2XdJ0SWpvb8+6TvThE7PmS5KWf/HMgiuJT1HjsoUrNua6vLcesX+q+ULF3D++63Va8PSGQI9eTWmCe6Wkle4+J/n9FtWCuwd3nylppiR1dnaWf18FaNL9l51cdAm5+uC0SfrgtElFl1EqDfeh3P33kp4xsyOTSSdLejRoVUAgZd59RnWkbbRdImlWckbJU5I+HK4kIJw8v1IVCCVVcLv7QkmdgWsBAKRQzcPNqCxaJSgDghuVQqukbxX43EppENwAEBmCG5VCqyRfjOLDILhRWRX4ZHRzMlwxReZ2mTcaBDeAYNg2hkFwA0BkCG4AiAzBDQCRIbhRWZzTjVgR3AAkledUySqcLURwA0BkCG4AwRQx+i3z+dvdCG4APcTeaaBVApRYFd7gRSty9FvmkTfBjWop8ZsZ1ZHPpaaBVlE3yp41Z4U2bd0hK/PQrGCs2TAIblTKzq6uHr//Zsk67b/38KDLpCOTry+c+zp9/o7FOnT83kWXEgzBjUoZt/eIHr9/7yNv1tET9wu6zK4u12F/+9Ogy2hVRWy0jp00Trf++fEFLDk/9LhRKUV0RWLpxMRSJwhuVEwR2UQPHVkjuAEgMgQ3KoXBb9+6z2vP8vx2VncYBDcARIbgRqUxItwtxN4I/f0wCG5USlm+ujQWzvcKBEFwo1IYAKIMCG5UCsGNMiC4AQRDjzsMghuVRq6ERY87DIIblcLByb6xZuJBcKNaSCeUAMENIBgaJWEQ3KgUBtyNOXHb8ghuVBo977BYu2GkDm4zazOzBWb2PyELAlCsLDdmnA4YxkBG3JdKWhyqECAPBEljtEpaX6rgNrNDJJ0p6VthywHCIrZRBmmvOfnvkv5G0uiAtSBy23a+rBFD2xrO9537lun+J9fv+n3Zuhe1dO1mvX3KQSHLkyTd89ja4Mvoz6jhjdcP0EjD4DazsyStdfd5Zva2fuabLmm6JLW3t2dWIOKxfN0WHfmqxtv26+9bpk1bdmjC2FGSpKVrN0uSnl7/otqGhD1efvgBe+uJNZt3/Z5X5+TDx3foO/ct17UfmJrPAptAFykeaUbcx0s628zOkLSXpDFm9gN3v7B+JnefKWmmJHV2dtIkQ5+6uqRTphyka957tCTp9Vf9TC+8tFM/nH6cxu09PJcazvrab/TwqudzWZYkXfnO1+rKd742t+Wh3BoOb9z9Mnc/xN07JF0g6Z7eoQ0MVI8zF7x7GoA0OI8buety15BX5nauu+p89xFilvbgpCTJ3X8p6ZdBKkFluPcM6e5vkOPDMEA6jLiRO5drSF1y7xr8ktulw0saBsGNzKT94EaX77ktkmerhDMo+kYbqfUR3MhdLRis1++MzoC0CG4UoPfByaTHneMwmFElYkZwI3e9WyWMuFtDiA0n28cwCG7kzt17nEFSxOmAQMwIbuTOpR6tkm6cDgikQ3Ajd11d3nO3vLtVQm6XDi9pGAQ3MpP2gJ+rV4+7gE4oByfzwWoOg+BG7tx7tkWcETcwIAQ3cufu2kOnhB53i2CU3PoIbuSuz4OTBeQ2o/ywWL1hENzIXZf3PDi5+0umAKRBcCN3tR533e/Jv7l+cjK3JVUbezRhENzIXe2sEr6rBGgWwY3MpD4dsNfBSZQXp12GMaALKYT280fX6KM3zNW5x0zQNecfXXQ5uZn91Ho9tHKjpp94+IDu99sl63bdfmjlRt372HO69JTJqe67auNWXfmTR7Rt58sDWmZ/Lrr+Ab3m4MYXC97xsu9xdF3IwUnG+YhQSwX3R2+YK0m6dcGqSgX3BTNnS9KAg/vCb8/Zdfvsr98nSamDe97TG/TzxWv0moPHaK9h2ex4rdu8TZu3jWw437GTxuqEyQfs+v2Wjx+n2x98NtceN17pwmmT9PCqTZp+wmFFl4IGWiq4kZ/uMzm+/v5jdPgB+wzqsTpm3CFJuvyM1+hjJw78Td/ZMU6dHeMGVcNAOfvwr7DvyGH6xoXHFl0GUqDHXVHduTWEUS4C4s8rDIK7oro4dxqIFsFdUXw/SE3Vnz/iRHBXVHeHN8tWSRHf8gdUEcFdMmkPunVxcA6IFsFdMqnzOECrhHOigXwQ3BXV3dbgrBIgPgR3yaQdcHdxcFISzx9xIrhLJm2Pe/cXO5FcQGwI7pJJP+LubpWEqyUGHKMNi/UbBsFdUbveTxUPbiBGBHfJpD+rhIOTQKwI7pJJ+yGYLi5eIImDk6GxfsMguEtmIBczkPK9XFgrofeaDw5+h0FwV9Tuj7wXWgaAJhDcFdXF6YBAtBoGt5lNNLN7zWyxmT1iZpfmURiaM9BWCbmNkPjisTDSXAFnp6S/dPf5ZjZa0jwzu9vdHw1cG5ow0DcKrRIgPg1H3O6+2t3nJ7dfkLRY0oTQhSGsrqofnFT3hSSq+fzzwvoNY0DXnDSzDknHSJrT/5xoxnuuu3/Qj3Hht+aoLcUwevWmlyTRKQFilDq4zWwfST+W9Gl3f34P/z9d0nRJam9vz6zAKhnW1vyx4v1GDdPGLTu017C2VPO3jxulP3n1eI0anm5+AK0jVXCb2TDVQnuWu9+6p3ncfaakmZLU2dnJEYkm3PixaQOav/vq6pK08IpTsy5nwCradQFyl+asEpP0bUmL3f2a8CUBKA025kGk2Tc/XtIHJZ1kZguTnzMC14UIxfRpRC6WjJg1bJW4+2/FdhNAMyLamMeET04CQGQIbmSGtgOQD4IbACJDcKOSuluv7CQgRgQ3gHDYMgZBcCMzMZ0OiJzwNxEEwQ0AkSG4kRnOKgHyQXCjknZfc7PgQoAmENwAwmHDGATBDSAYcjsMghuVxpkwYbF6wyC4UUlVvWQbyoHgRiVxcBIxI7gBIDIENwBEZkBXec/bTQ+s0EMrN+rqc9/Q1P3rr8nY24ihQ7RtZ9crpr9+wr5NLQvAK9GJCqOlg/uyWxdJUtPB3Z89hbYkjRk5VCOGxnHl82vfP1WfvHF+0WXojRP304PPbCy6DKAyWjq4i3DNe4/WQWP2ynWZ/e0Z9OfMNxysT96YcTFN6Jw0Nrrg5jQ1xKy0PW7nBF2kws58SLwLwyhxcDd3P97GAFpdeYO76AIAIJDyBjetEqBw7MGGUdrg7moyt4n7gYtyGxljzUCitMHtvDORAh95R4yiCO5m2h4cnASKx/ApjEiCO5/7oDmMWoF8xRHcTd2H5AZQTlEEd1cTw2cOTuYnxr2bCEuOEjtjYUQR3M21SnhrojGCJSzaaGHEEdxNjI+ajW3+zgC0ujiCu5kR956//A8Aolfe4KaLCRSOjmUYcQR3M60SDk6iHxwDQcyiCO5mzhBp5kwUVA9Xew+L1RtGquA2s9PM7HEzW2pmM0IX1VtTn5xscln8nQFodQ2D28zaJF0r6XRJUyS9z8ymhC6sHiNuANgtzYj7zZKWuvtT7r5d0g8lnRO2rF6a++gkAJRSmmtOTpD0TN3vKyW9JUw5u739ml/tuv3ub9yntgE2y3Y2+9FJeiUDNnxobfs/dEg8K2+vYbULQsdTcVyGWG1PeeSwOC68HZs0wb2nv+1XpKKZTZc0XZLa29ubKubYSWM17+kNkqTJB+2joW1DtPIPW3TUq0Y39XjL1r3Y4/fhbUO0/eW+T/B+/YR9deDofC8ULElXnDVFxx2+f1P3veBNE5vfSGXkkpNeLXfX+97S3OtehG9e1KnbFqzSpP1HFV1KKc2+7GS9+Qu/0M8/+9aiSykla3Tgz8yOk3SVu78j+f0ySXL3q/u6T2dnp8+dOzfLOgGg1Mxsnrt3ppk3TY/7/yRNNrNDzWy4pAsk3T6YAgEAzWvYKnH3nWb2KUk/k9Qm6Xp3fyR4ZQCAPUrT45a7/1TSTwPXAgBIIYpPTgIAdiO4ASAyBDcARIbgBoDIENwAEJmGH8Bp6kHNnpP0dJN3Hy9pXYblhEKd2YulVurMVix1SmFrneTuB6SZMUhwD4aZzU376aEiUWf2YqmVOrMVS51S69RKqwQAIkNwA0BkWjG4ZxZdQErUmb1YaqXObMVSp9QitbZcjxsA0L9WHHEDAPrRMsFd9AWJzWyimd1rZovN7BEzuzSZfpWZrTKzhcnPGXX3uSyp93Eze0eez8XMlpvZoqSmucm0cWZ2t5ktSf4dm0w3M/tqUs9DZja17nEuTuZfYmYXZ1zjkXXrbaGZPW9mn26FdWpm15vZWjN7uG5aZuvPzI5NXp+lyX2bvthOH7V+2cweS+q5zcz2S6Z3mNnWunV7XaOa+nreGdWZ2Wttta+WnpPUebPVvmY6qzpvrqtxuZktTKYXtj775e6F/6j2dbFPSjpM0nBJD0qaknMNB0uamtweLekJ1S6OfJWkv9rD/FOSOkdIOjSpvy2v5yJpuaTxvab9s6QZye0Zkr6U3D5D0p2qXc1omqQ5yfRxkp5K/h2b3B4b8DX+vaRJrbBOJZ0oaaqkh0OsP0kPSDouuc+dkk7PuNZTJQ1Nbn+prtaO+vl6Pc4ea+rreWdUZ2avtaT/knRBcvs6SZ/Iqs5e//+vkq4oen3299MqI+7CL0js7qvdfX5y+wVJi1W73mZfzpH0Q3ff5u7LJC1V7XkU+VzOkfS95Pb3JL2rbvoNXjNb0n5mdrCkd0i6293/4O4bJN0t6bRAtZ0s6Ul37++DWbmtU3f/taQ/7GH5g15/yf+Ncfffee3de0PdY2VSq7vf5e47k19nSzqkv8doUFNfz3vQdfZjQK91Mpo9SdItIetMlvNeSTf19xh5rM/+tEpw7+mCxP2FZlBm1iHpGElzkkmfSnZJr6/b7emr5ryei0u6y8zmWe16n5J0kLuvlmobIkkHtkitUu3KSfVvhlZcp1mtvwnJ7dD1dvuIaiO+boea2QIz+5WZnZBM66+mvp53VrJ4rfeXtLFuYxVqnZ4gaY27L6mb1mrrs2WCO9UFifNgZvtI+rGkT7v785K+IelwSUdLWq3abpTUd815PZfj3X2qpNMlfdLMTuxn3kJrTXqRZ0v6UTKpVddpXwZaV271mtnlknZKmpVMWi2p3d2PkfRZSTea2Zg8a+olq9c6r/rfp54DjFZbn5JaJ7hXSppY9/shkp7NuwgzG6ZaaM9y91slyd3XuPvL7t4l6Zuq7cpJfdecy3Nx92eTf9dKui2pa02yC9e9K7e2FWpVbeMy393XJDW35DpVdutvpXq2LoLUmxwMPUvSB5LddSWth/XJ7Xmq9YuPaFBTX8970DJ8rdep1qIa2mt6ZpLHPlfSzXX1t9T67NYqwV34BYmT3ta3JS1292vqph9cN9u7JXUfib5d0gVmNsLMDpU0WbWDFcGfi5ntbWaju2+rdqDq4WQ53Wc2XCzpJ3W1XmQ10yRtSnbhfibpVDMbm+zCnppMy1qPUUwrrtO65Q96/SX/94KZTUv+ri6qe6xMmNlpkj4n6Wx331I3/QAza0tuH6baOnyqQU19Pe8s6szktU42TPdKOi9EnYlTJD3m7rtaIK22PnfJ+mhnsz+qHbl/QrUt2uUFLP9PVNvVeUjSwuTnDEnfl7QomX67pIPr7nN5Uu/jqjtrIPRzUe2I+4PJzyPdy1CtD/gLSUuSf8cl003StUk9iyR11j3WR1Q7MLRU0ocD1DpK0npJ+9ZNK3ydqrYhWS1ph2qjpz/Lcv1J6lQtpJ6U9HUlH3bLsNalqvWCu/9Wr0vm/dPkb+JBSfMlvbNRTX0974zqzOy1Tv7uH0ie+48kjciqzmT6dyV9vNe8ha3P/n745CQARKZVWiUAgJQIbgCIDMENAJEhuAEgMgQ3AESG4AaAyBDcABAZghsAIvP/oVns2gS7ssAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26871103198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(PRED)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEiBJREFUeJzt3XuMXPV5xvHn9RJ7S2K6dqEtWrNZu1pHQo1UsBtDN6AqGy4xNPRCa6qUIChyekm6cRu1a6EW8gdi06px3TQqbkPapKWNAyEqihNgtUGphIwTzCUGDGuD2QbjcAl1sdQYgnn7x5ylw2YuZ3bP7Z35fqSVZ8+emXnPb9bPnnl3dl5zdwEA4lhSdgEAgM4Q3AAQDMENAMEQ3AAQDMENAMEQ3AAQDMENAMEQ3AAQDMENAMGclMeNnnrqqT48PJzHTQNAV9q7d+9L7n5amn1zCe7h4WE98MADedw0AHQlM5tNuy+tEgAIhuAGgGAIbgAIhuAGgGAIbgAIJpdXlaD3bJua0ZYL1pZdRiqjk9O6b2Ks8PvdNjWj7dMH9MzkJYXfdxnK/J4YnthVyv1KKuTx5Ywbmdg+faDsElI7fPR4KfcbaY2y0GvHWySCGwCCsTxmTq5fv975A5zuN/fUf77xsZHKtU1GJ6cbnmkPDvTn2jZptkZzuq1tUub3RJntkWY6eXzNbK+7r0+1L8GNLAxP7AoTQmXVOhcsUdZpscr8nojY4+4kuGmVAEAwBDcyMT42UnYJqQ0O9Jdyv5HWKAu9drxFolUCABVAqwQAuhjBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBpJrybmZbJF0rySXtk3S1u5czcRXIwLuvv0vXvHdNoSPWtk3N6DPfPKCnb+r+CTjtRrYVYWmfaebGjS33WbN1l95I3tk60mSitmfcZjYo6Y8krXf3n5fUJ+mKvAsD8nTs1ROFB8v26QNvhkS3Kzu0Jem1E+0XO+rjkbZVcpKknzCzkySdLOm5/EoCALSSagKOmY1LulHSDyXd4+4farDPZkmbJWloaGjd7OxsxqUCi/Pu6+/SsVdPNPxaXlPIW7UMlpi6qm1ShfZIM/Vtk/r2SDNltE0ynfJuZiskfUXSJklHJd0m6XZ3/9dm12F0GaqujInrvTTlvcwp6/XarXV9nWU/LlmPLnu/pEPu/qK7/0jSHZJ+aTEFAgAWLk1w/5ekc8zsZDMzSWOS9udbFpCv5cv6Cp9CPj42oiVW6F2WpgoT3pf2tV/sqI9H2h73J1Vrlbwu6SFJ17r7q832p1UCAJ3ppFWS6nXc7n69pOsXVRUAIBP85SQABENwA0AwBDcABENwA0AwBDcABENwA0AwBDcABENwA0AwBDcABENwA0AwBDcABENwA0AwBDcABJPq3QEBZGN0clr3TYwVep9lTqMpa6rM8MQuLV/Wp32fvLiU+88bZ9xAgQ4fPV52CT2j2XzRbkBwA0AwqSbgdIoJOMD/G52cbnimPTjQn1vbpCrDeuvl3TZpdcwR2iaZTnlfCIIbaGx4Ylfhfd9e7XGXef8LkfWUdwBAhRDcQIEGB/rLLqFnLF/WV3YJuaFVAgAVQKsEALoYwQ0AwRDcABAMwQ0AwRDcABAMwQ0AwRDcABAMwQ0AwRDcABAMwQ0AwRDcABAMwQ0AwRDcABBMquA2swEzu93MnjCz/WZ2bt6FAQAaSzvlfbuku9z9cjNbKunkHGtCh7ZNzWjLBWvLLgMplDWNJu0kmNHJaa1acbJ2fiTbc7MyJv90s7Zn3GZ2iqTzJd0iSe7+mrsfzbswpLd9+kDZJaBLHD56XHsOvVx2GWgjTatkjaQXJf2TmT1kZp8zs7fnXBcAoIm2E3DMbL2k+yWNuvseM9su6RV3//N5+22WtFmShoaG1s3OzuZUMqRae6TRmfb42Ahtk4qp2sT1+S2LZlPoJWnD6pULbpu0Om7aJj8u0ynvZvazku539+Hk8/MkTbh705VndFmx6B/GUfUed17T0fkebS/T0WXu/n1J3zOzdyWbxiQ9voj6AACLkPZVJR+TdGvyipKnJV2dX0no1PjYSNkloEsMDvRr1QpeNFZ1THkHgApgyjsAdDGCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCSft+3EDXKHuUGJNg8rdpx+7MJ9VXCWfcALpOt0+qJ7gBIBgm4KAnlN0eaYa2SXY27djd8Ex7MZPqi5TplPeFILhRZWWHOGGdv4hT5RldBgBdjOAG0HU2rF5Zdgm5olUCABVAqwQAuhjBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEEzq4DazPjN7yMy+lmdBAIDWOpnyPi5pv6RTcqoFgXU6caTsKTSDA/26fN0Z2nLB2sLuc3hiV5gxWqi2VGfcZrZK0iWSPpdvOUAxDh89ru3TBwq/326fPo5ipG2V/I2kP5X0Ro61AABSaDsBx8wulbTR3f/AzH5Z0ifc/dIG+22WtFmShoaG1s3OzuZQLqqkVbujUduk7PZIK+NjI7m0TVodM20T1Mt0yruZ3STpSkmvS+pXrcd9h7v/TrPrMLqs90Trcc8pchL43DFHmz6OYmQ6uszdt7r7KncflnSFpG+2Cm0AQL46eVUJ0DXmXlVStG6fPo5iMOUdACqAKe8A0MUIbgAIhuAGgGAIbgAIhuAGgGAIbgAIhuAGgGAIbgAIhuAGgGAIbgAIhuAGgGAIbgAIhuAGgGAIbgAIpnLvx92rU0K2Tc0saHTWph27tefQy3pm8hJt2rG7o1FYZU+hqX+MF3r8yNba676u05Yv030TY2WXghY4466IhU4cr58aHnmCeBkT1/HjXjvhOnz0eNlloA2CGwCCqcQEnHZP2bu1bbJtaqbhmWa7ieNz7ZFWmk0QL7s9kkZeE9fR2Nrrvq7XTjTOgcGBftomBcl0yvtCLGZ0Wa/2uDudkl5/Pam2XtEmrdfXutDjR7Z69f9fFTC6DAC6GMFdEeNjIwu6Xv3U8MgTxBd6/MjW0j7T4EB/2WWgjcq1SgCgF9EqAYAuRnADQDAENwAEQ3ADQDAENwAEQ3ADQDAENwAEQ3ADQDAENwAEQ3ADQDAENwAEQ3ADQDAENwAE0za4zewMM7vXzPab2WNmNl5EYQCAxtJMeX9d0p+4+4NmtlzSXjObcvfH8yhodHK6J0cllTWNhkknQDxtz7jd/Yi7P5hcPiZpv6TBvApiwjQAtNZRj9vMhiWdJWlPHsUAANpLPQHHzN4h6VuSbnT3Oxp8fbOkzZI0NDS0bnZ2NnURo5PTDc+0u33CdNnDeuejbQKUJ/Mp72b2Nklfk3S3u3+63f6LnfLeiwFCjxvobZmOLjMzk3SLpP1pQhsAkK80Pe5RSVdKep+ZPZx8bMyrICZMA0BrTHkHgApgyjsAdDGCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCIbgBIBiCGwCCSTPlvVSLnYjTbLLM+NiItk8fWPDt5mEhx7lpx25J0s6PnJt1OR3ZNjWjLResLbUGoFf07Bl31UJ7ofYcell7Dr1cdhlds55ABD0b3AAQVSUn4LQanJumnVC16ekL0eo4N+3Y3fQse8PqlYW1TbZNzTQ80x4fG6FtAnQo8ynvncpydFlePe4qWshxzh1f2dPaF/s4Ab2O0WUA0MV6NrjHx0bKLiETG1av1IbVK8suo2vWE4ig8q0SAOgFtEoAoIsR3AAQDMENAMEQ3AAQDMENAMEQ3AAQDMENAMEQ3AAQDMENAMEQ3AAQDMENAMEQ3AAQDMENAMGkCm4zu9jMnjSzg2Y2kXdRAIDm2k55N7M+SZ+VdIGkZyV9x8zudPfH8ygoj4k1y5f16dirJ1rus8SkN7y8STKLmZJehekzo5PTum9irNQaOsVk+nyt2bpLvzhc3Ci9XpLmjPs9kg66+9Pu/pqkL0m6LN+ystUutKVaaJcp+pT0w0ePl11Cx6KvedW94Wo6GxWLkya4ByV9r+7zZ5NtAIAStJ2AY2a/Kekid782+fxKSe9x94/N22+zpM2SNDQ0tG52djZ1EVUd6Jt3+2ExU9JbrVlRbZPRyemGZ9qDA/2VbZswmT5fa7buavrsdcNq2iatZDrl3czOlXSDu1+UfL5Vktz9pmbXWczosrJDvKxe8WL61FXocVehhk5FrDmSuf/LrHE6WY8u+46kETNbbWZLJV0h6c7FFAgAWLi2we3ur0v6qKS7Je2X9GV3fyzvwrK0fFlf232WWAGFtBB9SvrgQH/ZJXQs+ppX3RKrtUeQPaa8A0AFMOUdALoYwQ0AwRDcABAMwQ0AwRDcABBMLq8qMbMXJaX/08m3OlXSSxmWkxfqzF6UWqkzW1HqlPKt9Z3uflqaHXMJ7sUwswfSviSmTNSZvSi1Ume2otQpVadWWiUAEAzBDQDBVDG4/6HsAlKizuxFqZU6sxWlTqkitVauxw0AaK2KZ9wAgBYqE9xlDyQ2szPM7F4z229mj5nZeLL9BjM7bGYPJx8b666zNan3STO7qMhjMbNnzGxfUtMDybaVZjZlZgeSf1ck283M/jap57tmdnbd7VyV7H/AzK7KuMZ31a3bw2b2ipl9vApramafN7MXzOzRum2ZrZ+ZrUsen4PJdRf8/pNNav0rM3siqeerZjaQbB82sx/Wre3N7WpqdtwZ1ZnZY221t5bek9S502pvM51VnTvranzGzB5Otpe2ni25e+kfkvokPSVpjaSlkh6RdGbBNZwu6ezk8nJJM5LOlHSDpE802P/MpM5lklYn9fcVdSySnpF06rxtfylpIrk8IelTyeWNkr4hySSdI2lPsn2lpKeTf1ckl1fk+Bh/X9I7q7Cmks6XdLakR/NYP0nflnRucp1vSPpAxrVeKOmk5PKn6modrt9v3u00rKnZcWdUZ2aPtaQvS7oiuXyzpN/Pqs55X/9rSX9R9nq2+qjKGXfpA4nd/Yi7P5hcPqbae4+3mq15maQvufur7n5I0kHVjqPMY7lM0heSy1+Q9Kt127/oNfdLGjCz0yVdJGnK3V929/+WNCXp4pxqG5P0lLu3+sOswtbU3f9T0vxJtpmsX/K1U9x9t9f+936x7rYyqdXd7/Hae+VL0v2SVrW6jTY1NTvuRdfZQkePdXI2+z5Jt+dZZ3I/vyXp31vdRhHr2UpVgrtSA4nNbFjSWZL2JJs+mjwl/Xzd055mNRd1LC7pHjPba7V5n5L0M+5+RKr9IJL00xWpVapNTqr/z1DFNc1q/QaTy3nXO+ca1c745qw2s4fM7Ftmdl6yrVVNzY47K1k81j8l6WjdD6u81vQ8Sc+7e/1g0qqtZ2WCu1H/r5SXu5jZOyR9RdLH3f0VSX8v6eck/YKkI6o9jZKa11zUsYy6+9mSPiDpD83s/Bb7llpr0ov8oKTbkk1VXdNmOq2rsHrN7DpJr0u6Ndl0RNKQu58l6Y8l/ZuZnVJkTfNk9VgXVf9v660nGFVbT0nVCe5nJZ1R9/kqSc8VXYSZvU210L7V3e+QJHd/3t1PuPsbkv5RtadyUvOaCzkWd38u+fcFSV9N6no+eQo391TuhSrUqtoPlwfd/fmk5kquqbJbv2f11tZFLvUmvwy9VNKHkqfrSloPP0gu71WtX7y2TU3NjnvRMnysX1KtRXVSg/ozkdz2r0vaWVd/pdZzTlWCu/SBxElv6xZJ+93903XbT6/b7dckzf0m+k5JV5jZMjNbLWlEtV9W5H4sZvZ2M1s+d1m1X1Q9mtzP3CsbrpL0H3W1fthqzpH0P8lTuLslXWhmK5KnsBcm27L2lrOYKq5p3f0vev2Srx0zs3OS76sP191WJszsYkl/JumD7v6/ddtPM7O+5PIa1dbw6TY1NTvuLOrM5LFOfjDdK+nyPOpMvF/SE+7+Zgukauv5pqx/27nQD9V+cz+j2k+060q4//eq9lTnu5IeTj42SvoXSfuS7XdKOr3uOtcl9T6pulcN5H0sqv3G/ZHk47G5+1CtDzgt6UDy78pku0n6bFLPPknr627rGtV+MXRQ0tU51HqypB9I+sm6baWvqWo/SI5I+pFqZ0+/m+X6SVqvWkg9JenvlPyxW4a1HlStFzz3vXpzsu9vJN8Tj0h6UNKvtKup2XFnVGdmj3Xyff/t5Nhvk7QsqzqT7f8s6ffm7Vvaerb64C8nASCYqrRKAAApEdwAEAzBDQDBENwAEAzBDQDBENwAEAzBDQDBENwAEMz/AckSzbWTch3jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x267081cc1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(PRED,'+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5643\n",
      "5644\n",
      "5645\n",
      "5646\n",
      "5647\n",
      "5648\n",
      "5649\n",
      "5650\n",
      "5651\n",
      "5652\n",
      "5653\n",
      "5654\n",
      "5655\n",
      "5656\n",
      "5657\n",
      "5658\n",
      "5659\n",
      "5660\n",
      "5661\n",
      "5662\n",
      "5663\n",
      "5664\n",
      "5665\n",
      "5666\n",
      "5667\n",
      "5668\n",
      "5669\n",
      "5670\n",
      "5671\n",
      "5672\n",
      "5673\n",
      "5674\n",
      "5675\n",
      "5676\n",
      "5677\n",
      "5678\n",
      "5679\n",
      "5680\n",
      "5681\n",
      "5682\n",
      "5683\n",
      "5684\n",
      "5685\n",
      "5686\n",
      "5687\n",
      "5688\n",
      "5689\n",
      "5690\n",
      "5691\n",
      "5692\n",
      "5693\n",
      "5694\n",
      "5695\n",
      "5696\n",
      "5697\n",
      "5698\n",
      "5699\n",
      "5700\n",
      "5701\n",
      "5702\n",
      "5703\n",
      "5704\n",
      "5705\n",
      "5706\n",
      "5707\n",
      "5708\n",
      "5709\n",
      "5710\n",
      "5711\n",
      "5712\n",
      "5713\n",
      "5714\n",
      "5715\n",
      "5716\n",
      "5717\n",
      "5718\n",
      "5719\n",
      "5720\n",
      "5721\n",
      "5722\n",
      "5723\n",
      "5724\n",
      "5725\n",
      "5726\n",
      "5727\n",
      "5728\n",
      "5729\n",
      "5730\n",
      "5731\n",
      "5732\n",
      "5733\n",
      "5734\n",
      "5735\n",
      "5736\n",
      "5737\n",
      "5738\n",
      "5739\n",
      "5740\n",
      "5741\n",
      "5742\n",
      "5743\n",
      "5744\n",
      "5745\n",
      "5746\n",
      "5747\n",
      "5748\n",
      "5749\n",
      "5750\n",
      "5751\n",
      "5752\n",
      "5753\n",
      "5754\n",
      "5755\n",
      "5756\n",
      "5757\n",
      "5758\n",
      "5759\n",
      "5760\n",
      "5761\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5766\n",
      "5767\n",
      "5768\n",
      "5769\n",
      "5770\n",
      "5771\n",
      "5772\n",
      "5773\n",
      "5774\n",
      "5775\n",
      "5776\n",
      "5777\n",
      "5778\n",
      "5779\n",
      "5780\n",
      "5781\n",
      "5782\n",
      "5783\n",
      "5784\n",
      "5785\n",
      "5786\n",
      "5787\n",
      "5788\n",
      "5789\n",
      "5790\n",
      "5791\n",
      "5792\n",
      "5793\n",
      "5794\n",
      "5795\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5799\n",
      "5800\n",
      "5801\n",
      "5802\n",
      "5803\n",
      "5804\n",
      "5805\n",
      "5806\n",
      "5807\n",
      "5808\n",
      "5809\n",
      "5810\n",
      "5811\n",
      "5812\n",
      "5813\n",
      "5814\n",
      "5815\n",
      "5816\n",
      "5817\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5821\n",
      "5822\n",
      "5823\n",
      "5824\n",
      "5825\n",
      "5826\n",
      "5827\n",
      "5828\n",
      "5829\n",
      "5830\n",
      "5831\n",
      "5832\n",
      "5833\n",
      "5834\n",
      "5835\n",
      "5836\n",
      "5837\n",
      "5838\n",
      "5839\n",
      "5840\n",
      "5841\n",
      "5842\n",
      "5843\n",
      "5844\n",
      "5845\n",
      "5846\n",
      "5847\n",
      "5848\n",
      "5849\n",
      "5850\n",
      "5851\n",
      "5852\n",
      "5853\n",
      "5854\n",
      "5855\n",
      "5856\n",
      "5857\n",
      "5858\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5862\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5868\n",
      "5869\n",
      "5870\n",
      "5871\n",
      "5872\n",
      "5873\n",
      "5874\n",
      "5875\n",
      "5876\n",
      "5877\n",
      "5878\n",
      "5879\n",
      "5880\n",
      "5881\n",
      "5882\n",
      "5883\n",
      "5884\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5890\n",
      "5891\n",
      "5892\n",
      "5893\n",
      "5894\n",
      "5895\n",
      "5896\n",
      "5897\n",
      "5898\n",
      "5899\n",
      "5900\n",
      "5901\n",
      "5902\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5922\n",
      "5923\n",
      "5924\n",
      "5925\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5931\n",
      "5932\n",
      "5933\n",
      "5934\n",
      "5935\n",
      "5936\n",
      "5937\n",
      "5938\n",
      "5939\n",
      "5940\n",
      "5941\n",
      "5942\n",
      "5943\n",
      "5944\n",
      "5945\n",
      "5946\n",
      "5947\n",
      "5948\n",
      "5949\n",
      "5950\n",
      "5951\n",
      "5952\n",
      "5953\n",
      "5954\n",
      "5955\n",
      "5956\n",
      "5957\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5961\n",
      "5962\n",
      "5963\n",
      "5964\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5969\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5978\n",
      "5979\n",
      "5980\n",
      "5981\n",
      "5982\n",
      "5983\n",
      "5984\n",
      "5985\n",
      "5986\n",
      "5987\n",
      "5988\n",
      "5989\n",
      "5990\n",
      "5991\n",
      "5992\n",
      "5993\n",
      "5994\n",
      "5995\n",
      "5996\n",
      "5997\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6006\n",
      "6007\n",
      "6008\n",
      "6009\n",
      "6010\n",
      "6011\n",
      "6012\n",
      "6013\n",
      "6014\n",
      "6015\n",
      "6016\n",
      "6017\n",
      "6018\n",
      "6019\n",
      "6020\n",
      "6021\n",
      "6022\n",
      "6023\n",
      "6024\n",
      "\n",
      "6627\n",
      "6628\n",
      "6629\n",
      "6630\n",
      "6631\n",
      "6632\n",
      "6633\n",
      "6634\n",
      "6635\n",
      "6636\n",
      "6637\n",
      "6638\n",
      "6639\n",
      "6640\n",
      "6641\n",
      "6642\n",
      "6643\n",
      "6644\n",
      "6645\n",
      "6646\n",
      "6647\n",
      "6648\n",
      "6649\n",
      "6650\n",
      "6651\n",
      "6652\n",
      "6653\n",
      "6654\n",
      "6655\n",
      "6656\n",
      "6657\n",
      "6658\n",
      "6659\n",
      "6660\n",
      "6661\n",
      "6662\n",
      "6663\n",
      "6664\n",
      "6665\n",
      "6666\n",
      "6667\n",
      "6668\n",
      "6669\n",
      "6670\n",
      "6671\n",
      "6672\n",
      "6673\n",
      "6674\n",
      "6675\n",
      "6676\n",
      "6677\n",
      "6678\n",
      "6679\n",
      "6680\n",
      "6681\n",
      "6682\n",
      "6683\n",
      "6684\n",
      "6685\n",
      "6686\n",
      "6687\n",
      "6688\n",
      "6689\n",
      "6690\n",
      "6691\n",
      "6692\n",
      "6693\n",
      "6694\n",
      "6695\n",
      "6696\n",
      "6697\n",
      "6698\n",
      "6699\n",
      "6700\n",
      "6701\n",
      "6702\n",
      "6703\n",
      "6704\n",
      "6705\n",
      "6706\n",
      "6707\n",
      "6708\n",
      "6709\n",
      "6710\n",
      "6711\n",
      "6712\n",
      "6713\n",
      "6714\n",
      "6715\n",
      "6716\n",
      "6717\n",
      "6718\n",
      "6719\n",
      "6720\n",
      "6721\n",
      "6722\n",
      "6723\n",
      "6724\n",
      "6725\n",
      "6726\n",
      "6727\n",
      "6728\n",
      "6729\n",
      "6730\n",
      "6731\n",
      "6732\n",
      "6733\n",
      "6734\n",
      "6735\n",
      "6736\n",
      "6737\n",
      "6738\n",
      "6739\n",
      "6740\n",
      "6741\n",
      "6742\n",
      "6743\n",
      "6744\n",
      "6745\n",
      "6746\n",
      "6747\n",
      "6748\n",
      "6749\n",
      "6750\n",
      "6751\n",
      "6752\n",
      "6753\n",
      "6754\n",
      "6755\n",
      "6756\n",
      "6757\n",
      "6758\n",
      "6759\n",
      "6760\n",
      "6761\n",
      "6762\n",
      "6763\n",
      "6764\n",
      "6765\n",
      "6766\n",
      "6767\n",
      "6768\n",
      "6769\n",
      "6770\n",
      "6771\n",
      "6772\n",
      "6773\n",
      "6774\n",
      "6775\n",
      "6776\n",
      "6777\n",
      "6778\n",
      "6779\n",
      "6780\n",
      "6781\n",
      "6782\n",
      "6783\n",
      "6784\n",
      "6785\n",
      "6786\n",
      "6787\n",
      "6788\n",
      "6789\n",
      "6790\n",
      "6791\n",
      "6792\n",
      "6793\n",
      "6794\n",
      "6795\n",
      "6796\n",
      "6797\n",
      "6798\n",
      "6799\n",
      "6800\n",
      "6801\n",
      "6802\n",
      "6803\n",
      "6804\n",
      "6805\n",
      "6806\n",
      "6807\n",
      "6808\n",
      "6809\n",
      "6810\n",
      "6811\n",
      "6812\n",
      "6813\n",
      "6814\n",
      "6815\n",
      "6816\n",
      "6817\n",
      "6818\n",
      "6819\n",
      "6820\n",
      "6821\n",
      "6822\n",
      "6823\n",
      "6824\n",
      "6825\n",
      "6826\n",
      "6827\n",
      "6828\n",
      "6829\n",
      "6830\n",
      "6831\n",
      "6832\n",
      "6833\n",
      "6834\n",
      "6835\n",
      "6836\n",
      "6837\n",
      "6838\n",
      "6839\n",
      "6840\n",
      "6841\n",
      "6842\n",
      "6843\n",
      "6844\n",
      "6845\n",
      "6846\n",
      "6847\n",
      "6848\n",
      "6849\n",
      "6850\n",
      "6851\n",
      "6852\n",
      "6853\n",
      "6854\n",
      "6855\n",
      "6856\n",
      "6857\n",
      "6858\n",
      "6859\n",
      "6860\n",
      "6861\n",
      "6862\n",
      "6863\n",
      "6864\n",
      "6865\n",
      "6866\n",
      "6867\n",
      "6868\n",
      "6869\n",
      "6870\n",
      "6871\n",
      "6872\n",
      "6873\n",
      "6874\n",
      "6875\n",
      "6876\n",
      "6877\n",
      "6878\n",
      "6879\n",
      "6880\n",
      "6881\n",
      "6882\n",
      "6883\n",
      "6884\n",
      "6885\n",
      "6886\n",
      "6887\n",
      "6888\n",
      "6889\n",
      "6890\n",
      "6891\n",
      "6892\n",
      "6893\n",
      "6894\n",
      "6895\n",
      "6896\n",
      "6897\n",
      "6898\n",
      "6899\n",
      "6900\n",
      "6901\n",
      "6902\n",
      "6903\n",
      "6904\n",
      "6905\n",
      "6906\n",
      "6907\n",
      "6908\n",
      "6909\n",
      "6910\n",
      "6911\n",
      "6912\n",
      "6913\n",
      "6914\n",
      "6915\n",
      "6916\n",
      "6917\n",
      "6918\n",
      "6919\n",
      "6920\n",
      "6921\n",
      "6922\n",
      "6923\n",
      "6924\n",
      "6925\n",
      "6926\n",
      "6927\n",
      "6928\n",
      "6929\n",
      "6930\n",
      "6931\n",
      "6932\n",
      "6933\n",
      "6934\n",
      "6935\n",
      "6936\n",
      "6937\n",
      "6938\n",
      "6939\n",
      "6940\n",
      "6941\n",
      "6942\n",
      "6943\n",
      "6944\n",
      "6945\n",
      "6946\n",
      "6947\n",
      "6948\n",
      "6949\n",
      "6950\n",
      "6951\n",
      "6952\n",
      "6953\n",
      "6954\n",
      "6955\n",
      "6956\n",
      "6957\n",
      "6958\n",
      "6959\n",
      "6960\n",
      "6961\n",
      "6962\n",
      "6963\n",
      "6964\n",
      "6965\n",
      "6966\n",
      "6967\n",
      "6968\n",
      "6969\n",
      "6970\n",
      "6971\n",
      "6972\n",
      "6973\n",
      "6974\n",
      "6975\n",
      "6976\n",
      "6977\n",
      "6978\n",
      "6979\n",
      "6980\n",
      "6981\n",
      "6982\n",
      "6983\n",
      "6984\n",
      "6985\n",
      "6986\n",
      "6987\n",
      "6988\n",
      "6989\n",
      "6990\n",
      "6991\n",
      "6992\n",
      "6993\n",
      "6994\n",
      "6995\n",
      "6996\n",
      "6997\n",
      "6998\n",
      "6999\n",
      "7000\n",
      "7001\n",
      "7002\n",
      "7003\n",
      "7004\n",
      "7005\n",
      "7006\n",
      "7007\n",
      "7008\n",
      "7009\n",
      "7010\n",
      "7011\n",
      "7012\n",
      "7013\n",
      "7014\n",
      "7015\n",
      "7016\n",
      "7017\n",
      "7018\n",
      "7019\n",
      "7020\n",
      "7021\n",
      "7022\n",
      "7023\n",
      "7024\n",
      "7025\n",
      "7026\n",
      "7027\n",
      "7028\n",
      "7029\n",
      "7030\n",
      "7031\n",
      "7032\n",
      "7033\n",
      "7034\n",
      "7035\n",
      "7036\n",
      "7037\n",
      "7038\n",
      "7039\n",
      "7040\n",
      "7041\n",
      "7042\n",
      "7043\n",
      "7044\n",
      "7045\n",
      "7046\n",
      "7047\n",
      "7048\n",
      "7049\n",
      "7050\n",
      "7051\n",
      "7052\n",
      "7053\n",
      "7054\n",
      "7055\n",
      "7056\n",
      "7057\n",
      "7058\n",
      "7059\n",
      "7060\n",
      "7061\n",
      "7062\n",
      "7063\n",
      "7064\n",
      "7065\n",
      "7066\n",
      "7067\n",
      "7068\n",
      "7069\n",
      "7070\n",
      "7071\n",
      "7072\n",
      "7073\n",
      "7074\n",
      "7075\n",
      "7076\n",
      "7077\n",
      "7078\n",
      "7079\n",
      "7080\n",
      "7081\n",
      "7082\n",
      "7083\n",
      "7084\n",
      "7085\n",
      "7086\n",
      "7087\n",
      "7088\n",
      "7089\n",
      "7090\n",
      "7091\n",
      "7092\n",
      "7093\n",
      "7094\n",
      "7095\n",
      "7096\n",
      "7097\n",
      "7098\n",
      "7099\n",
      "7100\n",
      "7101\n",
      "7102\n",
      "7103\n",
      "7104\n",
      "7105\n",
      "7106\n",
      "7107\n",
      "7108\n",
      "7109\n",
      "7110\n",
      "7111\n",
      "7112\n",
      "7113\n",
      "7114\n",
      "7115\n",
      "7116\n",
      "7117\n",
      "7118\n",
      "7119\n",
      "7120\n",
      "7121\n",
      "7122\n",
      "7123\n",
      "7124\n",
      "7125\n",
      "7126\n",
      "7752\n",
      "7753\n",
      "7754\n",
      "7755\n",
      "7756\n",
      "7757\n",
      "7758\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7762\n",
      "7763\n",
      "7764\n",
      "7765\n",
      "7766\n",
      "7767\n",
      "7768\n",
      "7769\n",
      "7770\n",
      "7771\n",
      "7772\n",
      "7773\n",
      "7774\n",
      "7775\n",
      "7776\n",
      "7777\n",
      "7778\n",
      "7779\n",
      "7780\n",
      "7781\n",
      "7782\n",
      "7783\n",
      "7784\n",
      "7785\n",
      "7786\n",
      "7787\n",
      "7788\n",
      "7789\n",
      "7790\n",
      "7791\n",
      "7792\n",
      "7793\n",
      "7794\n",
      "7795\n",
      "7796\n",
      "7797\n",
      "7798\n",
      "7799\n",
      "7800\n",
      "7801\n",
      "7802\n",
      "7803\n",
      "7804\n",
      "7805\n",
      "7806\n",
      "7807\n",
      "7808\n",
      "7809\n",
      "7810\n",
      "7811\n",
      "7812\n",
      "7813\n",
      "7814\n",
      "7815\n",
      "7816\n",
      "7817\n",
      "7818\n",
      "7819\n",
      "7820\n",
      "7821\n",
      "7822\n",
      "7823\n",
      "7824\n",
      "7825\n",
      "7826\n",
      "7827\n",
      "7828\n",
      "7829\n",
      "7830\n",
      "7831\n",
      "7832\n",
      "7833\n",
      "7834\n",
      "7835\n",
      "7836\n",
      "7837\n",
      "7838\n",
      "7839\n",
      "7840\n",
      "7841\n",
      "7842\n",
      "7843\n",
      "7844\n",
      "7845\n",
      "7846\n",
      "7847\n",
      "7848\n",
      "7849\n",
      "7850\n",
      "7851\n",
      "7852\n",
      "7853\n",
      "7854\n",
      "7855\n",
      "7856\n",
      "7857\n",
      "7858\n",
      "7859\n",
      "7860\n",
      "7861\n",
      "7862\n",
      "7863\n",
      "7864\n",
      "7865\n",
      "7866\n",
      "7867\n",
      "7868\n",
      "7869\n",
      "7870\n",
      "7871\n",
      "7872\n",
      "7873\n",
      "7874\n",
      "7875\n",
      "7876\n",
      "7877\n",
      "7878\n",
      "7879\n",
      "7880\n",
      "7881\n",
      "7882\n",
      "7883\n",
      "7884\n",
      "7885\n",
      "7886\n",
      "7887\n",
      "7888\n",
      "7889\n",
      "7890\n",
      "7891\n",
      "7892\n",
      "7893\n",
      "7894\n",
      "7895\n",
      "7896\n",
      "7897\n",
      "7898\n",
      "7899\n",
      "7900\n",
      "7901\n",
      "7902\n",
      "7903\n",
      "7904\n",
      "7905\n",
      "7906\n",
      "7907\n",
      "7908\n",
      "7909\n",
      "7910\n",
      "7911\n",
      "7912\n",
      "7913\n",
      "7914\n",
      "7915\n",
      "7916\n",
      "7917\n",
      "7918\n",
      "7919\n",
      "7920\n",
      "7921\n",
      "7922\n",
      "7923\n",
      "7924\n",
      "7925\n",
      "7926\n",
      "7927\n",
      "7928\n",
      "7929\n",
      "7930\n",
      "7931\n",
      "7932\n",
      "7933\n",
      "7934\n",
      "7935\n",
      "7936\n",
      "7937\n",
      "7938\n",
      "7939\n",
      "7940\n",
      "7941\n",
      "7942\n",
      "7943\n",
      "7944\n",
      "7945\n",
      "7946\n",
      "7947\n",
      "7948\n",
      "7949\n",
      "7950\n",
      "7951\n",
      "7952\n",
      "7953\n",
      "7954\n",
      "7955\n",
      "7956\n",
      "7957\n",
      "7958\n",
      "7959\n",
      "7960\n",
      "7961\n",
      "7962\n",
      "7963\n",
      "7964\n",
      "7965\n",
      "7966\n",
      "7967\n",
      "7968\n",
      "7969\n",
      "7970\n",
      "7971\n",
      "7972\n",
      "7973\n",
      "7974\n",
      "7975\n",
      "7976\n",
      "7977\n",
      "7978\n",
      "7979\n",
      "7980\n",
      "7981\n",
      "7982\n",
      "7983\n",
      "7984\n",
      "7985\n",
      "7986\n",
      "7987\n",
      "7988\n",
      "7989\n",
      "7990\n",
      "7991\n",
      "7992\n",
      "7993\n",
      "7994\n",
      "7995\n",
      "7996\n",
      "7997\n",
      "7998\n",
      "7999\n",
      "8000\n",
      "8001\n",
      "8002\n",
      "8003\n",
      "8004\n",
      "8005\n",
      "8006\n",
      "8007\n",
      "8008\n",
      "8009\n",
      "8010\n",
      "8011\n",
      "8012\n",
      "8013\n",
      "8014\n",
      "8015\n",
      "8016\n",
      "8017\n",
      "8018\n",
      "8019\n",
      "8020\n",
      "8021\n",
      "8022\n",
      "8023\n",
      "8024\n",
      "8025\n",
      "8026\n",
      "8027\n",
      "8028\n",
      "8029\n",
      "8030\n",
      "8031\n",
      "8032\n",
      "8033\n",
      "8034\n",
      "8035\n",
      "8036\n",
      "8037\n",
      "8038\n",
      "8039\n",
      "8040\n",
      "8041\n",
      "8042\n",
      "8043\n",
      "8044\n",
      "8045\n",
      "8046\n",
      "8047\n",
      "8048\n",
      "8049\n",
      "8050\n",
      "8051\n",
      "8052\n",
      "8053\n",
      "8054\n",
      "8055\n",
      "8056\n",
      "8057\n",
      "8058\n",
      "8059\n",
      "8060\n",
      "8061\n",
      "8062\n",
      "8063\n",
      "8064\n",
      "8065\n",
      "8066\n",
      "8067\n",
      "8068\n",
      "8069\n",
      "8070\n",
      "8071\n",
      "8072\n",
      "8073\n",
      "8074\n",
      "8075\n",
      "8076\n",
      "8077\n",
      "8078\n",
      "8079\n",
      "8080\n",
      "8081\n",
      "8082\n",
      "8083\n",
      "8084\n",
      "8085\n",
      "8086\n",
      "8087\n",
      "8088\n",
      "8089\n",
      "8090\n",
      "8091\n",
      "8092\n",
      "8093\n",
      "8094\n",
      "8095\n",
      "8096\n",
      "8097\n",
      "8098\n",
      "8099\n",
      "8100\n",
      "8101\n",
      "8102\n",
      "8103\n",
      "8104\n",
      "8105\n",
      "8106\n",
      "8107\n",
      "8108\n",
      "8109\n",
      "8110\n",
      "8111\n",
      "8112\n",
      "8113\n",
      "8114\n",
      "8115\n",
      "8116\n",
      "8117\n",
      "8118\n",
      "8119\n",
      "8120\n",
      "8121\n",
      "8122\n",
      "8123\n",
      "8124\n",
      "8125\n",
      "8126\n",
      "8127\n",
      "8128\n",
      "8129\n",
      "8130\n",
      "8131\n",
      "8132\n",
      "8133\n",
      "8134\n",
      "8135\n",
      "8136\n",
      "8137\n",
      "8138\n",
      "8139\n",
      "8140\n",
      "8141\n",
      "8142\n",
      "8143\n",
      "8144\n",
      "8145\n",
      "8146\n",
      "8147\n",
      "8148\n",
      "8149\n",
      "8150\n",
      "8151\n",
      "8152\n",
      "8153\n",
      "8154\n",
      "8155\n",
      "8156\n",
      "8157\n",
      "8158\n",
      "8159\n",
      "8160\n",
      "8161\n",
      "8162\n",
      "8163\n",
      "8164\n",
      "8165\n",
      "8166\n",
      "8167\n",
      "8168\n",
      "8169\n",
      "8170\n",
      "8171\n",
      "8172\n",
      "8173\n",
      "8174\n",
      "8175\n",
      "8176\n",
      "8177\n",
      "8178\n",
      "8179\n",
      "8180\n",
      "8181\n",
      "8182\n",
      "8183\n",
      "8184\n",
      "8185\n",
      "8186\n",
      "8187\n",
      "8188\n",
      "8189\n",
      "8190\n",
      "8191\n",
      "8192\n",
      "8193\n",
      "8194\n",
      "8195\n",
      "8196\n",
      "8197\n",
      "8198\n",
      "8199\n",
      "8200\n",
      "8201\n",
      "8202\n",
      "8203\n",
      "8204\n",
      "8205\n",
      "8206\n",
      "8207\n",
      "8208\n",
      "8209\n",
      "8210\n",
      "8211\n",
      "8212\n",
      "8213\n",
      "8214\n",
      "8215\n",
      "8216\n",
      "8217\n",
      "8218\n",
      "8219\n",
      "8220\n",
      "8221\n",
      "8222\n",
      "8223\n",
      "8224\n",
      "8225\n",
      "8226\n",
      "8227\n",
      "8228\n",
      "8229\n",
      "8230\n",
      "8231\n",
      "8232\n",
      "8233\n",
      "8234\n",
      "8235\n",
      "8236\n",
      "8237\n",
      "8238\n",
      "8239\n",
      "8240\n",
      "8241\n",
      "8242\n",
      "8243\n",
      "8244\n",
      "8245\n",
      "8246\n",
      "8247\n",
      "8248\n",
      "8249\n",
      "8250\n",
      "8251\n",
      "8781\n",
      "8782\n",
      "8783\n",
      "8784\n",
      "8785\n",
      "8786\n",
      "8787\n",
      "8788\n",
      "8789\n",
      "8790\n",
      "8791\n",
      "8792\n",
      "8793\n",
      "8794\n",
      "8795\n",
      "8796\n",
      "8797\n",
      "8798\n",
      "8799\n",
      "8800\n",
      "8801\n",
      "8802\n",
      "8803\n",
      "8804\n",
      "8805\n",
      "8806\n",
      "8807\n",
      "8808\n",
      "8809\n",
      "8810\n",
      "8811\n",
      "8812\n",
      "8813\n",
      "8814\n",
      "8815\n",
      "8816\n",
      "8817\n",
      "8818\n",
      "8819\n",
      "8820\n",
      "8821\n",
      "8822\n",
      "8823\n",
      "8824\n",
      "8825\n",
      "8826\n",
      "8827\n",
      "8828\n",
      "8829\n",
      "8830\n",
      "8831\n",
      "8832\n",
      "8833\n",
      "8834\n",
      "8835\n",
      "8836\n",
      "8837\n",
      "8838\n",
      "8839\n",
      "8840\n",
      "8841\n",
      "8842\n",
      "8843\n",
      "8844\n",
      "8845\n",
      "8846\n",
      "8847\n",
      "8848\n",
      "8849\n",
      "8850\n",
      "8851\n",
      "8852\n",
      "8853\n",
      "8854\n",
      "8855\n",
      "8856\n",
      "8857\n",
      "8858\n",
      "8859\n",
      "8860\n",
      "8861\n",
      "8862\n",
      "8863\n",
      "8864\n",
      "8865\n",
      "8866\n",
      "8867\n",
      "8868\n",
      "8869\n",
      "8870\n",
      "8871\n",
      "8872\n",
      "8873\n",
      "8874\n",
      "8875\n",
      "8876\n",
      "8877\n",
      "8878\n",
      "8879\n",
      "8880\n",
      "8881\n",
      "8882\n",
      "8883\n",
      "8884\n",
      "8885\n",
      "8886\n",
      "8887\n",
      "8888\n",
      "8889\n",
      "8890\n",
      "8891\n",
      "8892\n",
      "8893\n",
      "8894\n",
      "8895\n",
      "8896\n",
      "8897\n",
      "8898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10820\n",
      "10821\n",
      "10822\n",
      "10823\n",
      "10824\n",
      "10825\n",
      "10826\n",
      "10827\n",
      "10828\n",
      "10829\n",
      "10830\n",
      "10831\n",
      "10832\n",
      "10833\n",
      "10834\n",
      "10835\n",
      "10836\n",
      "10837\n",
      "10838\n",
      "10839\n",
      "10840\n",
      "10841\n",
      "10842\n",
      "10843\n",
      "10844\n",
      "10845\n",
      "10846\n",
      "10847\n",
      "10848\n",
      "10849\n",
      "10850\n",
      "10851\n",
      "10852\n",
      "10853\n",
      "10854\n",
      "10855\n",
      "10856\n",
      "10857\n",
      "10858\n",
      "10859\n",
      "10860\n",
      "10861\n",
      "10862\n",
      "10863\n",
      "10864\n",
      "10865\n",
      "10866\n",
      "10867\n",
      "10868\n",
      "10869\n",
      "10870\n",
      "10871\n",
      "10872\n",
      "10873\n",
      "10874\n",
      "10875\n",
      "10876\n",
      "10877\n",
      "10878\n",
      "10879\n",
      "10880\n",
      "10881\n",
      "10882\n",
      "10883\n",
      "10884\n",
      "10885\n",
      "10886\n",
      "10887\n",
      "10888\n",
      "10889\n",
      "10890\n",
      "10891\n",
      "10892\n",
      "10893\n",
      "10894\n",
      "10895\n",
      "10896\n",
      "10897\n",
      "10898\n",
      "10899\n",
      "10900\n",
      "10901\n",
      "10902\n",
      "10903\n",
      "10904\n",
      "10905\n",
      "10906\n",
      "10907\n",
      "10908\n",
      "10909\n",
      "10910\n",
      "10911\n",
      "10912\n",
      "10913\n",
      "10914\n",
      "10915\n",
      "10916\n",
      "10917\n",
      "10918\n",
      "10919\n",
      "10920\n",
      "10921\n",
      "10922\n",
      "10923\n",
      "10924\n",
      "10925\n",
      "10926\n",
      "10927\n",
      "10928\n",
      "10929\n",
      "10930\n",
      "10931\n",
      "10932\n",
      "10933\n",
      "10934\n",
      "10935\n",
      "10936\n",
      "10937\n",
      "10938\n",
      "10939\n",
      "10940\n",
      "10941\n",
      "10942\n",
      "10943\n",
      "10944\n",
      "10945\n",
      "10946\n",
      "10947\n",
      "10948\n",
      "10949\n",
      "10950\n",
      "10951\n",
      "10952\n",
      "10953\n",
      "10954\n",
      "10955\n",
      "10956\n",
      "10957\n",
      "10958\n",
      "10959\n",
      "10960\n",
      "10961\n",
      "10962\n",
      "10963\n",
      "10964\n",
      "10965\n",
      "10966\n",
      "10967\n",
      "10968\n",
      "10969\n",
      "10970\n",
      "10971\n",
      "10972\n",
      "10973\n",
      "10974\n",
      "10975\n",
      "10976\n",
      "10977\n",
      "10978\n",
      "10979\n",
      "10980\n",
      "10981\n",
      "10982\n",
      "10983\n",
      "10984\n",
      "10985\n",
      "10986\n",
      "10987\n",
      "10988\n",
      "10989\n",
      "10990\n",
      "10991\n",
      "10992\n",
      "10993\n",
      "10994\n",
      "10995\n",
      "10996\n",
      "10997\n",
      "10998\n",
      "10999\n",
      "11000\n",
      "11001\n",
      "11002\n",
      "11003\n",
      "11004\n",
      "11005\n",
      "11006\n",
      "11007\n",
      "11008\n",
      "11009\n",
      "11010\n",
      "11011\n",
      "11012\n",
      "11013\n",
      "11014\n",
      "11015\n",
      "11016\n",
      "11017\n",
      "11018\n",
      "11019\n",
      "11020\n",
      "11021\n",
      "11022\n",
      "11023\n",
      "11024\n",
      "11025\n",
      "11026\n",
      "11027\n",
      "11028\n",
      "11029\n",
      "11030\n",
      "11031\n",
      "11032\n",
      "11033\n",
      "11034\n",
      "11035\n",
      "11036\n",
      "11037\n",
      "11038\n",
      "11039\n",
      "11040\n",
      "11041\n",
      "11042\n",
      "11043\n",
      "11044\n",
      "11045\n",
      "11046\n",
      "11047\n",
      "11048\n",
      "11049\n",
      "11050\n",
      "11051\n",
      "11052\n",
      "11053\n",
      "11054\n",
      "11055\n",
      "11056\n",
      "11057\n",
      "11058\n",
      "11059\n",
      "11060\n",
      "11061\n",
      "11062\n",
      "11063\n",
      "11064\n",
      "11065\n",
      "11066\n",
      "11067\n",
      "11068\n",
      "11069\n",
      "11070\n",
      "11071\n",
      "11072\n",
      "11073\n",
      "11074\n",
      "11075\n",
      "11076\n",
      "11077\n",
      "11078\n",
      "11079\n",
      "11080\n",
      "11081\n",
      "11082\n",
      "11083\n",
      "11084\n",
      "11085\n",
      "11086\n",
      "11087\n",
      "11088\n",
      "11089\n",
      "11090\n",
      "11091\n",
      "11092\n",
      "11093\n",
      "11094\n",
      "11095\n",
      "11096\n",
      "11097\n",
      "11098\n",
      "11099\n",
      "11100\n",
      "11101\n",
      "11102\n",
      "11103\n",
      "11104\n",
      "11105\n",
      "11106\n",
      "11107\n",
      "11108\n",
      "11109\n",
      "11110\n",
      "11111\n",
      "11112\n",
      "11113\n",
      "11114\n",
      "11115\n",
      "11116\n",
      "11697\n",
      "11698\n",
      "11699\n",
      "11700\n",
      "11701\n",
      "11702\n",
      "11703\n",
      "11704\n",
      "11705\n",
      "11706\n",
      "11707\n",
      "11708\n",
      "11709\n",
      "11710\n",
      "11711\n",
      "11712\n",
      "11713\n",
      "11714\n",
      "11715\n",
      "11716\n",
      "11717\n",
      "11718\n",
      "11719\n",
      "11720\n",
      "11721\n",
      "11722\n",
      "11723\n",
      "11724\n",
      "11725\n",
      "11726\n",
      "11727\n",
      "11728\n",
      "11729\n",
      "11730\n",
      "11731\n",
      "11732\n",
      "11733\n",
      "11734\n",
      "11735\n",
      "11736\n",
      "11737\n",
      "11738\n",
      "11739\n",
      "11740\n",
      "11741\n",
      "11742\n",
      "11743\n",
      "11744\n",
      "11745\n",
      "11746\n",
      "11747\n",
      "11748\n",
      "11749\n",
      "11750\n",
      "11751\n",
      "11752\n",
      "11753\n",
      "11754\n",
      "11755\n",
      "11756\n",
      "11757\n",
      "11758\n",
      "11759\n",
      "11760\n",
      "11761\n",
      "11762\n",
      "11763\n",
      "11764\n",
      "11765\n",
      "11766\n",
      "11767\n",
      "11768\n",
      "11769\n",
      "11770\n",
      "11771\n",
      "11772\n",
      "11773\n",
      "11774\n",
      "11775\n",
      "11776\n",
      "11777\n",
      "11778\n",
      "11779\n",
      "11780\n",
      "11781\n",
      "11782\n",
      "11783\n",
      "11784\n",
      "11785\n",
      "11786\n",
      "11787\n",
      "11788\n",
      "11789\n",
      "11790\n",
      "11791\n",
      "11792\n",
      "11793\n",
      "11794\n",
      "11795\n",
      "11796\n",
      "11797\n",
      "11798\n",
      "11799\n",
      "11800\n",
      "11801\n",
      "11802\n",
      "11803\n",
      "11804\n",
      "11805\n",
      "11806\n",
      "11807\n",
      "11808\n",
      "11809\n",
      "11810\n",
      "11811\n",
      "11812\n",
      "11813\n",
      "11814\n",
      "11815\n",
      "11816\n",
      "11817\n",
      "11818\n",
      "11819\n",
      "11820\n",
      "11821\n",
      "11822\n",
      "11823\n",
      "11824\n",
      "11825\n",
      "11826\n",
      "11827\n",
      "11828\n",
      "11829\n",
      "11830\n",
      "11831\n",
      "11832\n",
      "11833\n",
      "11834\n",
      "11835\n",
      "11836\n",
      "11837\n",
      "11838\n",
      "11839\n",
      "11840\n",
      "11841\n",
      "11842\n",
      "11843\n",
      "11844\n",
      "11845\n",
      "11846\n",
      "11847\n",
      "11848\n",
      "11849\n",
      "11850\n",
      "11851\n",
      "11852\n",
      "11853\n",
      "11854\n",
      "11855\n",
      "11856\n",
      "11857\n",
      "11858\n",
      "11859\n",
      "11860\n",
      "11861\n",
      "11862\n",
      "11863\n",
      "11864\n",
      "11865\n",
      "11866\n",
      "11867\n",
      "11868\n",
      "11869\n",
      "11870\n",
      "11871\n",
      "11872\n",
      "11873\n",
      "11874\n",
      "11875\n",
      "11876\n",
      "11877\n",
      "11878\n",
      "11879\n",
      "11880\n",
      "11881\n",
      "11882\n",
      "11883\n",
      "11884\n",
      "11885\n",
      "11886\n",
      "11887\n",
      "11888\n",
      "11889\n",
      "11890\n",
      "11891\n",
      "11892\n",
      "11893\n",
      "11894\n",
      "11895\n",
      "11896\n",
      "11897\n",
      "11898\n",
      "11899\n",
      "11900\n",
      "11901\n",
      "11902\n",
      "11903\n",
      "11904\n",
      "11905\n",
      "11906\n",
      "11907\n",
      "11908\n",
      "11909\n",
      "11910\n",
      "11911\n",
      "11912\n",
      "11913\n",
      "11914\n",
      "11915\n",
      "11916\n",
      "11917\n",
      "11918\n",
      "11919\n",
      "11920\n",
      "11921\n",
      "11922\n",
      "11923\n",
      "11924\n",
      "11925\n",
      "11926\n",
      "11927\n",
      "11928\n",
      "11929\n",
      "11930\n",
      "11931\n",
      "11932\n",
      "11933\n",
      "11934\n",
      "11935\n",
      "11936\n",
      "11937\n",
      "11938\n",
      "11939\n",
      "11940\n",
      "11941\n",
      "11942\n",
      "11943\n",
      "11944\n",
      "11945\n",
      "11946\n",
      "11947\n",
      "11948\n",
      "11949\n",
      "11950\n",
      "11951\n",
      "11952\n",
      "11953\n",
      "11954\n",
      "11955\n",
      "11956\n",
      "11957\n",
      "11958\n",
      "11959\n",
      "11960\n",
      "11961\n",
      "11962\n",
      "11963\n",
      "11964\n",
      "11965\n",
      "11966\n",
      "11967\n",
      "11968\n",
      "11969\n",
      "11970\n",
      "11971\n",
      "11972\n",
      "11973\n",
      "11974\n",
      "11975\n",
      "11976\n",
      "11977\n",
      "11978\n",
      "11979\n",
      "11980\n",
      "11981\n",
      "11982\n",
      "11983\n",
      "11984\n",
      "11985\n",
      "11986\n",
      "11987\n",
      "11988\n",
      "11989\n",
      "11990\n",
      "11991\n",
      "11992\n",
      "11993\n",
      "11994\n",
      "11995\n",
      "11996\n",
      "11997\n",
      "11998\n",
      "11999\n",
      "12000\n",
      "12001\n",
      "12002\n",
      "12003\n",
      "12004\n",
      "12005\n",
      "12006\n",
      "12007\n",
      "12008\n",
      "12009\n",
      "12010\n",
      "12011\n",
      "12012\n",
      "12013\n",
      "12014\n",
      "12015\n",
      "12016\n",
      "12017\n",
      "12018\n",
      "12019\n",
      "12020\n",
      "12021\n",
      "12022\n",
      "12023\n",
      "12024\n",
      "12025\n",
      "12026\n",
      "12027\n",
      "12028\n",
      "12029\n",
      "12030\n",
      "12031\n",
      "12032\n",
      "12033\n",
      "12034\n",
      "12035\n",
      "12036\n",
      "12037\n",
      "12038\n",
      "12039\n",
      "12040\n",
      "12041\n",
      "12042\n",
      "12043\n",
      "12044\n",
      "12045\n",
      "12046\n",
      "12047\n",
      "12048\n",
      "12049\n",
      "12050\n",
      "12051\n",
      "12052\n",
      "12053\n",
      "12054\n",
      "12055\n",
      "12056\n",
      "12057\n",
      "12058\n",
      "12059\n",
      "12060\n",
      "12061\n",
      "12062\n",
      "12063\n",
      "12064\n",
      "12065\n",
      "12066\n",
      "12067\n",
      "12068\n",
      "12069\n",
      "12070\n",
      "12071\n",
      "12072\n",
      "12073\n",
      "12074\n",
      "12075\n",
      "12076\n",
      "12077\n",
      "12078\n",
      "12079\n",
      "12080\n",
      "12081\n",
      "12082\n",
      "12083\n",
      "12084\n",
      "12085\n",
      "12086\n",
      "12087\n",
      "12088\n",
      "12089\n",
      "12090\n",
      "12091\n",
      "12092\n",
      "12093\n",
      "12094\n",
      "12095\n",
      "12096\n",
      "12097\n",
      "12098\n",
      "12099\n",
      "12100\n",
      "12101\n",
      "12102\n",
      "12103\n",
      "12104\n",
      "12105\n",
      "12106\n",
      "12107\n",
      "12108\n",
      "12109\n",
      "12110\n",
      "12111\n",
      "12112\n",
      "12113\n",
      "12114\n",
      "12115\n",
      "12116\n",
      "12117\n",
      "12118\n",
      "12119\n",
      "12120\n",
      "12121\n",
      "12122\n",
      "12123\n",
      "12124\n",
      "12125\n",
      "12126\n",
      "12127\n",
      "12128\n",
      "12129\n",
      "12130\n",
      "12131\n",
      "12132\n",
      "12133\n",
      "12134\n",
      "12135\n",
      "12136\n",
      "12137\n",
      "12138\n",
      "12139\n",
      "12140\n",
      "12141\n",
      "12142\n",
      "12143\n",
      "12144\n",
      "12145\n",
      "12146\n",
      "12147\n",
      "12148\n",
      "12149\n",
      "12150\n",
      "12151\n",
      "12152\n",
      "12153\n",
      "12154\n",
      "12155\n",
      "12156\n",
      "12157\n",
      "12158\n",
      "12159\n",
      "12160\n",
      "12161\n",
      "12162\n",
      "12163\n",
      "12164\n",
      "12165\n",
      "12166\n",
      "12167\n",
      "12168\n",
      "12169\n",
      "12170\n",
      "12171\n",
      "12172\n",
      "12173\n",
      "12174\n",
      "12175\n",
      "12176\n",
      "12177\n",
      "12178\n",
      "12179\n",
      "12180\n",
      "12181\n",
      "12182\n",
      "12183\n",
      "12184\n",
      "12185\n",
      "12186\n",
      "12187\n",
      "12188\n",
      "12189\n",
      "12190\n",
      "12191\n",
      "12192\n",
      "12193\n",
      "12194\n",
      "12195\n",
      "12196\n",
      "12744\n",
      "12745\n",
      "12746\n",
      "12747\n",
      "12748\n",
      "12749\n",
      "12750\n",
      "12751\n",
      "12752\n",
      "12753\n",
      "12754\n",
      "12755\n",
      "12756\n",
      "12757\n",
      "12758\n",
      "12759\n",
      "12760\n",
      "12761\n",
      "12762\n",
      "12763\n",
      "12764\n",
      "12765\n",
      "12766\n",
      "12767\n",
      "12768\n",
      "12769\n",
      "12770\n",
      "12771\n",
      "12772\n",
      "12773\n",
      "12774\n",
      "12775\n",
      "12776\n",
      "12777\n",
      "12778\n",
      "12779\n",
      "12780\n",
      "12781\n",
      "12782\n",
      "12783\n",
      "12784\n",
      "12785\n",
      "12786\n",
      "12787\n",
      "12788\n",
      "12789\n",
      "12790\n",
      "12791\n",
      "12792\n",
      "12793\n",
      "12794\n",
      "12795\n",
      "12796\n",
      "12797\n",
      "12798\n",
      "12799\n",
      "12800\n",
      "12801\n",
      "12802\n",
      "12803\n",
      "12804\n",
      "12805\n",
      "12806\n",
      "12807\n",
      "12808\n",
      "12809\n",
      "12810\n",
      "12811\n",
      "12812\n",
      "12813\n",
      "12814\n",
      "12815\n",
      "12816\n",
      "12817\n",
      "12818\n",
      "12819\n",
      "12820\n",
      "12821\n",
      "12822\n",
      "12823\n",
      "12824\n",
      "12825\n",
      "12826\n",
      "12827\n",
      "12828\n",
      "12829\n",
      "12830\n",
      "12831\n",
      "12832\n",
      "12833\n",
      "12834\n",
      "12835\n",
      "12836\n",
      "12837\n",
      "12838\n",
      "12839\n",
      "12840\n",
      "12841\n",
      "12842\n",
      "12843\n",
      "12844\n",
      "12845\n",
      "12846\n",
      "12847\n",
      "12848\n",
      "12849\n",
      "12850\n",
      "12851\n",
      "12852\n",
      "12853\n",
      "12854\n",
      "12855\n",
      "12856\n",
      "12857\n",
      "12858\n",
      "12859\n",
      "12860\n",
      "12861\n",
      "12862\n",
      "12863\n",
      "12864\n",
      "12865\n",
      "12866\n",
      "12867\n",
      "12868\n",
      "12869\n",
      "12870\n",
      "12871\n",
      "12872\n",
      "12873\n",
      "12874\n",
      "12875\n",
      "12876\n",
      "12877\n",
      "12878\n",
      "12879\n",
      "12880\n",
      "12881\n",
      "12882\n",
      "12883\n",
      "12884\n",
      "12885\n",
      "12886\n",
      "12887\n",
      "12888\n",
      "12889\n",
      "12890\n",
      "12891\n",
      "12892\n",
      "12893\n",
      "12894\n",
      "12895\n",
      "12896\n",
      "12897\n",
      "12898\n",
      "12899\n",
      "12900\n",
      "12901\n",
      "12902\n",
      "12903\n",
      "12904\n",
      "12905\n",
      "12906\n",
      "12907\n",
      "12908\n",
      "12909\n",
      "12910\n",
      "12911\n",
      "12912\n",
      "12913\n",
      "12914\n",
      "12915\n",
      "12916\n",
      "12917\n",
      "12918\n",
      "12919\n",
      "12920\n",
      "12921\n",
      "12922\n",
      "12923\n",
      "12924\n",
      "12925\n",
      "12926\n",
      "12927\n",
      "12928\n",
      "12929\n",
      "12930\n",
      "12931\n",
      "12932\n",
      "12933\n",
      "12934\n",
      "12935\n",
      "12936\n",
      "12937\n",
      "12938\n",
      "12939\n",
      "12940\n",
      "12941\n",
      "12942\n",
      "12943\n",
      "12944\n",
      "12945\n",
      "12946\n",
      "12947\n",
      "12948\n",
      "12949\n",
      "12950\n",
      "12951\n",
      "12952\n",
      "12953\n",
      "12954\n",
      "12955\n",
      "12956\n",
      "12957\n",
      "12958\n",
      "12959\n",
      "12960\n",
      "12961\n",
      "12962\n",
      "12963\n",
      "12964\n",
      "12965\n",
      "12966\n",
      "12967\n",
      "12968\n",
      "12969\n",
      "12970\n",
      "12971\n",
      "12972\n",
      "12973\n",
      "12974\n",
      "12975\n",
      "12976\n",
      "12977\n",
      "12978\n",
      "12979\n",
      "12980\n",
      "12981\n",
      "12982\n",
      "12983\n",
      "12984\n",
      "12985\n",
      "12986\n",
      "12987\n",
      "12988\n",
      "12989\n",
      "12990\n",
      "12991\n",
      "12992\n",
      "12993\n",
      "12994\n",
      "12995\n",
      "12996\n",
      "12997\n",
      "12998\n",
      "12999\n",
      "13000\n",
      "13001\n",
      "13002\n",
      "13003\n",
      "13004\n",
      "13005\n",
      "13006\n",
      "13007\n",
      "13008\n",
      "13009\n",
      "13010\n",
      "13011\n",
      "13012\n",
      "13013\n",
      "13014\n",
      "13015\n",
      "13016\n",
      "13017\n",
      "13018\n",
      "13019\n",
      "13020\n",
      "13021\n",
      "13022\n",
      "13023\n",
      "13024\n",
      "13025\n",
      "13026\n",
      "13027\n",
      "13028\n",
      "13029\n",
      "13030\n",
      "13031\n",
      "13032\n",
      "13033\n",
      "13034\n",
      "13035\n",
      "13036\n",
      "13037\n",
      "13038\n",
      "13039\n",
      "13040\n",
      "13041\n",
      "13042\n",
      "13043\n",
      "13044\n",
      "13045\n",
      "13046\n",
      "13047\n",
      "13048\n",
      "13049\n",
      "13050\n",
      "13051\n",
      "13052\n",
      "13053\n",
      "13054\n",
      "13055\n",
      "13056\n",
      "13057\n",
      "13058\n",
      "13059\n",
      "13060\n",
      "13061\n",
      "13062\n",
      "13063\n",
      "13064\n",
      "13065\n",
      "13066\n",
      "13067\n",
      "13068\n",
      "13069\n",
      "13070\n",
      "13071\n",
      "13072\n",
      "13073\n",
      "13074\n",
      "13075\n",
      "13076\n",
      "13077\n",
      "13078\n",
      "13079\n",
      "13080\n",
      "13081\n",
      "13082\n",
      "13083\n",
      "13084\n",
      "13085\n",
      "13086\n",
      "13087\n",
      "13088\n",
      "13089\n",
      "13090\n",
      "13091\n",
      "13092\n",
      "13093\n",
      "13094\n",
      "13095\n",
      "13096\n",
      "13097\n",
      "13098\n",
      "13099\n",
      "13100\n",
      "13101\n",
      "13102\n",
      "13103\n",
      "13104\n",
      "13105\n",
      "13106\n",
      "13107\n",
      "13108\n",
      "13109\n",
      "13110\n",
      "13111\n",
      "13112\n",
      "13113\n",
      "13114\n",
      "13115\n",
      "13116\n",
      "13117\n",
      "13118\n",
      "13119\n",
      "13120\n",
      "13121\n",
      "13122\n",
      "13123\n",
      "13124\n",
      "13125\n",
      "13126\n",
      "13127\n",
      "13128\n",
      "13129\n",
      "13130\n",
      "13131\n",
      "13132\n",
      "13133\n",
      "13134\n",
      "13135\n",
      "13136\n",
      "13137\n",
      "13138\n",
      "13139\n",
      "13140\n",
      "13141\n",
      "13142\n",
      "13143\n",
      "13144\n",
      "13145\n",
      "13146\n",
      "13147\n",
      "13148\n",
      "13149\n",
      "13150\n",
      "13151\n",
      "13152\n",
      "13153\n",
      "13154\n",
      "13155\n",
      "13156\n",
      "13157\n",
      "13158\n",
      "13159\n",
      "13160\n",
      "13161\n",
      "13162\n",
      "13163\n",
      "13164\n",
      "13165\n",
      "13166\n",
      "13167\n",
      "13168\n",
      "13169\n",
      "13170\n",
      "13171\n",
      "13172\n",
      "13173\n",
      "13174\n",
      "13175\n",
      "13176\n",
      "13177\n",
      "13178\n",
      "13179\n",
      "13180\n",
      "13181\n",
      "13182\n",
      "13183\n",
      "13184\n",
      "13185\n",
      "13186\n",
      "13187\n",
      "13188\n",
      "13189\n",
      "13190\n",
      "13191\n",
      "13192\n",
      "13193\n",
      "13194\n",
      "13195\n",
      "13196\n",
      "13197\n",
      "13198\n",
      "13199\n",
      "13200\n",
      "13201\n",
      "13202\n",
      "13203\n",
      "13204\n",
      "13205\n",
      "13206\n",
      "13207\n",
      "13208\n",
      "13209\n",
      "13210\n",
      "13211\n",
      "13212\n",
      "13213\n",
      "13214\n",
      "13215\n",
      "13216\n",
      "13217\n",
      "13218\n",
      "13219\n",
      "13220\n",
      "13221\n",
      "13222\n",
      "13223\n",
      "13224\n",
      "13225\n",
      "13226\n",
      "13227\n",
      "13228\n",
      "13229\n",
      "13230\n",
      "13231\n",
      "13232\n",
      "13233\n",
      "13234\n",
      "13235\n",
      "13236\n",
      "13237\n",
      "13238\n",
      "13239\n",
      "13240\n",
      "13241\n",
      "13242\n",
      "13243\n",
      "\n",
      "13752\n",
      "13753\n",
      "13754\n",
      "13755\n",
      "13756\n",
      "13757\n",
      "13758\n",
      "13759\n",
      "13760\n",
      "13761\n",
      "13762\n",
      "13763\n",
      "13764\n",
      "13765\n",
      "13766\n",
      "13767\n",
      "13768\n",
      "13769\n",
      "13770\n",
      "13771\n",
      "13772\n",
      "13773\n",
      "13774\n",
      "13775\n",
      "13776\n",
      "13777\n",
      "13778\n",
      "13779\n",
      "13780\n",
      "13781\n",
      "13782\n",
      "13783\n",
      "13784\n",
      "13785\n",
      "13786\n",
      "13787\n",
      "13788\n",
      "13789\n",
      "13790\n",
      "13791\n",
      "13792\n",
      "13793\n",
      "13794\n",
      "13795\n",
      "13796\n",
      "13797\n",
      "13798\n",
      "13799\n",
      "13800\n",
      "13801\n",
      "13802\n",
      "13803\n",
      "13804\n",
      "13805\n",
      "13806\n",
      "13807\n",
      "13808\n",
      "13809\n",
      "13810\n",
      "13811\n",
      "13812\n",
      "13813\n",
      "13814\n",
      "13815\n",
      "13816\n",
      "13817\n",
      "13818\n",
      "13819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13820\n",
      "13821\n",
      "13822\n",
      "13823\n",
      "13824\n",
      "13825\n",
      "13826\n",
      "13827\n",
      "13828\n",
      "13829\n",
      "13830\n",
      "13831\n",
      "13832\n",
      "13833\n",
      "13834\n",
      "13835\n",
      "13836\n",
      "13837\n",
      "13838\n",
      "13839\n",
      "13840\n",
      "13841\n",
      "13842\n",
      "13843\n",
      "13844\n",
      "13845\n",
      "13846\n",
      "13847\n",
      "13848\n",
      "13849\n",
      "13850\n",
      "13851\n",
      "13852\n",
      "13853\n",
      "13854\n",
      "13855\n",
      "13856\n",
      "13857\n",
      "13858\n",
      "13859\n",
      "13860\n",
      "13861\n",
      "13862\n",
      "13863\n",
      "13864\n",
      "13865\n",
      "13866\n",
      "13867\n",
      "13868\n",
      "13869\n",
      "13870\n",
      "13871\n",
      "13872\n",
      "13873\n",
      "13874\n",
      "13875\n",
      "13876\n",
      "13877\n",
      "13878\n",
      "13879\n",
      "13880\n",
      "13881\n",
      "13882\n",
      "13883\n",
      "13884\n",
      "13885\n",
      "13886\n",
      "13887\n",
      "13888\n",
      "13889\n",
      "13890\n",
      "13891\n",
      "13892\n",
      "13893\n",
      "13894\n",
      "13895\n",
      "13896\n",
      "13897\n",
      "13898\n",
      "13899\n",
      "13900\n",
      "13901\n",
      "13902\n",
      "13903\n",
      "13904\n",
      "13905\n",
      "13906\n",
      "13907\n",
      "13908\n",
      "13909\n",
      "13910\n",
      "13911\n",
      "13912\n",
      "13913\n",
      "13914\n",
      "13915\n",
      "13916\n",
      "13917\n",
      "13918\n",
      "13919\n",
      "13920\n",
      "13921\n",
      "13922\n",
      "13923\n",
      "13924\n",
      "13925\n",
      "13926\n",
      "13927\n",
      "13928\n",
      "13929\n",
      "13930\n",
      "13931\n",
      "13932\n",
      "13933\n",
      "13934\n",
      "13935\n",
      "13936\n",
      "13937\n",
      "13938\n",
      "13939\n",
      "13940\n",
      "13941\n",
      "13942\n",
      "13943\n",
      "13944\n",
      "13945\n",
      "13946\n",
      "13947\n",
      "13948\n",
      "13949\n",
      "13950\n",
      "13951\n",
      "13952\n",
      "13953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15731\n",
      "15732\n",
      "15733\n",
      "15734\n",
      "15735\n",
      "15736\n",
      "15737\n",
      "15738\n",
      "15739\n",
      "15740\n",
      "15741\n",
      "15742\n",
      "15743\n",
      "15744\n",
      "15745\n",
      "15746\n",
      "15747\n",
      "15748\n",
      "15749\n",
      "15750\n",
      "15751\n",
      "15752\n",
      "15753\n",
      "15754\n",
      "15755\n",
      "15756\n",
      "15757\n",
      "15758\n",
      "15759\n",
      "15760\n",
      "15761\n",
      "15762\n",
      "15763\n",
      "15764\n",
      "15765\n",
      "15766\n",
      "15767\n",
      "15768\n",
      "15769\n",
      "15770\n",
      "15771\n",
      "15772\n",
      "15773\n",
      "15774\n",
      "15775\n",
      "15776\n",
      "15777\n",
      "15778\n",
      "15779\n",
      "15780\n",
      "15781\n",
      "15782\n",
      "15783\n",
      "15784\n",
      "15785\n",
      "15786\n",
      "15787\n",
      "15788\n",
      "15789\n",
      "15790\n",
      "15791\n",
      "15792\n",
      "15793\n",
      "15794\n",
      "15795\n",
      "15796\n",
      "15797\n",
      "15798\n",
      "15799\n",
      "15800\n",
      "15801\n",
      "15802\n",
      "15803\n",
      "15804\n",
      "15805\n",
      "15806\n",
      "15807\n",
      "15808\n",
      "15809\n",
      "15810\n",
      "15811\n",
      "15812\n",
      "15813\n",
      "15814\n",
      "15815\n",
      "15816\n",
      "15817\n",
      "15818\n",
      "15819\n",
      "15820\n",
      "15821\n",
      "15822\n",
      "15823\n",
      "15824\n",
      "15825\n",
      "15826\n",
      "15827\n",
      "15828\n",
      "15829\n",
      "15830\n",
      "15831\n",
      "15832\n",
      "15833\n",
      "15834\n",
      "15835\n",
      "15836\n",
      "15837\n",
      "15838\n",
      "15839\n",
      "15840\n",
      "15841\n",
      "15842\n",
      "15843\n",
      "15844\n",
      "15845\n",
      "15846\n",
      "15847\n",
      "15848\n",
      "15849\n",
      "15850\n",
      "15851\n",
      "15852\n",
      "15853\n",
      "15854\n",
      "15855\n",
      "15856\n",
      "15857\n",
      "15858\n",
      "15859\n",
      "15860\n",
      "15861\n",
      "15862\n",
      "15863\n",
      "15864\n",
      "15865\n",
      "15866\n",
      "15867\n",
      "15868\n",
      "15869\n",
      "15870\n",
      "15871\n",
      "15872\n",
      "15873\n",
      "15874\n",
      "15875\n",
      "15876\n",
      "15877\n",
      "15878\n",
      "15879\n",
      "15880\n",
      "15881\n",
      "15882\n",
      "15883\n",
      "15884\n",
      "15885\n",
      "15886\n",
      "15887\n",
      "15888\n",
      "15889\n",
      "15890\n",
      "15891\n",
      "15892\n",
      "15893\n",
      "15894\n",
      "15895\n",
      "15896\n",
      "15897\n",
      "15898\n",
      "15899\n",
      "15900\n",
      "15901\n",
      "15902\n",
      "15903\n",
      "15904\n",
      "15905\n",
      "15906\n",
      "15907\n",
      "15908\n",
      "15909\n",
      "15910\n",
      "15911\n",
      "15912\n",
      "15913\n",
      "15914\n",
      "15915\n",
      "15916\n",
      "15917\n",
      "15918\n",
      "15919\n",
      "15920\n",
      "15921\n",
      "15922\n",
      "15923\n",
      "15924\n",
      "15925\n",
      "15926\n",
      "15927\n",
      "15928\n",
      "15929\n",
      "15930\n",
      "15931\n",
      "15932\n",
      "15933\n",
      "15934\n",
      "15935\n",
      "15936\n",
      "15937\n",
      "15938\n",
      "15939\n",
      "15940\n",
      "15941\n",
      "15942\n",
      "15943\n",
      "15944\n",
      "15945\n",
      "15946\n",
      "15947\n",
      "15948\n",
      "15949\n",
      "15950\n",
      "15951\n",
      "15952\n",
      "15953\n",
      "15954\n",
      "15955\n",
      "15956\n",
      "15957\n",
      "15958\n",
      "15959\n",
      "15960\n",
      "15961\n",
      "15962\n",
      "15963\n",
      "15964\n",
      "15965\n",
      "15966\n",
      "15967\n",
      "15968\n",
      "15969\n",
      "15970\n",
      "15971\n",
      "15972\n",
      "15973\n",
      "15974\n",
      "15975\n",
      "15976\n",
      "15977\n",
      "15978\n",
      "15979\n",
      "15980\n",
      "15981\n",
      "15982\n",
      "15983\n",
      "15984\n",
      "15985\n",
      "15986\n",
      "15987\n",
      "15988\n",
      "15989\n",
      "15990\n",
      "15991\n",
      "15992\n",
      "15993\n",
      "15994\n",
      "15995\n",
      "15996\n",
      "15997\n",
      "15998\n",
      "15999\n",
      "16000\n",
      "16001\n",
      "16002\n",
      "16003\n",
      "16004\n",
      "16005\n",
      "16006\n",
      "16007\n",
      "16008\n",
      "16009\n",
      "16010\n",
      "16011\n",
      "16012\n",
      "16013\n",
      "16014\n",
      "16015\n",
      "16016\n",
      "16017\n",
      "16018\n",
      "16019\n",
      "16020\n",
      "16021\n",
      "16022\n",
      "16023\n",
      "16024\n",
      "16025\n",
      "16026\n",
      "16027\n",
      "16028\n",
      "16029\n",
      "16030\n",
      "16031\n",
      "16032\n",
      "16033\n",
      "16034\n",
      "16035\n",
      "16036\n",
      "16037\n",
      "16038\n",
      "16039\n",
      "16040\n",
      "16041\n",
      "16042\n",
      "16043\n",
      "16044\n",
      "16045\n",
      "16046\n",
      "16047\n",
      "16048\n",
      "16049\n",
      "16050\n",
      "16051\n",
      "16052\n",
      "16053\n",
      "16054\n",
      "16055\n",
      "16056\n",
      "16057\n",
      "16058\n",
      "16059\n",
      "16060\n",
      "16061\n",
      "16062\n",
      "16063\n",
      "16064\n",
      "16065\n",
      "16066\n",
      "16067\n",
      "16068\n",
      "16069\n",
      "16070\n",
      "16071\n",
      "16072\n",
      "16073\n",
      "16074\n",
      "16075\n",
      "16076\n",
      "16077\n",
      "16078\n",
      "16079\n",
      "16080\n",
      "16081\n",
      "16082\n",
      "16083\n",
      "16084\n",
      "16085\n",
      "16086\n",
      "16087\n",
      "16088\n",
      "16089\n",
      "16090\n",
      "16091\n",
      "16092\n",
      "16093\n",
      "16094\n",
      "16095\n",
      "16096\n",
      "16097\n",
      "16098\n",
      "16099\n",
      "16100\n",
      "16101\n",
      "16102\n",
      "16103\n",
      "16104\n",
      "16105\n",
      "16106\n",
      "16107\n",
      "16108\n",
      "16109\n",
      "16110\n",
      "16111\n",
      "16112\n",
      "16113\n",
      "16114\n",
      "16115\n",
      "16116\n",
      "16117\n",
      "16118\n",
      "16119\n",
      "16120\n",
      "16121\n",
      "16122\n",
      "16123\n",
      "16124\n",
      "16125\n",
      "1612616674\n",
      "16675\n",
      "16676\n",
      "16677\n",
      "16678\n",
      "16679\n",
      "16680\n",
      "16681\n",
      "16682\n",
      "16683\n",
      "16684\n",
      "16685\n",
      "16686\n",
      "16687\n",
      "16688\n",
      "16689\n",
      "16690\n",
      "16691\n",
      "16692\n",
      "16693\n",
      "16694\n",
      "16695\n",
      "16696\n",
      "16697\n",
      "16698\n",
      "16699\n",
      "16700\n",
      "16701\n",
      "16702\n",
      "16703\n",
      "16704\n",
      "16705\n",
      "16706\n",
      "16707\n",
      "16708\n",
      "16709\n",
      "16710\n",
      "16711\n",
      "16712\n",
      "16713\n",
      "16714\n",
      "16715\n",
      "16716\n",
      "16717\n",
      "16718\n",
      "16719\n",
      "16720\n",
      "16721\n",
      "16722\n",
      "16723\n",
      "16724\n",
      "16725\n",
      "16726\n",
      "16727\n",
      "16728\n",
      "16729\n",
      "16730\n",
      "16731\n",
      "16732\n",
      "16733\n",
      "16734\n",
      "16735\n",
      "16736\n",
      "16737\n",
      "16738\n",
      "16739\n",
      "16740\n",
      "16741\n",
      "16742\n",
      "16743\n",
      "16744\n",
      "16745\n",
      "16746\n",
      "16747\n",
      "16748\n",
      "16749\n",
      "16750\n",
      "16751\n",
      "16752\n",
      "16753\n",
      "16754\n",
      "16755\n",
      "16756\n",
      "16757\n",
      "16758\n",
      "16759\n",
      "16760\n",
      "16761\n",
      "16762\n",
      "16763\n",
      "16764\n",
      "16765\n",
      "16766\n",
      "16767\n",
      "16768\n",
      "16769\n",
      "16770\n",
      "16771\n",
      "16772\n",
      "16773\n",
      "16774\n",
      "16775\n",
      "16776\n",
      "16777\n",
      "16778\n",
      "16779\n",
      "16780\n",
      "16781\n",
      "16782\n",
      "16783\n",
      "16784\n",
      "16785\n",
      "16786\n",
      "16787\n",
      "16788\n",
      "16789\n",
      "16790\n",
      "16791\n",
      "16792\n",
      "16793\n",
      "16794\n",
      "16795\n",
      "16796\n",
      "16797\n",
      "16798\n",
      "16799\n",
      "16800\n",
      "16801\n",
      "16802\n",
      "16803\n",
      "16804\n",
      "16805\n",
      "16806\n",
      "16807\n",
      "16808\n",
      "16809\n",
      "16810\n",
      "16811\n",
      "16812\n",
      "16813\n",
      "16814\n",
      "16815\n",
      "16816\n",
      "16817\n",
      "16818\n",
      "16819\n",
      "16820\n",
      "16821\n",
      "16822\n",
      "16823\n",
      "16824\n",
      "16825\n",
      "16826\n",
      "16827\n",
      "16828\n",
      "16829\n",
      "16830\n",
      "16831\n",
      "16832\n",
      "16833\n",
      "16834\n",
      "16835\n",
      "16836\n",
      "16837\n",
      "16838\n",
      "16839\n",
      "16840\n",
      "16841\n",
      "16842\n",
      "16843\n",
      "16844\n",
      "16845\n",
      "16846\n",
      "16847\n",
      "16848\n",
      "16849\n",
      "16850\n",
      "16851\n",
      "16852\n",
      "16853\n",
      "16854\n",
      "16855\n",
      "16856\n",
      "16857\n",
      "16858\n",
      "16859\n",
      "16860\n",
      "16861\n",
      "16862\n",
      "16863\n",
      "16864\n",
      "16865\n",
      "16866\n",
      "16867\n",
      "16868\n",
      "16869\n",
      "16870\n",
      "16871\n",
      "16872\n",
      "16873\n",
      "16874\n",
      "16875\n",
      "16876\n",
      "16877\n",
      "16878\n",
      "16879\n",
      "16880\n",
      "16881\n",
      "16882\n",
      "16883\n",
      "16884\n",
      "16885\n",
      "16886\n",
      "16887\n",
      "16888\n",
      "16889\n",
      "16890\n",
      "16891\n",
      "16892\n",
      "16893\n",
      "16894\n",
      "16895\n",
      "16896\n",
      "16897\n",
      "16898\n",
      "16899\n",
      "16900\n",
      "16901\n",
      "16902\n",
      "16903\n",
      "16904\n",
      "16905\n",
      "16906\n",
      "16907\n",
      "16908\n",
      "16909\n",
      "16910\n",
      "16911\n",
      "16912\n",
      "16913\n",
      "16914\n",
      "16915\n",
      "16916\n",
      "16917\n",
      "16918\n",
      "16919\n",
      "16920\n",
      "16921\n",
      "16922\n",
      "16923\n",
      "16924\n",
      "16925\n",
      "16926\n",
      "16927\n",
      "16928\n",
      "16929\n",
      "16930\n",
      "16931\n",
      "16932\n",
      "16933\n",
      "16934\n",
      "16935\n",
      "16936\n",
      "16937\n",
      "16938\n",
      "16939\n",
      "16940\n",
      "16941\n",
      "16942\n",
      "16943\n",
      "16944\n",
      "16945\n",
      "16946\n",
      "16947\n",
      "16948\n",
      "16949\n",
      "16950\n",
      "16951\n",
      "16952\n",
      "16953\n",
      "16954\n",
      "16955\n",
      "16956\n",
      "16957\n",
      "16958\n",
      "16959\n",
      "16960\n",
      "16961\n",
      "16962\n",
      "16963\n",
      "16964\n",
      "16965\n",
      "16966\n",
      "16967\n",
      "16968\n",
      "16969\n",
      "16970\n",
      "16971\n",
      "16972\n",
      "16973\n",
      "16974\n",
      "16975\n",
      "16976\n",
      "16977\n",
      "16978\n",
      "16979\n",
      "16980\n",
      "16981\n",
      "16982\n",
      "16983\n",
      "16984\n",
      "16985\n",
      "16986\n",
      "16987\n",
      "16988\n",
      "16989\n",
      "16990\n",
      "16991\n",
      "16992\n",
      "16993\n",
      "16994\n",
      "16995\n",
      "16996\n",
      "16997\n",
      "16998\n",
      "16999\n",
      "17000\n",
      "17001\n",
      "17002\n",
      "17003\n",
      "17004\n",
      "17005\n",
      "17006\n",
      "17007\n",
      "17008\n",
      "17009\n",
      "17010\n",
      "17011\n",
      "17012\n",
      "17013\n",
      "17014\n",
      "17015\n",
      "17016\n",
      "17017\n",
      "17018\n",
      "17019\n",
      "17020\n",
      "17021\n",
      "17022\n",
      "17023\n",
      "17024\n",
      "17025\n",
      "17026\n",
      "17027\n",
      "17028\n",
      "17029\n",
      "17030\n",
      "17031\n",
      "17032\n",
      "17033\n",
      "17034\n",
      "17035\n",
      "17036\n",
      "17037\n",
      "17038\n",
      "17039\n",
      "17040\n",
      "17041\n",
      "17042\n",
      "17043\n",
      "17044\n",
      "17045\n",
      "17046\n",
      "17047\n",
      "17048\n",
      "17049\n",
      "17050\n",
      "17051\n",
      "17052\n",
      "17053\n",
      "17054\n",
      "17055\n",
      "17056\n",
      "17057\n",
      "17058\n",
      "17059\n",
      "17060\n",
      "17061\n",
      "17062\n",
      "17063\n",
      "17064\n",
      "17065\n",
      "17066\n",
      "17067\n",
      "17068\n",
      "17069\n",
      "17070\n",
      "17071\n",
      "17072\n",
      "17073\n",
      "17074\n",
      "17075\n",
      "17076\n",
      "17077\n",
      "17078\n",
      "17079\n",
      "17080\n",
      "17081\n",
      "17082\n",
      "17083\n",
      "17084\n",
      "17085\n",
      "17086\n",
      "17087\n",
      "17088\n",
      "17089\n",
      "17090\n",
      "17091\n",
      "17092\n",
      "17093\n",
      "17094\n",
      "17095\n",
      "17096\n",
      "17097\n",
      "17098\n",
      "17099\n",
      "17100\n",
      "17101\n",
      "17102\n",
      "17103\n",
      "17104\n",
      "17105\n",
      "17106\n",
      "17107\n",
      "17108\n",
      "17109\n",
      "17110\n",
      "17111\n",
      "17112\n",
      "17113\n",
      "17114\n",
      "17115\n",
      "17116\n",
      "17117\n",
      "17118\n",
      "17119\n",
      "17120\n",
      "17121\n",
      "17122\n",
      "17123\n",
      "17124\n",
      "17125\n",
      "17126\n",
      "17127\n",
      "17128\n",
      "17129\n",
      "17130\n",
      "17131\n",
      "17132\n",
      "17133\n",
      "17134\n",
      "17135\n",
      "17136\n",
      "17137\n",
      "17138\n",
      "17139\n",
      "17140\n",
      "17141\n",
      "17142\n",
      "17143\n",
      "17144\n",
      "17145\n",
      "17146\n",
      "17147\n",
      "17148\n",
      "17149\n",
      "17150\n",
      "17151\n",
      "17152\n",
      "17153\n",
      "17154\n",
      "17155\n",
      "17156\n",
      "17157\n",
      "17158\n",
      "17159\n",
      "17160\n",
      "17161\n",
      "17162\n",
      "17163\n",
      "17164\n",
      "17165\n",
      "17166\n",
      "17167\n",
      "17168\n",
      "17169\n",
      "17170\n",
      "17171\n",
      "17172\n",
      "17173\n",
      "\n",
      "17752\n",
      "17753\n",
      "17754\n",
      "17755\n",
      "17756\n",
      "17757\n",
      "17758\n",
      "17759\n",
      "17760\n",
      "17761\n",
      "17762\n",
      "17763\n",
      "17764\n",
      "17765\n",
      "17766\n",
      "17767\n",
      "17768\n",
      "17769\n",
      "17770\n",
      "17771\n",
      "17772\n",
      "17773\n",
      "17774\n",
      "17775\n",
      "17776\n",
      "17777\n",
      "17778\n",
      "17779\n",
      "17780\n",
      "17781\n",
      "17782\n",
      "17783\n",
      "17784\n",
      "17785\n",
      "17786\n",
      "17787\n",
      "17788\n",
      "17789\n",
      "17790\n",
      "17791\n",
      "17792\n",
      "17793\n",
      "17794\n",
      "17795\n",
      "17796\n",
      "17797\n",
      "17798\n",
      "17799\n",
      "17800\n",
      "17801\n",
      "17802\n",
      "17803\n",
      "17804\n",
      "17805\n",
      "17806\n",
      "17807\n",
      "17808\n",
      "17809\n",
      "17810\n",
      "17811\n",
      "17812\n",
      "17813\n",
      "17814\n",
      "17815\n",
      "17816\n",
      "17817\n",
      "17818\n",
      "17819\n",
      "17820\n",
      "17821\n",
      "17822\n",
      "17823\n",
      "17824\n",
      "17825\n",
      "17826\n",
      "17827\n",
      "17828\n",
      "17829\n",
      "17830\n",
      "17831\n",
      "17832\n",
      "17833\n",
      "17834\n",
      "17835\n",
      "17836\n",
      "17837\n",
      "17838\n",
      "17839\n",
      "17840\n",
      "17841\n",
      "17842\n",
      "17843\n",
      "17844\n",
      "17845\n",
      "17846\n",
      "17847\n",
      "17848\n",
      "17849\n",
      "17850\n",
      "17851\n",
      "17852\n",
      "17853\n",
      "17854\n",
      "17855\n",
      "17856\n",
      "17857\n",
      "17858\n",
      "17859\n",
      "17860\n",
      "17861\n",
      "17862\n",
      "17863\n",
      "17864\n",
      "17865\n",
      "17866\n",
      "17867\n",
      "17868\n",
      "17869\n",
      "17870\n",
      "17871\n",
      "17872\n",
      "17873\n",
      "17874\n",
      "17875\n",
      "17876\n",
      "17877\n",
      "17878\n",
      "17879\n",
      "17880\n",
      "17881\n",
      "17882\n",
      "17883\n",
      "17884\n",
      "17885\n",
      "17886\n",
      "17887\n",
      "17888\n",
      "17889\n",
      "17890\n",
      "17891\n",
      "17892\n",
      "17893\n",
      "17894\n",
      "17895\n",
      "17896\n",
      "17897\n",
      "17898\n",
      "17899\n",
      "17900\n",
      "17901\n",
      "17902\n",
      "17903\n",
      "17904\n",
      "17905\n",
      "17906\n",
      "17907\n",
      "17908\n",
      "17909\n",
      "17910\n",
      "17911\n",
      "17912\n",
      "17913\n",
      "17914\n",
      "17915\n",
      "17916\n",
      "17917\n",
      "17918\n",
      "17919\n",
      "17920\n",
      "17921\n",
      "17922\n",
      "17923\n",
      "17924\n",
      "17925\n",
      "17926\n",
      "17927\n",
      "17928\n",
      "17929\n",
      "17930\n",
      "17931\n",
      "17932\n",
      "17933\n",
      "17934\n",
      "17935\n",
      "17936\n",
      "17937\n",
      "17938\n",
      "17939\n",
      "17940\n",
      "17941\n",
      "17942\n",
      "17943\n",
      "17944\n",
      "17945\n",
      "17946\n",
      "17947\n",
      "17948\n",
      "17949\n",
      "17950\n",
      "17951\n",
      "17952\n",
      "17953\n",
      "17954\n",
      "17955\n",
      "17956\n",
      "17957\n",
      "17958\n",
      "17959\n",
      "17960\n",
      "17961\n",
      "17962\n",
      "17963\n",
      "17964\n",
      "17965\n",
      "17966\n",
      "17967\n",
      "17968\n",
      "17969\n",
      "17970\n",
      "17971\n",
      "17972\n",
      "17973\n",
      "17974\n",
      "17975\n",
      "17976\n",
      "17977\n",
      "17978\n",
      "17979\n",
      "17980\n",
      "17981\n",
      "17982\n",
      "17983\n",
      "17984\n",
      "17985\n",
      "17986\n",
      "17987\n",
      "17988\n",
      "17989\n",
      "17990\n",
      "17991\n",
      "17992\n",
      "17993\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26707fa6b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for J in range(3,17994):\n",
    "    print(J)\n",
    "    if PRED[J-3]==PRED[J-2] and PRED[J-2]==PRED[J-1] and PRED[J-1]==PRED[J+1] and PRED[J+1]==PRED[J+2] and PRED[J+2]==PRED[J+3]:\n",
    "        PRED[J]=PRED[J-1]\n",
    "plt.plot(PRED,'+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('a.txt',PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.load('a.txt.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17997, 1)\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比如V1.0版本，这里把resnet18改成了resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapmaxmin(x):\n",
    "    MIN=np.min(x)\n",
    "    MAX=np.max(x)\n",
    "    return (x-MIN)/(MAX-MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
