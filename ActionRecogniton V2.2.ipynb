{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、import some package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils,plot_model\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mping\n",
    "import numpy as np\n",
    "import resnet\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from skimage import data, exposure, img_as_float\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、读取图片进入X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets2//A1.jpg\n",
      "datasets2//A2.jpg\n",
      "datasets2//A3.jpg\n",
      "datasets2//A4.jpg\n",
      "datasets2//A5.jpg\n",
      "datasets2//A6.jpg\n",
      "datasets2//A7.jpg\n",
      "datasets2//A8.jpg\n",
      "datasets2//A9.jpg\n",
      "datasets2//A10.jpg\n",
      "datasets2//A11.jpg\n",
      "datasets2//A12.jpg\n",
      "datasets2//A13.jpg\n",
      "datasets2//A14.jpg\n",
      "datasets2//A15.jpg\n",
      "datasets2//A16.jpg\n",
      "datasets2//A17.jpg\n",
      "datasets2//A18.jpg\n",
      "datasets2//A19.jpg\n",
      "datasets2//A20.jpg\n",
      "datasets2//A21.jpg\n",
      "datasets2//A22.jpg\n",
      "datasets2//A23.jpg\n",
      "datasets2//A24.jpg\n",
      "datasets2//A25.jpg\n",
      "datasets2//B1.jpg\n",
      "datasets2//B2.jpg\n",
      "datasets2//B3.jpg\n",
      "datasets2//B4.jpg\n",
      "datasets2//B5.jpg\n",
      "datasets2//B6.jpg\n",
      "datasets2//B7.jpg\n",
      "datasets2//B8.jpg\n",
      "datasets2//B9.jpg\n",
      "datasets2//B10.jpg\n",
      "datasets2//B11.jpg\n",
      "datasets2//B12.jpg\n",
      "datasets2//B13.jpg\n",
      "datasets2//B14.jpg\n",
      "datasets2//B15.jpg\n",
      "datasets2//B16.jpg\n",
      "datasets2//B17.jpg\n",
      "datasets2//B18.jpg\n",
      "datasets2//B19.jpg\n",
      "datasets2//B20.jpg\n",
      "datasets2//B21.jpg\n",
      "datasets2//B22.jpg\n",
      "datasets2//B23.jpg\n",
      "datasets2//B24.jpg\n",
      "datasets2//B25.jpg\n",
      "datasets2//C1.jpg\n",
      "datasets2//C2.jpg\n",
      "datasets2//C3.jpg\n",
      "datasets2//C4.jpg\n",
      "datasets2//C5.jpg\n",
      "datasets2//C6.jpg\n",
      "datasets2//C7.jpg\n",
      "datasets2//C8.jpg\n",
      "datasets2//C9.jpg\n",
      "datasets2//C10.jpg\n",
      "datasets2//C11.jpg\n",
      "datasets2//C12.jpg\n",
      "datasets2//C13.jpg\n",
      "datasets2//C14.jpg\n",
      "datasets2//C15.jpg\n",
      "datasets2//C16.jpg\n",
      "datasets2//C17.jpg\n",
      "datasets2//C18.jpg\n",
      "datasets2//C19.jpg\n",
      "datasets2//C20.jpg\n",
      "datasets2//C21.jpg\n",
      "datasets2//C22.jpg\n",
      "datasets2//C23.jpg\n",
      "datasets2//C24.jpg\n",
      "datasets2//C25.jpg\n",
      "datasets2//D1.jpg\n",
      "datasets2//D2.jpg\n",
      "datasets2//D3.jpg\n",
      "datasets2//D4.jpg\n",
      "datasets2//D5.jpg\n",
      "datasets2//D6.jpg\n",
      "datasets2//D7.jpg\n",
      "datasets2//D8.jpg\n",
      "datasets2//D9.jpg\n",
      "datasets2//D10.jpg\n",
      "datasets2//D11.jpg\n",
      "datasets2//D12.jpg\n",
      "datasets2//D13.jpg\n",
      "datasets2//D14.jpg\n",
      "datasets2//D15.jpg\n",
      "datasets2//D16.jpg\n",
      "datasets2//D17.jpg\n",
      "datasets2//D18.jpg\n",
      "datasets2//D19.jpg\n",
      "datasets2//D20.jpg\n",
      "datasets2//D21.jpg\n",
      "datasets2//D22.jpg\n",
      "datasets2//D23.jpg\n",
      "datasets2//D24.jpg\n",
      "datasets2//D25.jpg\n",
      "datasets2//E1.jpg\n",
      "datasets2//E2.jpg\n",
      "datasets2//E3.jpg\n",
      "datasets2//E4.jpg\n",
      "datasets2//E5.jpg\n",
      "datasets2//E6.jpg\n",
      "datasets2//E7.jpg\n",
      "datasets2//E8.jpg\n",
      "datasets2//E9.jpg\n",
      "datasets2//E10.jpg\n",
      "datasets2//E11.jpg\n",
      "datasets2//E12.jpg\n",
      "datasets2//E13.jpg\n",
      "datasets2//E14.jpg\n",
      "datasets2//E15.jpg\n",
      "datasets2//E16.jpg\n",
      "datasets2//E17.jpg\n",
      "datasets2//E18.jpg\n",
      "datasets2//E19.jpg\n",
      "datasets2//E20.jpg\n",
      "datasets2//E21.jpg\n",
      "datasets2//E22.jpg\n",
      "datasets2//E23.jpg\n",
      "datasets2//E24.jpg\n",
      "datasets2//E25.jpg\n",
      "datasets2//F1.jpg\n",
      "datasets2//F2.jpg\n",
      "datasets2//F3.jpg\n",
      "datasets2//F4.jpg\n",
      "datasets2//F5.jpg\n",
      "datasets2//F6.jpg\n",
      "datasets2//F7.jpg\n",
      "datasets2//F8.jpg\n",
      "datasets2//F9.jpg\n",
      "datasets2//F10.jpg\n",
      "datasets2//F11.jpg\n",
      "datasets2//F12.jpg\n",
      "datasets2//F13.jpg\n",
      "datasets2//F14.jpg\n",
      "datasets2//F15.jpg\n",
      "datasets2//F16.jpg\n",
      "datasets2//F17.jpg\n",
      "datasets2//F18.jpg\n",
      "datasets2//F19.jpg\n",
      "datasets2//F20.jpg\n",
      "datasets2//F21.jpg\n",
      "datasets2//F22.jpg\n",
      "datasets2//F23.jpg\n",
      "datasets2//F24.jpg\n",
      "datasets2//F25.jpg\n",
      "datasets2//G1.jpg\n",
      "datasets2//G2.jpg\n",
      "datasets2//G3.jpg\n",
      "datasets2//G4.jpg\n",
      "datasets2//G5.jpg\n",
      "datasets2//G6.jpg\n",
      "datasets2//G7.jpg\n",
      "datasets2//G8.jpg\n",
      "datasets2//G9.jpg\n",
      "datasets2//G10.jpg\n",
      "datasets2//G11.jpg\n",
      "datasets2//G12.jpg\n",
      "datasets2//G13.jpg\n",
      "datasets2//G14.jpg\n",
      "datasets2//G15.jpg\n",
      "datasets2//G16.jpg\n",
      "datasets2//G17.jpg\n",
      "datasets2//G18.jpg\n",
      "datasets2//G19.jpg\n",
      "datasets2//G20.jpg\n",
      "datasets2//G21.jpg\n",
      "datasets2//G22.jpg\n",
      "datasets2//G23.jpg\n",
      "datasets2//G24.jpg\n",
      "datasets2//G25.jpg\n",
      "datasets2//H1.jpg\n",
      "datasets2//H2.jpg\n",
      "datasets2//H3.jpg\n",
      "datasets2//H4.jpg\n",
      "datasets2//H5.jpg\n",
      "datasets2//H6.jpg\n",
      "datasets2//H7.jpg\n",
      "datasets2//H8.jpg\n",
      "datasets2//H9.jpg\n",
      "datasets2//H10.jpg\n",
      "datasets2//H11.jpg\n",
      "datasets2//H12.jpg\n",
      "datasets2//H13.jpg\n",
      "datasets2//H14.jpg\n",
      "datasets2//H15.jpg\n",
      "datasets2//H16.jpg\n",
      "datasets2//H17.jpg\n",
      "datasets2//H18.jpg\n",
      "datasets2//H19.jpg\n",
      "datasets2//H20.jpg\n",
      "datasets2//H21.jpg\n",
      "datasets2//H22.jpg\n",
      "datasets2//H23.jpg\n",
      "datasets2//H24.jpg\n",
      "datasets2//H25.jpg\n",
      "datasets2//J1.jpg\n",
      "datasets2//J2.jpg\n",
      "datasets2//J3.jpg\n",
      "datasets2//J4.jpg\n",
      "datasets2//J5.jpg\n",
      "datasets2//J6.jpg\n",
      "datasets2//J7.jpg\n",
      "datasets2//J8.jpg\n",
      "datasets2//J9.jpg\n",
      "datasets2//J10.jpg\n",
      "datasets2//J11.jpg\n",
      "datasets2//J12.jpg\n",
      "datasets2//J13.jpg\n",
      "datasets2//J14.jpg\n",
      "datasets2//J15.jpg\n",
      "datasets2//J16.jpg\n",
      "datasets2//J17.jpg\n",
      "datasets2//J18.jpg\n",
      "datasets2//J19.jpg\n",
      "datasets2//J20.jpg\n",
      "datasets2//J21.jpg\n",
      "datasets2//J22.jpg\n",
      "datasets2//J23.jpg\n",
      "datasets2//J24.jpg\n",
      "datasets2//J25.jpg\n",
      "datasets2//K1.jpg\n",
      "datasets2//K2.jpg\n",
      "datasets2//K3.jpg\n",
      "datasets2//K4.jpg\n",
      "datasets2//K5.jpg\n",
      "datasets2//K6.jpg\n",
      "datasets2//K7.jpg\n",
      "datasets2//K8.jpg\n",
      "datasets2//K9.jpg\n",
      "datasets2//K10.jpg\n",
      "datasets2//K11.jpg\n",
      "datasets2//K12.jpg\n",
      "datasets2//K13.jpg\n",
      "datasets2//K14.jpg\n",
      "datasets2//K15.jpg\n",
      "datasets2//K16.jpg\n",
      "datasets2//K17.jpg\n",
      "datasets2//K18.jpg\n",
      "datasets2//K19.jpg\n",
      "datasets2//K20.jpg\n",
      "datasets2//K21.jpg\n",
      "datasets2//K22.jpg\n",
      "datasets2//K23.jpg\n",
      "datasets2//K24.jpg\n",
      "datasets2//K25.jpg\n",
      "datasets2//L1.jpg\n",
      "datasets2//L2.jpg\n",
      "datasets2//L3.jpg\n",
      "datasets2//L4.jpg\n",
      "datasets2//L5.jpg\n",
      "datasets2//L6.jpg\n",
      "datasets2//L7.jpg\n",
      "datasets2//L8.jpg\n",
      "datasets2//L9.jpg\n",
      "datasets2//L10.jpg\n",
      "datasets2//L11.jpg\n",
      "datasets2//L12.jpg\n",
      "datasets2//L13.jpg\n",
      "datasets2//L14.jpg\n",
      "datasets2//L15.jpg\n",
      "datasets2//L16.jpg\n",
      "datasets2//L17.jpg\n",
      "datasets2//L18.jpg\n",
      "datasets2//L19.jpg\n",
      "datasets2//L20.jpg\n",
      "datasets2//L21.jpg\n",
      "datasets2//L22.jpg\n",
      "datasets2//L23.jpg\n",
      "datasets2//L24.jpg\n",
      "datasets2//L25.jpg\n",
      "datasets2//M1.jpg\n",
      "datasets2//M2.jpg\n",
      "datasets2//M3.jpg\n",
      "datasets2//M4.jpg\n",
      "datasets2//M5.jpg\n",
      "datasets2//M6.jpg\n",
      "datasets2//M7.jpg\n",
      "datasets2//M8.jpg\n",
      "datasets2//M9.jpg\n",
      "datasets2//M10.jpg\n",
      "datasets2//M11.jpg\n",
      "datasets2//M12.jpg\n",
      "datasets2//M13.jpg\n",
      "datasets2//M14.jpg\n",
      "datasets2//M15.jpg\n",
      "datasets2//M16.jpg\n",
      "datasets2//M17.jpg\n",
      "datasets2//M18.jpg\n",
      "datasets2//M19.jpg\n",
      "datasets2//M20.jpg\n",
      "datasets2//M21.jpg\n",
      "datasets2//M22.jpg\n",
      "datasets2//M23.jpg\n",
      "datasets2//M24.jpg\n",
      "datasets2//M25.jpg\n",
      "datasets2//N1.jpg\n",
      "datasets2//N2.jpg\n",
      "datasets2//N3.jpg\n",
      "datasets2//N4.jpg\n",
      "datasets2//N5.jpg\n",
      "datasets2//N6.jpg\n",
      "datasets2//N7.jpg\n",
      "datasets2//N8.jpg\n",
      "datasets2//N9.jpg\n",
      "datasets2//N10.jpg\n",
      "datasets2//N11.jpg\n",
      "datasets2//N12.jpg\n",
      "datasets2//N13.jpg\n",
      "datasets2//N14.jpg\n",
      "datasets2//N15.jpg\n",
      "datasets2//N16.jpg\n",
      "datasets2//N17.jpg\n",
      "datasets2//N18.jpg\n",
      "datasets2//N19.jpg\n",
      "datasets2//N20.jpg\n",
      "datasets2//N21.jpg\n",
      "datasets2//N22.jpg\n",
      "datasets2//N23.jpg\n",
      "datasets2//N24.jpg\n",
      "datasets2//N25.jpg\n",
      "datasets2//O1.jpg\n",
      "datasets2//O2.jpg\n",
      "datasets2//O3.jpg\n",
      "datasets2//O4.jpg\n",
      "datasets2//O5.jpg\n",
      "datasets2//O6.jpg\n",
      "datasets2//O7.jpg\n",
      "datasets2//O8.jpg\n",
      "datasets2//O9.jpg\n",
      "datasets2//O10.jpg\n",
      "datasets2//O11.jpg\n",
      "datasets2//O12.jpg\n",
      "datasets2//O13.jpg\n",
      "datasets2//O14.jpg\n",
      "datasets2//O15.jpg\n",
      "datasets2//O16.jpg\n",
      "datasets2//O17.jpg\n",
      "datasets2//O18.jpg\n",
      "datasets2//O19.jpg\n",
      "datasets2//O20.jpg\n",
      "datasets2//O21.jpg\n",
      "datasets2//O22.jpg\n",
      "datasets2//O23.jpg\n",
      "datasets2//O24.jpg\n",
      "datasets2//O25.jpg\n",
      "datasets2//P1.jpg\n",
      "datasets2//P2.jpg\n",
      "datasets2//P3.jpg\n",
      "datasets2//P4.jpg\n",
      "datasets2//P5.jpg\n",
      "datasets2//P6.jpg\n",
      "datasets2//P7.jpg\n",
      "datasets2//P8.jpg\n",
      "datasets2//P9.jpg\n",
      "datasets2//P10.jpg\n",
      "datasets2//P11.jpg\n",
      "datasets2//P12.jpg\n",
      "datasets2//P13.jpg\n",
      "datasets2//P14.jpg\n",
      "datasets2//P15.jpg\n",
      "datasets2//P16.jpg\n",
      "datasets2//P17.jpg\n",
      "datasets2//P18.jpg\n",
      "datasets2//P19.jpg\n",
      "datasets2//P20.jpg\n",
      "datasets2//P21.jpg\n",
      "datasets2//P22.jpg\n",
      "datasets2//P23.jpg\n",
      "datasets2//P24.jpg\n",
      "datasets2//P25.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26a2e13a470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "A=60\n",
    "B=58\n",
    "C=55\n",
    "D=62\n",
    "E=60\n",
    "F=71\n",
    "G=60\n",
    "H=55\n",
    "J=67\n",
    "K=52\n",
    "L=70\n",
    "M=61\n",
    "N=66\n",
    "O=28\n",
    "P=37\n",
    "\n",
    "T=25\n",
    "A=T\n",
    "B=T\n",
    "C=T\n",
    "D=T\n",
    "E=T\n",
    "F=T\n",
    "G=T\n",
    "H=T\n",
    "J=T\n",
    "K=T\n",
    "L=T\n",
    "M=T\n",
    "N=T\n",
    "O=T\n",
    "P=T\n",
    "\n",
    "resize_rate=2\n",
    "nb_classes = 15\n",
    "dataset_num=A+B+C+D+E+F+G+H+J+K+L+M+N+O+P\n",
    "input_img_rows=2448\n",
    "input_img_cols=3264\n",
    "img_rows=256\n",
    "img_cols=256\n",
    "img_channels=3\n",
    "X=np.zeros([dataset_num,img_rows,img_cols,img_channels])\n",
    "Y=np.zeros([dataset_num,nb_classes])\n",
    "\n",
    "class_num=0\n",
    "\n",
    "for i in range(A):\n",
    "    imagepath=r'datasets2//A'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[i,:,:,:]=image\n",
    "    Y[i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(B):\n",
    "    imagepath=r'datasets2//B'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+i,:,:,:]=image\n",
    "    Y[A+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(C):\n",
    "    imagepath=r'datasets2//C'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+i,:,:,:]=image\n",
    "    Y[A+B+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(D):\n",
    "    imagepath=r'datasets2//D'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+i,:,:,:]=image\n",
    "    Y[A+B+C+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(E):\n",
    "    imagepath=r'datasets2//E'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+i,:,:,:]=image\n",
    "    Y[A+B+C+D+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(F):\n",
    "    imagepath=r'datasets2//F'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(G):\n",
    "    imagepath=r'datasets2//G'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(H):\n",
    "    imagepath=r'datasets2//H'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(J):\n",
    "    imagepath=r'datasets2//J'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+H+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+H+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(K):\n",
    "    imagepath=r'datasets2//K'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+H+J+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+H+J+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(L):\n",
    "    imagepath=r'datasets2//L'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+H+J+K+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+H+J+K+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(M):\n",
    "    imagepath=r'datasets2//M'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+H+J+K+L+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+H+J+K+L+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(N):\n",
    "    imagepath=r'datasets2//N'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+H+J+K+L+M+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+H+J+K+L+M+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(O):\n",
    "    imagepath=r'datasets2//O'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+H+J+K+L+M+N+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+H+J+K+L+M+N+i,class_num]=1\n",
    "class_num +=1\n",
    "for i in range(P):\n",
    "    imagepath=r'datasets2//P'+str(i+1)+r'.jpg'\n",
    "    image_readin=mping.imread(imagepath)\n",
    "    print(imagepath)\n",
    "    image=image_readin[:,408:408+input_img_rows,:]\n",
    "    image=resize(image, (img_rows, img_cols), mode='reflect')\n",
    "    X[A+B+C+D+E+F+G+H+J+K+L+M+N+O+i,:,:,:]=image\n",
    "    Y[A+B+C+D+E+F+G+H+J+K+L+M+N+O+i,class_num]=1\n",
    "class_num +=1\n",
    "image_sample=X[2]\n",
    "imshow(image_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、网络的一些超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=7, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.0005, patience=40)\n",
    "csv_logger = CSVLogger('imagemodel2.2.csv')\n",
    "checkpointer = ModelCheckpoint('imagemodel2.2.h5', verbose=1, save_best_only=True)\n",
    "batch_size = 10\n",
    "nb_epoch = 300\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、预处理，白化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.astype('float32')\n",
    "mean_X=np.mean(X,axis=0)\n",
    "X -= mean_X  #白化，X减去所有数据的均值\n",
    "X /=128 #这样X大概就是在-1~1之间的分布了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五、搭建网络模型，编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 64)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 64)   0           add_1[0][0]                      \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 64)   0           add_2[0][0]                      \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 64)   256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 128)  73856       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 128)  8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 128)  147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 128)  0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 128)  147584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 128)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 128)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 128)  147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 128)  0           add_6[0][0]                      \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 128)  512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 256)  295168      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 256)  33024       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 256)  590080      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 256)  0           conv2d_19[0][0]                  \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 256)  1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 256)  590080      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 256)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 256)  590080      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 256)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 256)  1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 256)  590080      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 256)  1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 256)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 256)  590080      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 256)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 256)  1024        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 256)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 256)  590080      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 256)  1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 256)  590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 256)  0           add_10[0][0]                     \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 256)  1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 256)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 256)  1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 256)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 256)  590080      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 256)  0           add_11[0][0]                     \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 256)  1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 256)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 256)  590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 256)  1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 256)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 256)  0           add_12[0][0]                     \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 256)  1024        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 256)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 512)    1180160     activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 512)    2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 512)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 512)    131584      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 512)    2359808     activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 512)    0           conv2d_32[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 512)    2048        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 512)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 512)    2359808     activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 512)    2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 512)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 512)    2359808     activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 512)    0           add_14[0][0]                     \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 512)    2048        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 512)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 512)    2359808     activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 512)    2048        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 512)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 512)    2359808     activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 512)    0           add_15[0][0]                     \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 512)    2048        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 512)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 15)           7695        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,314,319\n",
      "Trainable params: 21,299,087\n",
      "Non-trainable params: 15,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet.ResnetBuilder.build_resnet_34((img_channels, img_rows, img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model V2.0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "六、分割X={X_train,X_val}  Y={Y_train,Y_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= (225, 256, 256, 3)\n",
      "Y_train.shape= (225, 15)\n",
      "X_val.shape= (150, 256, 256, 3)\n",
      "Y_val.shape= (150, 15)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"Y_train.shape=\",Y_train.shape)\n",
    "print(\"X_val.shape=\",X_val.shape)\n",
    "print(\"Y_val.shape=\",Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "七、数据增强，训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 225 samples, validate on 150 samples\n",
      "Epoch 1/300\n",
      "225/225 [==============================] - 167s 744ms/step - loss: 3.8961 - acc: 0.1733 - val_loss: 4.5835 - val_acc: 0.0800\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.58352, saving model to imagemodel2.2.h5\n",
      "Epoch 2/300\n",
      "225/225 [==============================] - 162s 721ms/step - loss: 3.3648 - acc: 0.2889 - val_loss: 10.5765 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/300\n",
      "225/225 [==============================] - 160s 713ms/step - loss: 2.8947 - acc: 0.4133 - val_loss: 16.1637 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "225/225 [==============================] - 162s 719ms/step - loss: 2.6386 - acc: 0.4622 - val_loss: 15.5508 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "225/225 [==============================] - 162s 719ms/step - loss: 2.2900 - acc: 0.6000 - val_loss: 16.0639 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "225/225 [==============================] - 159s 705ms/step - loss: 2.0910 - acc: 0.6622 - val_loss: 15.4174 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "225/225 [==============================] - 170s 756ms/step - loss: 1.8505 - acc: 0.7378 - val_loss: 15.9800 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "225/225 [==============================] - 163s 722ms/step - loss: 1.5762 - acc: 0.8533 - val_loss: 9.7609 - val_acc: 0.1333\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "225/225 [==============================] - 167s 744ms/step - loss: 1.5621 - acc: 0.8089 - val_loss: 4.3053 - val_acc: 0.2667\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.58352 to 4.30532, saving model to imagemodel2.2.h5\n",
      "Epoch 10/300\n",
      "225/225 [==============================] - 162s 720ms/step - loss: 1.4336 - acc: 0.8267 - val_loss: 15.4876 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "225/225 [==============================] - 158s 704ms/step - loss: 1.3088 - acc: 0.8756 - val_loss: 4.8974 - val_acc: 0.2067\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "225/225 [==============================] - 158s 704ms/step - loss: 1.1157 - acc: 0.9467 - val_loss: 10.8616 - val_acc: 0.0867\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 1.2518 - acc: 0.8667 - val_loss: 15.8204 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 1.1194 - acc: 0.9111 - val_loss: 15.8048 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 1.1304 - acc: 0.9156 - val_loss: 14.3875 - val_acc: 0.0933\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 0.9263 - acc: 0.9689 - val_loss: 13.5087 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 0.8889 - acc: 0.9644 - val_loss: 14.6944 - val_acc: 0.0933\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 0.8340 - acc: 0.9733 - val_loss: 13.7521 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 0.7374 - acc: 0.9956 - val_loss: 12.4782 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 0.7058 - acc: 1.0000 - val_loss: 8.1547 - val_acc: 0.0733\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "225/225 [==============================] - 162s 719ms/step - loss: 0.6830 - acc: 1.0000 - val_loss: 6.1316 - val_acc: 0.1000\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "225/225 [==============================] - 158s 704ms/step - loss: 0.6589 - acc: 1.0000 - val_loss: 4.8348 - val_acc: 0.2867\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "225/225 [==============================] - 158s 703ms/step - loss: 0.6460 - acc: 1.0000 - val_loss: 3.3616 - val_acc: 0.3667\n",
      "\n",
      "Epoch 00023: val_loss improved from 4.30532 to 3.36157, saving model to imagemodel2.2.h5\n",
      "Epoch 24/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 0.6358 - acc: 0.9911 - val_loss: 2.9423 - val_acc: 0.3933\n",
      "\n",
      "Epoch 00024: val_loss improved from 3.36157 to 2.94234, saving model to imagemodel2.2.h5\n",
      "Epoch 25/300\n",
      "225/225 [==============================] - 158s 702ms/step - loss: 0.6299 - acc: 0.9911 - val_loss: 1.1057 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.94234 to 1.10566, saving model to imagemodel2.2.h5\n",
      "Epoch 26/300\n",
      "225/225 [==============================] - 158s 703ms/step - loss: 0.6310 - acc: 0.9956 - val_loss: 1.8370 - val_acc: 0.4800\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "225/225 [==============================] - 157s 699ms/step - loss: 0.6254 - acc: 0.9867 - val_loss: 4.5488 - val_acc: 0.3533\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.6114 - acc: 0.9956 - val_loss: 9.0135 - val_acc: 0.2267\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.6005 - acc: 0.9956 - val_loss: 5.6280 - val_acc: 0.3667\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.5628 - acc: 1.0000 - val_loss: 3.8554 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.5377 - acc: 1.0000 - val_loss: 1.8099 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "225/225 [==============================] - 161s 714ms/step - loss: 0.5316 - acc: 1.0000 - val_loss: 1.7894 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "225/225 [==============================] - 157s 699ms/step - loss: 0.5421 - acc: 0.9867 - val_loss: 1.2408 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.5535 - acc: 0.9778 - val_loss: 1.1913 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.5099 - acc: 1.0000 - val_loss: 0.8466 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.10566 to 0.84660, saving model to imagemodel2.2.h5\n",
      "Epoch 36/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.5108 - acc: 0.9956 - val_loss: 0.6754 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.84660 to 0.67535, saving model to imagemodel2.2.h5\n",
      "Epoch 37/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.4959 - acc: 1.0000 - val_loss: 0.5889 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.67535 to 0.58892, saving model to imagemodel2.2.h5\n",
      "Epoch 38/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.4880 - acc: 1.0000 - val_loss: 0.5661 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.58892 to 0.56612, saving model to imagemodel2.2.h5\n",
      "Epoch 39/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4776 - acc: 1.0000 - val_loss: 0.5519 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.56612 to 0.55191, saving model to imagemodel2.2.h5\n",
      "Epoch 40/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4768 - acc: 1.0000 - val_loss: 0.5461 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.55191 to 0.54614, saving model to imagemodel2.2.h5\n",
      "Epoch 41/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4732 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.54614 to 0.53534, saving model to imagemodel2.2.h5\n",
      "Epoch 42/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.4652 - acc: 1.0000 - val_loss: 0.5198 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.53534 to 0.51977, saving model to imagemodel2.2.h5\n",
      "Epoch 43/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 157s 700ms/step - loss: 0.4626 - acc: 1.0000 - val_loss: 0.5122 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.51977 to 0.51225, saving model to imagemodel2.2.h5\n",
      "Epoch 44/300\n",
      "225/225 [==============================] - 159s 708ms/step - loss: 0.4581 - acc: 1.0000 - val_loss: 0.5160 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.4571 - acc: 1.0000 - val_loss: 0.5238 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.4551 - acc: 1.0000 - val_loss: 0.5254 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.4541 - acc: 1.0000 - val_loss: 0.5198 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.4425 - acc: 1.0000 - val_loss: 0.4892 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.51225 to 0.48918, saving model to imagemodel2.2.h5\n",
      "Epoch 49/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.4373 - acc: 1.0000 - val_loss: 0.4804 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.48918 to 0.48044, saving model to imagemodel2.2.h5\n",
      "Epoch 50/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.4333 - acc: 1.0000 - val_loss: 0.4741 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.48044 to 0.47405, saving model to imagemodel2.2.h5\n",
      "Epoch 51/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.4286 - acc: 1.0000 - val_loss: 0.4676 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.47405 to 0.46765, saving model to imagemodel2.2.h5\n",
      "Epoch 52/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4238 - acc: 1.0000 - val_loss: 0.4592 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.46765 to 0.45919, saving model to imagemodel2.2.h5\n",
      "Epoch 53/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.4212 - acc: 1.0000 - val_loss: 0.4662 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4161 - acc: 1.0000 - val_loss: 0.4634 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "225/225 [==============================] - 160s 712ms/step - loss: 0.4143 - acc: 1.0000 - val_loss: 0.4639 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.4097 - acc: 1.0000 - val_loss: 0.4558 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.45919 to 0.45585, saving model to imagemodel2.2.h5\n",
      "Epoch 57/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4128 - acc: 1.0000 - val_loss: 0.5178 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4019 - acc: 1.0000 - val_loss: 0.5510 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.4015 - acc: 1.0000 - val_loss: 0.4762 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3971 - acc: 1.0000 - val_loss: 0.4567 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3907 - acc: 1.0000 - val_loss: 0.4385 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.45585 to 0.43848, saving model to imagemodel2.2.h5\n",
      "Epoch 62/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3865 - acc: 1.0000 - val_loss: 0.4331 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.43848 to 0.43313, saving model to imagemodel2.2.h5\n",
      "Epoch 63/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3839 - acc: 1.0000 - val_loss: 0.4662 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3793 - acc: 1.0000 - val_loss: 0.4660 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3755 - acc: 1.0000 - val_loss: 0.4387 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/300\n",
      "225/225 [==============================] - 158s 703ms/step - loss: 0.3709 - acc: 1.0000 - val_loss: 0.4322 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.43313 to 0.43222, saving model to imagemodel2.2.h5\n",
      "Epoch 67/300\n",
      "225/225 [==============================] - 160s 710ms/step - loss: 0.3679 - acc: 1.0000 - val_loss: 0.4343 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3709 - acc: 1.0000 - val_loss: 0.4220 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.43222 to 0.42199, saving model to imagemodel2.2.h5\n",
      "Epoch 69/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3635 - acc: 1.0000 - val_loss: 0.4432 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3627 - acc: 1.0000 - val_loss: 0.4161 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.42199 to 0.41614, saving model to imagemodel2.2.h5\n",
      "Epoch 71/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3568 - acc: 1.0000 - val_loss: 0.3768 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.41614 to 0.37676, saving model to imagemodel2.2.h5\n",
      "Epoch 72/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3546 - acc: 1.0000 - val_loss: 0.3928 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3665 - acc: 0.9956 - val_loss: 0.5239 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3689 - acc: 0.9956 - val_loss: 2.8454 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3534 - acc: 1.0000 - val_loss: 0.7170 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3491 - acc: 1.0000 - val_loss: 0.4562 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3456 - acc: 1.0000 - val_loss: 0.4456 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/300\n",
      "225/225 [==============================] - 161s 714ms/step - loss: 0.3419 - acc: 1.0000 - val_loss: 0.4645 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3349 - acc: 1.0000 - val_loss: 0.4284 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3350 - acc: 1.0000 - val_loss: 0.4053 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3329 - acc: 1.0000 - val_loss: 0.3790 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3309 - acc: 1.0000 - val_loss: 0.3667 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.37676 to 0.36670, saving model to imagemodel2.2.h5\n",
      "Epoch 83/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3327 - acc: 1.0000 - val_loss: 0.3616 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.36670 to 0.36159, saving model to imagemodel2.2.h5\n",
      "Epoch 84/300\n",
      "225/225 [==============================] - 156s 695ms/step - loss: 0.3320 - acc: 1.0000 - val_loss: 0.3724 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3309 - acc: 1.0000 - val_loss: 0.3754 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3282 - acc: 1.0000 - val_loss: 0.3621 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3279 - acc: 1.0000 - val_loss: 0.3486 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.36159 to 0.34858, saving model to imagemodel2.2.h5\n",
      "Epoch 88/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3234 - acc: 1.0000 - val_loss: 0.3494 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/300\n",
      "225/225 [==============================] - 159s 707ms/step - loss: 0.3229 - acc: 1.0000 - val_loss: 0.3524 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/300\n",
      "225/225 [==============================] - 158s 704ms/step - loss: 0.3211 - acc: 1.0000 - val_loss: 0.3507 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3230 - acc: 1.0000 - val_loss: 0.3474 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.34858 to 0.34742, saving model to imagemodel2.2.h5\n",
      "Epoch 92/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3201 - acc: 1.0000 - val_loss: 0.3458 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.34742 to 0.34582, saving model to imagemodel2.2.h5\n",
      "Epoch 93/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3188 - acc: 1.0000 - val_loss: 0.3450 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.34582 to 0.34503, saving model to imagemodel2.2.h5\n",
      "Epoch 94/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3182 - acc: 1.0000 - val_loss: 0.3437 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.34503 to 0.34370, saving model to imagemodel2.2.h5\n",
      "Epoch 95/300\n",
      "225/225 [==============================] - 156s 696ms/step - loss: 0.3163 - acc: 1.0000 - val_loss: 0.3537 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3160 - acc: 1.0000 - val_loss: 0.3441 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3137 - acc: 1.0000 - val_loss: 0.3427 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.34370 to 0.34269, saving model to imagemodel2.2.h5\n",
      "Epoch 98/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3139 - acc: 1.0000 - val_loss: 0.3355 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.34269 to 0.33549, saving model to imagemodel2.2.h5\n",
      "Epoch 99/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3123 - acc: 1.0000 - val_loss: 0.3342 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.33549 to 0.33415, saving model to imagemodel2.2.h5\n",
      "Epoch 100/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3099 - acc: 1.0000 - val_loss: 0.3350 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/300\n",
      "225/225 [==============================] - 160s 712ms/step - loss: 0.3119 - acc: 1.0000 - val_loss: 0.3380 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3087 - acc: 1.0000 - val_loss: 0.3339 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.33415 to 0.33393, saving model to imagemodel2.2.h5\n",
      "Epoch 103/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.3078 - acc: 1.0000 - val_loss: 0.3312 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.33393 to 0.33125, saving model to imagemodel2.2.h5\n",
      "Epoch 104/300\n",
      "225/225 [==============================] - 157s 699ms/step - loss: 0.3088 - acc: 1.0000 - val_loss: 0.3257 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.33125 to 0.32568, saving model to imagemodel2.2.h5\n",
      "Epoch 105/300\n",
      "225/225 [==============================] - 157s 700ms/step - loss: 0.3054 - acc: 1.0000 - val_loss: 0.3279 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/300\n",
      "225/225 [==============================] - 157s 699ms/step - loss: 0.3056 - acc: 1.0000 - val_loss: 0.3255 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.32568 to 0.32554, saving model to imagemodel2.2.h5\n",
      "Epoch 107/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.3039 - acc: 1.0000 - val_loss: 0.3240 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.32554 to 0.32399, saving model to imagemodel2.2.h5\n",
      "Epoch 108/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3024 - acc: 1.0000 - val_loss: 0.3218 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.32399 to 0.32176, saving model to imagemodel2.2.h5\n",
      "Epoch 109/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.3018 - acc: 1.0000 - val_loss: 0.3254 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 110/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2997 - acc: 1.0000 - val_loss: 0.3311 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 111/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2995 - acc: 1.0000 - val_loss: 0.3268 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 112/300\n",
      "225/225 [==============================] - 160s 713ms/step - loss: 0.2988 - acc: 1.0000 - val_loss: 0.3209 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.32176 to 0.32093, saving model to imagemodel2.2.h5\n",
      "Epoch 113/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.2969 - acc: 1.0000 - val_loss: 0.3118 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.32093 to 0.31183, saving model to imagemodel2.2.h5\n",
      "Epoch 114/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2958 - acc: 1.0000 - val_loss: 0.3115 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.31183 to 0.31154, saving model to imagemodel2.2.h5\n",
      "Epoch 115/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2949 - acc: 1.0000 - val_loss: 0.3107 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.31154 to 0.31073, saving model to imagemodel2.2.h5\n",
      "Epoch 116/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2931 - acc: 1.0000 - val_loss: 0.3128 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 117/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2915 - acc: 1.0000 - val_loss: 0.3116 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2908 - acc: 1.0000 - val_loss: 0.3090 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.31073 to 0.30901, saving model to imagemodel2.2.h5\n",
      "Epoch 119/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2904 - acc: 1.0000 - val_loss: 0.3067 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.30901 to 0.30672, saving model to imagemodel2.2.h5\n",
      "Epoch 120/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2885 - acc: 1.0000 - val_loss: 0.3096 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2895 - acc: 1.0000 - val_loss: 0.3039 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.30672 to 0.30386, saving model to imagemodel2.2.h5\n",
      "Epoch 122/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2861 - acc: 1.0000 - val_loss: 0.3021 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.30386 to 0.30212, saving model to imagemodel2.2.h5\n",
      "Epoch 123/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2858 - acc: 1.0000 - val_loss: 0.3048 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 124/300\n",
      "225/225 [==============================] - 161s 714ms/step - loss: 0.2848 - acc: 1.0000 - val_loss: 0.3064 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 125/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2832 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.30212 to 0.30175, saving model to imagemodel2.2.h5\n",
      "Epoch 126/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2827 - acc: 1.0000 - val_loss: 0.3118 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2811 - acc: 1.0000 - val_loss: 0.3086 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2818 - acc: 1.0000 - val_loss: 0.3049 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 129/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2978 - acc: 0.9956 - val_loss: 0.3049 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2881 - acc: 0.9956 - val_loss: 0.3027 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2812 - acc: 1.0000 - val_loss: 0.2980 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.30175 to 0.29795, saving model to imagemodel2.2.h5\n",
      "Epoch 132/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2784 - acc: 1.0000 - val_loss: 0.3019 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2781 - acc: 1.0000 - val_loss: 0.3022 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 134/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2753 - acc: 1.0000 - val_loss: 0.3070 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 135/300\n",
      "225/225 [==============================] - 160s 712ms/step - loss: 0.2765 - acc: 1.0000 - val_loss: 0.2991 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 136/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2756 - acc: 1.0000 - val_loss: 0.2996 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00136: val_loss did not improve\n",
      "Epoch 137/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2736 - acc: 1.0000 - val_loss: 0.3004 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00137: val_loss did not improve\n",
      "Epoch 138/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2717 - acc: 1.0000 - val_loss: 0.2931 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.29795 to 0.29311, saving model to imagemodel2.2.h5\n",
      "Epoch 139/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2701 - acc: 1.0000 - val_loss: 0.2880 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.29311 to 0.28801, saving model to imagemodel2.2.h5\n",
      "Epoch 140/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2712 - acc: 1.0000 - val_loss: 0.2843 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.28801 to 0.28425, saving model to imagemodel2.2.h5\n",
      "Epoch 141/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2708 - acc: 1.0000 - val_loss: 0.3012 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00141: val_loss did not improve\n",
      "Epoch 142/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2690 - acc: 1.0000 - val_loss: 0.2843 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 143/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2703 - acc: 1.0000 - val_loss: 0.3365 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00143: val_loss did not improve\n",
      "Epoch 144/300\n",
      "225/225 [==============================] - 156s 696ms/step - loss: 0.3115 - acc: 0.9867 - val_loss: 0.6038 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00144: val_loss did not improve\n",
      "Epoch 145/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2779 - acc: 1.0000 - val_loss: 0.5478 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 146/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.2696 - acc: 1.0000 - val_loss: 0.3764 - val_acc: 0.9533\n",
      "\n",
      "Epoch 00146: val_loss did not improve\n",
      "Epoch 147/300\n",
      "225/225 [==============================] - 160s 712ms/step - loss: 0.2684 - acc: 1.0000 - val_loss: 0.2930 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 148/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2652 - acc: 1.0000 - val_loss: 0.2813 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.28425 to 0.28126, saving model to imagemodel2.2.h5\n",
      "Epoch 149/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2654 - acc: 1.0000 - val_loss: 0.2766 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.28126 to 0.27655, saving model to imagemodel2.2.h5\n",
      "Epoch 150/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2639 - acc: 1.0000 - val_loss: 0.2835 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      "Epoch 151/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2644 - acc: 1.0000 - val_loss: 0.2820 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00151: val_loss did not improve\n",
      "Epoch 152/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2658 - acc: 1.0000 - val_loss: 0.2757 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.27655 to 0.27566, saving model to imagemodel2.2.h5\n",
      "Epoch 153/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2642 - acc: 1.0000 - val_loss: 0.2809 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00153: val_loss did not improve\n",
      "Epoch 154/300\n",
      "225/225 [==============================] - 157s 700ms/step - loss: 0.2625 - acc: 1.0000 - val_loss: 0.2911 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00154: val_loss did not improve\n",
      "Epoch 155/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2606 - acc: 1.0000 - val_loss: 0.2855 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      "Epoch 156/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2586 - acc: 1.0000 - val_loss: 0.2790 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 157/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2592 - acc: 1.0000 - val_loss: 0.2759 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 158/300\n",
      "225/225 [==============================] - 160s 711ms/step - loss: 0.2598 - acc: 1.0000 - val_loss: 0.2721 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.27566 to 0.27208, saving model to imagemodel2.2.h5\n",
      "Epoch 159/300\n",
      "225/225 [==============================] - 157s 698ms/step - loss: 0.2587 - acc: 1.0000 - val_loss: 0.2643 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.27208 to 0.26433, saving model to imagemodel2.2.h5\n",
      "Epoch 160/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2595 - acc: 1.0000 - val_loss: 0.2629 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.26433 to 0.26286, saving model to imagemodel2.2.h5\n",
      "Epoch 161/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2552 - acc: 1.0000 - val_loss: 0.2639 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 162/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2557 - acc: 1.0000 - val_loss: 0.2613 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.26286 to 0.26131, saving model to imagemodel2.2.h5\n",
      "Epoch 163/300\n",
      "225/225 [==============================] - 157s 697ms/step - loss: 0.2531 - acc: 1.0000 - val_loss: 0.2600 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.26131 to 0.26003, saving model to imagemodel2.2.h5\n",
      "Epoch 164/300\n",
      "225/225 [==============================] - 157s 696ms/step - loss: 0.2540 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 165/300\n",
      "220/225 [============================>.] - ETA: 3s - loss: 0.2522 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8cc5f6458aa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m               callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using real-time data augmentation.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1247\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[0;32m   1248\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m                                                        verbose=0)\n\u001b[0m\u001b[0;32m   1250\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1427\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              verbose=1,\n",
    "              validation_data=(X_val, Y_val),\n",
    "              shuffle=True,\n",
    "              callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=True,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=True,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=False,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=False,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        epochs=nb_epoch, verbose=1, max_q_size=100,\n",
    "                        callbacks=[lr_reducer,checkpointer, early_stopper, csv_logger])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "八、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('imagemodel2.2.h5')\n",
    "pred=model.predict(X_val[:10])\n",
    "print(pred[:10])\n",
    "print(Y_val[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比如V1.0版本，这里把resnet18改成了resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=np.argmax(pred,1)\n",
    "print(prediction)\n",
    "print(np.argmax(Y_val,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
